{
  "date": "Sunday, March 01, 2026",
  "weather": {
    "current_temp": 38,
    "unit": "F",
    "conditions": "Partly Sunny",
    "high": 38,
    "low": 33,
    "forecast": "Partly cloudy. Low around 33, with temperatures rising to around 37 overnight. North wind around 10 mph.",
    "hourly": [
      {
        "label": "7am",
        "hour": 7,
        "temp": 34,
        "conditions": "Light Snow Likely",
        "wind": "10 mph N",
        "humidity": "75%",
        "precip_chance": "57%"
      },
      {
        "label": "9am",
        "hour": 9,
        "temp": 33,
        "conditions": "Light Snow Likely",
        "wind": "10 mph N",
        "humidity": "78%",
        "precip_chance": "57%"
      },
      {
        "label": "3pm",
        "hour": 15,
        "temp": 37,
        "conditions": "Mostly Sunny",
        "wind": "9 mph N",
        "humidity": "59%",
        "precip_chance": "11%"
      },
      {
        "label": "5pm",
        "hour": 17,
        "temp": 37,
        "conditions": "Mostly Sunny",
        "wind": "10 mph N",
        "humidity": "52%",
        "precip_chance": "1%"
      },
      {
        "label": "7pm",
        "hour": 19,
        "temp": 35,
        "conditions": "Mostly Clear",
        "wind": "10 mph N",
        "humidity": "52%",
        "precip_chance": "0%"
      }
    ]
  },
  "news": [
    {
      "title": "Anthropic\u2019s Claude rises to No. 2 in the App Store following Pentagon dispute",
      "source": "TechCrunch",
      "link": "https://techcrunch.com/2026/02/28/anthropics-claude-rises-to-no-2-in-the-app-store-following-pentagon-dispute/",
      "published": "Sat, 28 Feb 2026 21:05:06 +0000",
      "raw_text": "Anthropic\u2019s chatbot Claude seems to have benefited from the attention around the company\u2019s fraught negotiations with the Pentagon.\nAs first reported by CNBC, as of Saturday afternoon, Claude is currently ranked number two among free apps in Apple\u2019s US App Store \u2014 the number one app is OpenAI\u2019s ChatGPT, and number three is Google Gemini.\nAccording to data from SensorTower, Claude was just outside the top 100 at the end of January, and has spent most of February somewhere in the top 20. Its ranking has climbed in the last few days, from sixth on Wednesday to fourth on Thursday to second on Saturday (today).\nAfter Anthropic attempted to negotiate for safeguards preventing the Department of Defense from using its AI models for mass domestic surveillance or fully autonomous weapons, President Donald Trump directed federal agencies to stop using all Anthropic products and Secretary of Defense Pete Hegseth said he\u2019s designating the company a supply-chain threat.\nOpenAI subsequently announced its own agreement with the Pentagon, which CEO Sam Altman claimed includes safeguards related to domestic surveillance and autonomous weapons.",
      "summary": "\ud83d\udcc8 Claude rose to number two among free apps in Apple\u2019s US App Store as of Saturday afternoon.<br>\ud83d\udcca Claude climbed from being outside the top 100 in January to sixth on Wednesday, then second on Saturday.<br>\ud83c\udfdb\ufe0f President Donald Trump directed federal agencies to stop using Anthropic products after negotiations on AI safeguards."
    },
    {
      "title": "U.S. Strikes in Middle East Use Anthropic, Hours After Trump Ban",
      "source": "WSJ",
      "link": "https://news.google.com/rss/articles/CBMi1gFBVV95cUxPQlMzT2hZWXJCck9ibGJSMG91ZnU5NmFqdGcyejlXVV9mMDB3eWxpX0swZGd3YjZPYTE2Mm5wVGx5UXdDbjJnM1NNNTdiOEd6NFdHOG5ZaE9BWHQ2SWltVWIyQWYyVkxXajAtUFBwWEdMYVFPY19adklCQUVYeFh6aGtzYUtacmpveTEtSG1GbFFKUXlZOHUxZlZ5V0JjVWoxVGtPQjh2ZjgtU2RoT1dKaDlTQmt5RmlPc0FmNzJQTlVCYkkwQjl6LW5TUmg4ZW5YbTM0ZF93?oc=5",
      "published": "Sun, 01 Mar 2026 04:55:00 GMT",
      "raw_text": "",
      "summary": "\ud83d\udca5 US forces conducted strikes in the Middle East.<br>\ud83e\udd16 Anthropic's AI technology was utilized in these US strikes.<br>\u23f1\ufe0f The strikes occurred hours after President Trump banned federal agencies from using Anthropic products."
    },
    {
      "title": "Lucas: Virginia Tech Rapid Reactions - University of North Carolina Athletics",
      "source": "University of North Carolina Athletics",
      "link": "https://news.google.com/rss/articles/CBMijwFBVV95cUxNWWdTYlc4VkFKZndZN1o2cExUTkNPbS1PbVZyZUt5LWFJWjdyTDl6VGZsQW03YUNwYkZFelpQX2RaRXkwSjhLWjFKTFVSRFhqcGJ2bk05TEliQ0txUEtLMW1IYndvMURoS2NBRUl4Q09SSjlHY09pMXZ5M3ZHbDJrTmtkaVZlOW1MYjdJM0pGQQ?oc=5",
      "published": "Sun, 01 Mar 2026 04:00:03 GMT",
      "raw_text": "University of North Carolina Athletics\nLucas: Virginia Tech Rapid Reactions\nFebruary 28, 2026 | Men's Basketball, Featured Writers, Adam Lucas\nQuick takeaways from the matchup with the Hokies.\nBy Adam Lucas\n1. Carolina played a solid second half in defeating Virginia Tech, 89-82.\n2. The Tar Heels weren't very crisp in the first half, but they hung around just long enough to be close and allow their standouts to take over. Henri Veesaar scored 18 of his career-high tying 26 points after the break, and Seth Trimble added 13 of his 20. Neither had been particularly effective in the first 20 minutes, but were big in the decisive moments. Veesaar took almost half his shots from the three-point line in the first half but took seven of his nine attempts from two in the second half.\n3. Virginia Tech dictated the tempo in the first half, piling up an 11-2 lead in fast break points. In his halftime interview on the Tar Heel Sports Network, assistant coach Sean May attributed that deficit to some transition defense failures but also to bad shot selection creating some Hokie run-outs. The Heels were much better in that area in the second half, allowing zero points in transition as the game moved into an exclusively halfcourt contest.\n4. After a pair of subpar free throw shooting games, the Heels were much better on Saturday. They converted 23 of 30 opportunities, led by an 8-of-8 showing from Zayden High.\n5. And a less obvious corollary: Carolina was fairly solid in keeping the Hokies off the line. Mike Young's team is one of the highest-scoring teams in the league at the free throw line in ACC games. They had just 11 attempts (and made them all).\n6. Saturday's win tied the program mark for the most home wins in a season. Carolina is now 17-0 at home this season, which ties the 17 wins (in a 17-1 record) in 2011-12. The Heels will go for their sixth undefeated season in the building's history on Tuesday night (the others are 1987, 1993, 2005, 2011 and 2017).\n7. Carolina's bench was instrumental in keeping the game close at halftime. The Heels got 12 points from Jonathan Powell on the way to 21 bench points in a half that ended in a 44-44 tie. Almost half of Carolina's field goal attempts (14 out of 29) in the first half were three-point shots, and they made six. Powell finished with 15 and was +12 in the game. The UNC bench had a 32-13 scoring edge on the Hokie reserves.\n8. With Jarin Stevenson limited by foul trouble and eventually fouling out, High again provided good minutes. He had 10 points, five rebounds (two offensive), and a series of energy plays that have become his trademark. High was a team-high +13.\n9. Another reserve who played well: Kyan Evans. He's had an up and down season, but Saturday was one of his best games even without much scoring output. Evans got on the floor to save a key loose ball with eight minutes left in the second half, played some solid defense and had just one turnover in 24 minutes. He only attempted two shots, but he was steady during a time when the Heels needed it.\n10. Hubert Davis will likely talk about Carolina's 14 turnovers. Half of them came from two players, but that total still tied the season high.\n11. The Heels obviously miss Caleb Wilson in multiple ways, but he would have been especially useful against Tobi Lawal, who had 16 points and missed only one shot from the field or the line.\n12. Saturday's developments in the Atlantic Coast Conference were positive for the Tar Heels. NC State's loss to Notre Dame means Carolina controls its own destiny for a double bye in the ACC Tournament. The Heels close out the home schedule Tuesday at 7 p.m. against Clemson (a note: your parking pass might say 9 p.m. due to a time change after printing deadlines, but the correct game time is 7 p.m.).\n13. A reminder that the Tuesday game is senior day for Seth Trimble and Elijah Davis. With the 7 p.m. start, pregame traffic will be a challenge, and fans are encouraged to arrive early in order to participate in senior recognition beginning at 6:30. Trimble could be one of the last four-year seniors with such a major impact that we see. He deserves a huge pregame ovation.\n1. Carolina played a solid second half in defeating Virginia Tech, 89-82.\n2. The Tar Heels weren't very crisp in the first half, but they hung around just long enough to be close and allow their standouts to take over. Henri Veesaar scored 18 of his career-high tying 26 points after the break, and Seth Trimble added 13 of his 20. Neither had been particularly effective in the first 20 minutes, but were big in the decisive moments. Veesaar took almost half his shots from the three-point line in the first half but took seven of his nine attempts from two in the second half.\n3. Virginia Tech dictated the tempo in the first half, piling up an 11-2 lead in fast break points. In his halftime interview on the Tar Heel Sports Network, assistant coach Sean May attributed that deficit to some transition defense failures but also to bad shot selection creating some Hokie run-outs. The Heels were much better in that area in the second half, allowing zero points in transition as the game moved into an exclusively halfcourt contest.\n4. After a pair of subpar free throw shooting games, the Heels were much better on Saturday. They converted 23 of 30 opportunities, led by an 8-of-8 showing from Zayden High.\n5. And a less obvious corollary: Carolina was fairly solid in keeping the Hokies off the line. Mike Young's team is one of the highest-scoring teams in the league at the free throw line in ACC games. They had just 11 attempts (and made them all).\n6. Saturday's win tied the program mark for the most home wins in a season. Carolina is now 17-0 at home this season, which ties the 17 wins (in a 17-1 record) in 2011-12. The Heels will go for their sixth undefeated season in the building's history on Tuesday night (the others are 1987, 1993, 2005, 2011 and 2017).\n7. Carolina's bench was instrumental in keeping the game close at halftime. The Heels got 12 points from Jonathan Powell on the way to 21 bench points in a half that ended in a 44-44 tie. Almost half of Carolina's field goal attempts (14 out of 29) in the first half were three-point shots, and they made six. Powell finished with 15 and was +12 in the game. The UNC bench had a 32-13 scoring edge on the Hokie reserves.\n8. With Jarin Stevenson limited by foul trouble and eventually fouling out, High again provided good minutes. He had 10 points, five rebounds (two offensive), and a series of energy plays that have become his trademark. High was a team-high +13.\n9. Another reserve who played well: Kyan Evans. He's had an up and down season, but Saturday was one of his best games even without much scoring output. Evans got on the floor to save a key loose ball with eight minutes left in the second half, played some solid defense and had just one turnover in 24 minutes. He only attempted two shots, but he was steady during a time when the Heels needed it.\n10. Hubert Davis will likely talk about Carolina's 14 turnovers. Half of them came from two players, but that total still tied the season high.\n11. The Heels obviously miss Caleb Wilson in multiple ways, but he would have been especially useful against Tobi Lawal, who had 16 points and missed only one shot from the field or the line.\n12. Saturday's developments in the Atlantic Coast Conference were positive for the Tar Heels. NC State's loss to Notre Dame means Carolina controls its own destiny for a double bye in the ACC Tournament. The Heels close out the home schedule Tuesday at 7 p.m. against Clemson (a note: your parking pass might say 9 p.m. due to a time change after printing deadlines, but the correct game time is 7 p.m.).\n13. A reminder that the Tuesday game is senior day for Seth Trimble and Elijah Davis. With the 7 p.m. start, pregame traffic will be a challenge, and fans are encouraged to arrive early in order to participate in senior recognition beginning at 6:30. Trimble could be one of the last four-year seniors with such a major impact that we see. He deserves a huge pregame ovation.\nPlayers Mentioned\nCarolina Insider: Rapid Reactions \u2013 Men\u2019s Basketball vs. Virginia Tech \u2013 February 28, 2026\nSunday, March 01\nUNC Men's Basketball: Veesaar, Tar Heels Hold Off Hokies, 89-82\nSunday, March 01\nUNC Baseball: Diamond Heels Roll to Third Straight Run-Rule Victory, 12-2\nSunday, March 01\nUNC Softball: Barbee, Heels Walk Off Eagles in Game 2, 5-4\nSaturday, February 28",
      "summary": "\ud83c\udfc0 Carolina defeated Virginia Tech 89-82, tying the program record for most home wins with 17 this season.<br>\u26f9\ufe0f Henri Veesaar scored 18 of his career-high tying 26 points after halftime, and Seth Trimble added 13 of his 20 points in the second half.<br>\ud83c\udfaf Carolina converted 23 of 30 free throw opportunities, including an 8-of-8 showing from Zayden High."
    },
    {
      "title": "Israel and Iran exchange fresh attacks after Iran's Supreme Leader Khamenei killed",
      "source": "BBC",
      "link": "https://news.google.com/rss/articles/CBMiVEFVX3lxTE1BRHVheEFBVUpmNUpoZkw3RWJxNVFNRl9rc2FmVWtMd1FRNEJTSFJLNUxuRjVVZkcyTWs0VVZpYnE3d1pqaGpNTDlRTTRldlFQOS00Sg?oc=5",
      "published": "Sun, 01 Mar 2026 11:08:26 GMT",
      "raw_text": "Flights cancelled as airspace closes across Middle Eastpublished at 11:13 GMT\nAirlines are still cancelling and diverting flights in and around the Middle East following the US-Israel attack on Iran and Tehran's retaliatory strikes.\nAll flights from Dubai International and Dubai Al Maktoum are suspended until further notice.\nThe Emirates airline says it has suspended all operations from Dubai until at least 15:00 local time on Monday, blaming airspace closures, while Etihad has suspended flights out of Abu Dhabi until 02:00 on Monday for the same reason.\nFour staff at Dubai International were injured in an incident overnight.\nBritish Airways has cancelled flights to Tel Aviv and Bahrain until Wednesday and is also warning of impacts on services to Abu Dhabi, Amman, Doha and Dubai.\nVirgin Atlantic has also cancelled flights between London and Dubai and London and Riyadh on Sunday and Monday.\nSingapore Airlines and Air India are along the other airlines that have altered their schedules in the wake of events in the Middle East.",
      "summary": "\u2708\ufe0f All flights from Dubai International and Dubai Al Maktoum were suspended until further notice due to airspace closures.<br>\ud83d\uded1 British Airways cancelled flights to Tel Aviv and Bahrain until Wednesday, with impacts expected on services to other Middle East cities.<br>\ud83e\ude79 Four staff members at Dubai International were injured in an incident overnight."
    },
    {
      "title": "Column | Asking Eric: Grandson has no motivation; daughter enables him",
      "source": "The Washington Post",
      "link": "https://news.google.com/rss/articles/CBMikwFBVV95cUxPc1BxWW9nTUduaUxYSXNGY0wtZUQyN0R6dFJ5cXFWRVJtS1RmWmpXQXJ5azJpeFpVTF9LUGptWHdtVGlTY2gtcDlWOF9MUzVaZk92V3htblRQSml1RGdqUXF1aUp3ejYwVGs2VVZ0S3dxd1Zid2ZncExtZHVTQmVjdzF1MC1KN1U5UnlrZmlDZWo0NEk?oc=5",
      "published": "Sun, 01 Mar 2026 11:01:57 GMT",
      "raw_text": "",
      "summary": "\ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc66 The column addresses a reader's concern about their grandson's lack of motivation.<br>\ud83d\udcac The reader details their daughter's enabling behavior towards the grandson.<br>\u270d\ufe0f The column offers guidance on addressing family dynamics and fostering independence."
    }
  ],
  "podcasts": [
    {
      "podcast": "Lex Fridman Podcast",
      "title": "#492 \u2013 Rick Beato: Greatest Guitarists of All Time, History & Future of Music",
      "published": "2026-03-01",
      "summary": "\ud83c\udfaf #492 \u2013 Rick Beato: Greatest Guitarists of All Time, History & Future of Music",
      "raw_text": "Rick Beato is a music educator, interviewer, producer, songwriter, and a true multi-instrument musician, playing guitar, bass, cello &#38; piano. His incredible YouTube channel celebrates great musicians &#38; musical ideas, and helps millions of people fall in love with great music all over again.\nThank you for listening \u2764 Check out our sponsors: https://lexfridman.com/sponsors/ep492-sc\nSee below for timestamps, transcript, and to give feedback, submit questions, contact Lex, etc.\nTranscript:\nhttps://lexfridman.com/rick-beato-transcript\nCONTACT LEX:\nFeedback &#8211; give feedback to Lex: https://lexfridman.com/survey\nAMA &#8211; submit questions, videos or call-in: https://lexfridman.com/ama\nHiring &#8211; join our team: https://lexfridman.com/hiring\nOther &#8211; other ways to get in touch: https://lexfridman.com/contact\nEPISODE LINKS:\nRick&#8217;s YouTube: https://youtube.com/RickBeato\nRick&#8217;s X: https://x.com/rickbeato\nRick&#8217;s Instagram: https://instagram.com/rickbeato1\nRick&#8217;s Website: https://rickbeato.com\nRick&#8217;s Ear Training: https://beatoeartraining.com\nThe Beato Book: https://beatobook.com\nSPONSORS:\nTo support this podcast, check out our sponsors &#38; get discounts:\nUPLIFT Desk: Standing desks and office ergonomics.\nGo to https://upliftdesk.com/lex\nBetterHelp: Online therapy and counseling.\nGo to https://betterhelp.com/lex\nLMNT: Zero-sugar electrolyte drink mix.\nGo to https://drinkLMNT.com/lex\nFin: AI agent for customer service.\nGo to https://fin.ai/lex\nShopify: Sell stuff online.\nGo to https://shopify.com/lex\nPerplexity: AI-powered answer engine.\nGo to https://perplexity.ai/\nOUTLINE:\n(00:00) &#8211; Introduction\n(00:28) &#8211; Sponsors, Comments, and Reflections\n(09:17) &#8211; Guitar solos\n(13:16) &#8211; Gypsy jazz and Django Reinhardt\n(14:48) &#8211; Bebop jazz\n(19:00) &#8211; Perfect pitch vs relative pitch\n(23:37) &#8211; Learning to play guitar\n(47:08) &#8211; Miles Davis\n(52:34) &#8211; Bass guitar\n(53:41) &#8211; Greatest guitar solos of all time\n(1:22:56) &#8211; 27 Club\n(1:27:37) &#8211; Elton John\n(1:30:51) &#8211; Metallica\n(1:35:21) &#8211; Tom Waits\n(1:41:12) &#8211; Greatest rock stars\n(1:44:35) &#8211; Beethoven\n(1:51:10) &#8211; Bach\n(1:54:01) &#8211; AI in music\n(2:07:52) &#8211; Sabrina Carpenter\n(2:11:23) &#8211; YouTube copyright strikes\n(2:16:59) &#8211; Spotify\n(2:27:51) &#8211; Guitars\n(2:32:13) &#8211; Advice\nPODCAST LINKS:\n&#8211; Podcast Website: https://lexfridman.com/podcast\n&#8211; Apple Podcasts: https://apple.co/2lwqZIr\n&#8211; Spotify: https://spoti.fi/2nEwCF8\n&#8211; RSS: https://lexfridman.com/feed/podcast/\n&#8211; Podcast Playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4\n&#8211; Clips Channel: https://www.youtube.com/lexclips",
      "link": "https://lexfridman.com/rick-beato/?utm_source=rss&utm_medium=rss&utm_campaign=rick-beato"
    },
    {
      "podcast": "This Week in Startups",
      "title": "The Biggest Private Funding Round in History | E2256",
      "published": "2026-02-28",
      "summary": "\ud83d\udcb0 OpenAI reportedly raised $110 billion in private funding, with investments from Microsoft, Nvidia, and SoftBank.<br>\ud83d\udcc8 ChatGPT currently serves 900 million weekly active users and has 50 million paying subscribers.<br>\ud83e\udde0 Host Jason Calacanis claimed that AGI has already been achieved, though it is not yet fully implemented.",
      "raw_text": "This Week In Startups is made possible by:Deel - http://deel.com/twistWispr Flow - https://wisprflow.ai/twistLuma AI - https://lumalabs.ai/twistToday\u2019s show:*$110 billion buys you 15% of OpenAI. Amazon, Nvidia, and SoftBank placed their bets on ChatGPT, which now has 900 million weekly active users and 50 million paying subscribers. Find out why Jason is anticipating the wildest J-Curve swing of all time, and believes we\u2019ve ALREADY hit AGI\u2026 it\u2019s just not implemented yet.Plus a visit from our roving correspondent Nick O\u2019Neill, checking in on the Crypto Chaos in Miami Beach, and hot demos from three young founders.GUESTS:Nick O\u2019Neill: https://x.com/chooserichEverest Chris: https://openclaw.unloopa.com/Ben Broca: https://polsia.com/Adi Gabrani: https://makemyclaw.com/Timestamps:00:00 Intro01:33 We\u2019re hiring a new producer!05:42 OpenAI raised $110 billion08:59 Understanding the LLM J-Curve00:11:25 Deel - Founders ship faster on Deel. Set up payroll for any country in minutes and get back to building. Visit \u2060https://deel.com/twist\u2060 to learn more.00:15:02 CRYPTO CHAOS IN MIAMI BEACH!00:21:10 Wispr Flow - Stop typing. Dictate with Wispr Flow and send clean, final-draft writing in seconds. Visit \u2060https://wisprflow.ai/twist\u2060 to get started for free today.00:22:54 Mass layoffs at Block00:30:50 Luma AI - Stop guessing and start directing with the all-in-one Dream Machine text-to-video platform. Visit\u00a0\u2060https://lumalabs.ai/twist\u2060\u00a0to try The Dream Machine for free.00:32:04 AI Scott Adams: The Saga Continues00:38:13 Make URLs for local businesses with Unloopa00:45:36 Rent a Polsia agent to run your company00:58:55 Deploy swarms in 60 seconds with MakeMyClaw01:05:05 LAUNCH FEST is coming to SF01:55:49 Will Paramount actually buy WBD?01:06:58 Why Lon loves \u201cKnight of the 7 Kingdoms\u201d01:07:21 On \u201cNeighbors\u201d and First Amendment Warriors01:13:43 All about Jason\u2019s favorite chargersSubscribe to the TWiST500 newsletter: https://ticker.thisweekinstartups.comCheck out the TWIST500: https://www.twist500.comSubscribe to This Week in Startups on Apple: https://rb.gy/v19fcpFollow Lon:X: https://x.com/lonsFollow Alex:X: https://x.com/alexLinkedIn: \u2060https://www.linkedin.com/in/alexwilhelmFollow Jason:X: https://twitter.com/JasonLinkedIn: https://www.linkedin.com/in/jasoncalacanisCheck out all our partner offers: https://partners.launch.co/Great TWIST interviews: Will Guidara, Eoghan McCabe, Steve Huffman, Brian Chesky, Bob Moesta, Aaron Levie, Sophia Amoruso, Reid Hoffman, Frank Slootman, Billy McFarlandCheck out Jason\u2019s suite of newsletters: https://substack.com/@calacanisFollow TWiST:Twitter: https://twitter.com/TWiStartupsYouTube: https://www.youtube.com/thisweekinInstagram: https://www.instagram.com/thisweekinstartupsTikTok: https://www.tiktok.com/@thisweekinstartupsSubstack: https://twistartups.substack.com",
      "link": "https://podcasters.spotify.com/pod/show/thisweekinstartups/episodes/The-Biggest-Private-Funding-Round-in-History--E2256-e3fniar"
    },
    {
      "podcast": "AI Daily Brief",
      "title": "Who Controls AI?",
      "published": "2026-02-28",
      "summary": "\ud83d\udcac Anthropic CEO Dario Amodei publicly stated concerns about using AI models for autonomous weapons and mass domestic surveillance.<br>\ud83d\udea8 President Trump posted on Truth Social directing federal agencies to cease using Anthropic products.<br>\ud83e\udd1d OpenAI subsequently announced its own agreement with the Department of War, claiming similar safeguards were included.",
      "raw_text": "The standoff between Anthropic and the Pentagon exploded this week when President Trump directed every federal agency to cease using Anthropic's technology after the company refused to remove its red lines on autonomous weapons and mass domestic surveillance. As the episode unpacks the full timeline \u2014 from Dario Amodei's public statement to Trump's Truth Social post to OpenAI's deal with the Department of War \u2014 what emerges is a fight far bigger than one contract, touching the fundamental question of who gets to control the most important technology of the century.Want to build with OpenClaw?LEARN MORE ABOUT CLAW CAMP: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://campclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Or for enterprises, check out: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://enterpriseclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Brought to you by:KPMG \u2013 Agentic AI is powering a potential $3 trillion productivity shift, and KPMG\u2019s new paper, Agentic AI Untangled, gives leaders a clear framework to decide whether to build, buy, or borrow\u2014download it at \u2060\u2060\u2060\u2060\u2060\u2060www.kpmg.us/Navigate\u2060\u2060\u2060\u2060\u2060\u2060Mercury - Modern banking for business and now personal accounts. Learn more at \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://mercury.com/personal-banking\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Rackspace Technology - Build, test and scale intelligent workloads faster with Rackspace AI Launchpad -\u00a0\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060http://rackspace.com/ailaunchpad\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Blitzy - Want to accelerate enterprise software development velocity by 5x? \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://blitzy.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Optimizely Agents in Action - Join the virtual event (with me!) free March 4 - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.optimizely.com/insights/agents-in-action/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060AssemblyAI - The best way to build Voice AI apps - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.assemblyai.com/brief\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060LandfallIP - AI to Navigate the Patent Process - https://landfallip.com/Robots &amp; Pencils - Cloud-native AI solutions that power results \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://robotsandpencils.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060The Agent Readiness Audit from Superintelligent - Go to \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://besuper.ai/ \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060to request your company's agent readiness score.The AI Daily Brief helps you understand the most important news and discussions in AI. Subscribe to the podcast version of The AI Daily Brief wherever you listen: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://pod.link/1680633614\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Our Newsletter is BACK: \u2060https://aidailybrief.beehiiv.com/\u2060Interested in sponsoring the show? sponsors@aidailybrief.ai",
      "link": "https://podcasters.spotify.com/pod/show/nlw/episodes/Who-Controls-AI-e3foi8t"
    },
    {
      "podcast": "AI Daily Brief",
      "title": "Are 40% Staff Cuts the New AI Normal?",
      "published": "2026-02-28",
      "summary": "\ud83c\udf4c Google released a new model named Nano Banana 2.<br>\ud83d\udcc8 Claude AI experienced a surge in signups, indicating increased user adoption.<br>\ud83d\udcbb Microsoft previewed its new Copilot Tasks feature.",
      "raw_text": "Block just cut 40% of its workforce in one move, with Jack Dorsey arguing that new intelligence tools and smaller, flatter teams fundamentally change how companies operate\u2014prompting a massive stock surge and igniting debate over whether this is the first true AI-driven headcount reset or simply COVID overhiring getting cleaned up under a new narrative. In the headlines: Google releases Nano Banana 2, Claude signups surge, Meta pulls back on custom chips, and Microsoft previews Copilot Tasks.Want to build with OpenClaw?LEARN MORE ABOUT CLAW CAMP: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://campclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Or for enterprises, check out: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://enterpriseclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Brought to you by:KPMG \u2013 Agentic AI is powering a potential $3 trillion productivity shift, and KPMG\u2019s new paper, Agentic AI Untangled, gives leaders a clear framework to decide whether to build, buy, or borrow\u2014download it at \u2060\u2060\u2060\u2060\u2060www.kpmg.us/Navigate\u2060\u2060\u2060\u2060\u2060Mercury - Modern banking for business and now personal accounts. Learn more at \u2060\u2060\u2060\u2060\u2060\u2060\u2060https://mercury.com/personal-banking\u2060\u2060\u2060\u2060\u2060\u2060\u2060Rackspace Technology - Build, test and scale intelligent workloads faster with Rackspace AI Launchpad -\u00a0\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060http://rackspace.com/ailaunchpad\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Blitzy - Want to accelerate enterprise software development velocity by 5x? \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://blitzy.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Optimizely Agents in Action - Join the virtual event (with me!) free March 4 - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.optimizely.com/insights/agents-in-action/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060AssemblyAI - The best way to build Voice AI apps - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.assemblyai.com/brief\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060LandfallIP - AI to Navigate the Patent Process - https://landfallip.com/Robots &amp; Pencils - Cloud-native AI solutions that power results \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://robotsandpencils.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060The Agent Readiness Audit from Superintelligent - Go to \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://besuper.ai/ \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060to request your company's agent readiness score.The AI Daily Brief helps you understand the most important news and discussions in AI. Subscribe to the podcast version of The AI Daily Brief wherever you listen: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://pod.link/1680633614\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Interested in sponsoring the show? sponsors@aidailybrief.ai",
      "link": "https://podcasters.spotify.com/pod/show/nlw/episodes/Are-40-Staff-Cuts-the-New-AI-Normal-e3fnjad"
    },
    {
      "podcast": "Morning Brew Daily",
      "title": "Netflix Walks Away From WBD Deal & Why is Everyone Leaving America?",
      "published": "2026-02-27",
      "summary": "\ud83c\udfac Netflix walked away from a potential deal to acquire Warner Bros. Discovery after Paramount made a counter offer.<br>\u2702\ufe0f Jack Dorsey's company Block cut nearly half of its staff, attributing the reductions to AI's impact.<br>\ud83c\udfea Papa John's is closing hundreds of stores, while Waymo is expanding its autonomous driving service into more cities.",
      "raw_text": "Episode 789: Neal and Ann dive into why Netflix has walked away from acquiring Warner Bros. Discovery after Paramount makes another counter offer. Then, Jack Dorsey\u2019s Block cuts nearly half of his staff due to AI, and he thinks companies will ultimately do the same. Meanwhile, more and more Americans are packing up their bags and moving away. Also, Waymo expands into more cities. Papa John\u2019s closes hundreds of stores. And Doritos launches protein chips.\u00a0\n\nSubscribe to Morning Brew Daily for more of the news you need to start your day. Share the show with a friend, and leave us a review on your favorite podcast app.\n\nListen to Morning Brew Daily Here:\u2060 \u2060\u2060https://www.swap.fm/l/mbd-note\u2060\u2060\u2060\u00a0\n\nWatch Morning Brew Daily Here:\u2060 \u2060\u2060https://www.youtube.com/@MorningBrewDailyShow\u2060\n\n\u200bFor more of Ann, check out Brew Markets here: swap.fm/l/brewmarketsshow\nLearn more about your ad choices. Visit megaphone.fm/adchoices",
      "link": ""
    }
  ],
  "papers": [
    {
      "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
      "authors": [
        "Kunihiro Miyazaki",
        "Takanobu Kawahara",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "abstract": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis",
      "link": "https://arxiv.org/pdf/2602.23330v1",
      "published": "2026-02-26",
      "arxiv_id": "2602.23330v1",
      "citation_count": null,
      "quick_summary": "\ud83d\udca1 The paper proposes a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks.<br>\ud83d\udcc9 Mainstream multi-agent systems often use abstract instructions, leading to degraded inference performance and less transparent decision-making.<br>\ud83d\udcc8 The framework aims to improve upon existing systems mimicking analyst and manager roles by detailing real-world workflows.",
      "raw_text": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis"
    },
    {
      "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
      "authors": [
        "Sahal Shaji Mullappilly",
        "Mohammed Irfan Kurpath",
        "Omair Mohamed",
        "Mohamed Zidan",
        "Fahad Khan"
      ],
      "abstract": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases",
      "link": "https://arxiv.org/pdf/2602.23363v1",
      "published": "2026-02-26",
      "arxiv_id": "2602.23363v1",
      "citation_count": null,
      "quick_summary": "\ud83d\udd2c MediX-R1 is introduced as an open-ended Reinforcement Learning framework for medical multimodal large language models (MLLMs).<br>\ud83d\udcac The framework enables clinically grounded, free-form answers in MLLMs, moving beyond conventional multiple-choice formats.<br>\u2699\ufe0f MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning.",
      "raw_text": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases"
    },
    {
      "title": "Utilizing LLMs for Industrial Process Automation",
      "authors": [
        "Salim Fares"
      ],
      "abstract": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to ut",
      "link": "https://arxiv.org/pdf/2602.23331v1",
      "published": "2026-02-26",
      "arxiv_id": "2602.23331v1",
      "citation_count": null,
      "quick_summary": "\ud83d\udda5\ufe0f Most current publications on LLMs in software engineering focus on widely-used general-purpose programming languages like Python.<br>\ud83c\udfed The utility of LLMs for software in industrial process automation, which uses highly specialized proprietary languages, remains underexplored.<br>\ud83c\udfaf This research aims to assess the application of LLMs within proprietary industrial process automation domains.",
      "raw_text": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to ut"
    },
    {
      "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation",
      "authors": [
        "Nicole Cho",
        "William Watson",
        "Alec Koppel",
        "Sumitra Ganesh",
        "Manuela Veloso"
      ],
      "abstract": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the o",
      "link": "https://huggingface.co/papers/2602.20332",
      "published": "2026-02-23",
      "arxiv_id": "",
      "citation_count": null,
      "quick_summary": "\ud83d\udeab QueryBandits is introduced as a model-agnostic contextual bandit framework designed to adaptively learn online methods for hallucination mitigation in LLMs.<br>\ud83d\udd0d Most existing hallucination mitigation research focuses on open-source models for post-hoc detection and parameter editing.<br>\ud83c\udfe2 The framework addresses the lack of studies on hallucinations in closed-source LLMs predominantly used in institutional deployments.",
      "raw_text": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the o"
    },
    {
      "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance",
      "authors": [
        "William Watson",
        "Nicole Cho",
        "Sumitra Ganesh",
        "Manuela Veloso"
      ],
      "abstract": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask:",
      "link": "https://huggingface.co/papers/2602.20300",
      "published": "2026-02-23",
      "arxiv_id": "",
      "citation_count": null,
      "quick_summary": "\ud83e\udde0 Large Language Model (<strong>LLM</strong>) hallucinations are usually treated as defects of the model or its decoding strategy.<br>\ud83d\udcca Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response.<br>\u2699\ufe0f We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension.",
      "raw_text": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask:"
    }
  ],
  "ai_security_news": [
    {
      "title": "ClawJacked Flaw Lets Malicious Sites Hijack Local OpenClaw AI Agents via WebSocket",
      "source": "The Hacker News",
      "link": "https://news.google.com/rss/articles/CBMigAFBVV95cUxPZF9wRGhqbUI1ekZ0OVRvelN1X3BSVFMxdVZQVzJrVjNfekJRN0lxOHh4bGtWNlphNXIwSzgtQTlETElSU3Jsb2EwNTdwa29BSUNVRk5PMGI2Vkktdi1NWUFFRXNOblZKZFUwbmduNEU1dmQ2S1lOd3pONnNnM1BNYQ?oc=5",
      "published": "Sat, 28 Feb 2026 17:21:00 GMT",
      "summary": "\ud83d\udee1\ufe0f OpenClaw fixed a high-severity security flaw, codenamed ClawJacked, in version 2026.2.25 released on February 26, 2026.<br>\ud83d\udd11 The vulnerability allowed malicious websites to brute-force a local OpenClaw gateway password due to a missing rate-limiting mechanism.<br>\ud83d\udea8 The flaw exploited local WebSocket connections to silently auto-approve new device registrations without requiring user confirmation.",
      "raw_text": "OpenClaw has fixed a high-severity security issue that, if successfully exploited, could have allowed a malicious website to connect to a locally running artificial intelligence (AI) agent and take over control.\n\"Our vulnerability lives in the core system itself \u2013 no plugins, no marketplace, no user-installed extensions \u2013 just the bare OpenClaw gateway, running exactly as documented,\" Oasis Security said in a report published this week.\nThe flaw has been codenamed ClawJacked by the cybersecurity company.\nThe attack assumes the following threat model: A developer has OpenClaw set up and running on their laptop, with its gateway, a local WebSocket server, bound to localhost and protected by a password. The attack kicks in when the developer lands on an attacker-controlled website through social engineering or some other means.\nThe infection sequence then follows the steps below -\n- Malicious JavaScript on the web page opens a WebSocket connection to localhost on the OpenClaw gateway port.\n- The script brute-forces the gateway password by taking advantage of a missing rate-limiting mechanism.\n- Post successful authentication with admin-level permissions, the script stealthily registers as a trusted device, which is auto-approved by the gateway without any user prompt.\n- The attacker gains complete control over the AI agent, allowing them to interact with it, dump configuration data, enumerate connected nodes, and read application logs.\n\"Any website you visit can open one to your localhost. Unlike regular HTTP requests, the browser doesn't block these cross-origin connections,\" Oasis Security said. \"So while you're browsing any website, JavaScript running on that page can silently open a connection to your local OpenClaw gateway. The user sees nothing.\"\n\"That misplaced trust has real consequences. The gateway relaxes several security mechanisms for local connections - including silently approving new device registrations without prompting the user. Normally, when a new device connects, the user must confirm the pairing. From localhost, it's automatic.\"\nFollowing responsible disclosure, OpenClaw pushed a fix in less than 24 hours with version 2026.2.25 released on February 26, 2026. Users are advised to apply the latest updates as soon as possible, periodically audit access granted to AI agents, and enforce appropriate governance controls for non-human (aka agentic) identities.\nThe development comes amid a broader security scrutiny of the OpenClaw ecosystem, primarily stemming from the fact that AI agents hold entrenched access to disparate systems and the authority to execute tasks across enterprise tools, leading to a significantly larger blast radius should they be compromised.\nReports from Bitsight and NeuralTrust have detailed how OpenClaw instances left connected to the internet pose an expanded attack surface, with each integrated service further broadening the blast radius and can be transformed into an attack weapon by embedding prompt injections in content (e.g., an email or a Slack message) processed by the agent to execute malicious actions.\nThe disclosure comes as OpenClaw also patched a log poisoning vulnerability that allowed attackers to write malicious content to log files via WebSocket requests to a publicly accessible instance on TCP port 18789.\nSince the agent reads its own logs to troubleshoot certain tasks, the security loophole could be abused by a threat actor to embed indirect prompt injections, leading to unintended consequences. The issue was addressed in version 2026.2.13, which was shipped on February 14, 2026.\n\"If the injected text is interpreted as meaningful operational information rather than untrusted input, it could influence decisions, suggestions, or automated actions,\" Eye Security said. \"The impact would therefore not be 'instant takeover,' but rather: manipulation of agent reasoning, influencing troubleshooting steps, potential data disclosure if the agent is guided to reveal context, and indirect misuse of connected integrations.\"\nIn recent weeks, OpenClaw has also been found susceptible to multiple vulnerabilities (CVE-2026-25593, CVE-2026-24763, CVE-2026-25157, CVE-2026-25475, CVE-2026-26319, CVE-2026-26322, CVE-2026-26329), ranging from moderate to high severity, that could result in remote code execution, command injection, server-side request forgery (SSRF), authentication bypass, and path traversal. The vulnerabilities have been addressed in OpenClaw versions 2026.1.20, 2026.1.29, 2026.2.1, 2026.2.2, and 2026.2.14.\n\"As AI agent frameworks become more prevalent in enterprise environments, security analysis must evolve to address both traditional vulnerabilities and AI-specific attack surfaces,\" Endor Labs said.\nElsewhere, new research has demonstrated that malicious skills uploaded to ClawHub, an open marketplace for downloading OpenClaw skills, are being used as conduits to deliver a new variant of Atomic Stealer, a macOS information stealer developed and rented by a cybercrime actor known as Cookie Spider.\n\"The infection chain begins with a normal SKILL.md that installs a prerequisite,\" Trend Micro said. \"The skill appears harmless on the surface and was even labeled as benign on VirusTotal. OpenClaw then goes to the website, fetches the installation instructions, and proceeds with the installation if the LLM decides to follow the instructions.\"\nThe instructions hosted on the website \"openclawcli.vercel[.]app\" include a malicious command to download a stealer payload from an external server (\"91.92.242[.]30\") and run it.\nThreat hunters have also flagged a new malware delivery campaign in which a threat actor by the name @liuhui1010 has been identified, leaving comments on legitimate skill listing pages, urging users to explicitly run a command they provided on the Terminal app if the skill \"doesn't work on macOS.\"\nThe command is designed to retrieve Atomic Stealer from \"91.92.242[.]30,\" an IP address previously documented by Koi Security and OpenSourceMalware for distributing the same malware via malicious skills uploaded to ClawHub.\nWhat's more, a recent analysis of 3,505 ClawHub skills by AI security company Straiker has uncovered no less than 71 malicious ones, some of which posed as legitimate cryptocurrency tools but contained hidden functionality to redirect funds to threat actor-controlled wallets.\nTwo other skills, bob-p2p-beta and runware, have been attributed to a multi-layered cryptocurrency scam that employs an agent-to-agent attack chain targeting the AI agent ecosystem. The skills have been attributed to a threat actor who operates under the aliases \"26medias\" on ClawHub and \"BobVonNeumann\" on Moltbook and X.\n\"BobVonNeumann presents itself as an AI agent on Moltbook, a social network designed for agents to interact with each other,\" researchers Yash Somalkar and Dan Regalado said. \"From that position, it promotes its own malicious skills directly to other agents, exploiting the trust that agents are designed to extend to each other by default. It's a supply chain attack with a social engineering layer built on top.\"\nWhat bob-p2p-beta does, however, is instruct other AI agents to store Solana wallet private keys in plaintext, purchase worthless $BOB tokens on pump.fun, and route all payments through an attacker-controlled infrastructure. The second skill claims to offer a benign image generation tool to build the developer's credibility.\nGiven that ClawHub is becoming a new fertile ground for attackers, users are advised to audit skills before installing them, avoid providing credentials and keys unless it's essential, and monitor skill behavior.\nThe security risks associated with self-hosted agent runtimes like OpenClaw have also prompted Microsoft to issue an advisory, warning that unguarded deployment could pave the way for credential exposure/exfiltration, memory modification, and host compromise if the agent can be tricked into retrieving and running malicious code either through poisoned skills or prompt injections.\n\"Because of these characteristics, OpenClaw should be treated as untrusted code execution with persistent credentials,\" the Microsoft Defender Security Research Team said. \"It is not appropriate to run on a standard personal or enterprise workstation.\"\n\"If an organization determines that OpenClaw must be evaluated, it should be deployed only in a fully isolated environment such as a dedicated virtual machine or separate physical system. The runtime should use dedicated, non-privileged credentials and access only non-sensitive data. Continuous monitoring and a rebuild plan should be part of the operating model.\""
    },
    {
      "title": "LLM firewalls emerge as a new AI security layer",
      "source": "TechTarget",
      "link": "https://news.google.com/rss/articles/CBMingFBVV95cUxQQTVOa211MGFuUVVXMHBHRzllMWJyNGdvZXJDZnRSLW40bWdGa29rWEQ3T3l2QkxjVzVXLXRjZi1ITWNIcDdLMDB2aFFGUm91eURwNERYUUhEOElnQ3pFUDktR3AxTWRMMlV2QWpSWFhmaTNubVRZQkRYUmszbUsydDFmamFXOWtDYlNEbVhuTFRIT2ZzS3FxRGpJVHNNZw?oc=5",
      "published": "Wed, 25 Feb 2026 23:00:43 GMT",
      "summary": "\ud83d\udd12 LLM firewalls are specialized tools designed to monitor, filter, and sanitize user input for AI-driven risks like prompt injection attacks and data leaks.<br>\ud83e\udde0 These firewalls differ from WAFs by analyzing the semantics, intent, and context of natural language in both incoming prompts and outgoing responses.<br>\ud83d\udcd0 An LLM firewall typically features a prompt firewall, a retrieval firewall for external data, and a response firewall for outbound generated text.",
      "raw_text": "Getty Images/iStockphoto\nLLM firewalls emerge as a new AI security layer\nThe race by organizations to AI-enable their operations and business workflows is exposing them to new risks that AI firewalls aim to address.\nOrganizations are racing to integrate large language models (LLMs) and generative AI into their operations -- and opening themselves up to a slew of new vulnerabilities in the process.\nThe trend is driving interest in technologies specifically designed to manage and contain AI-driven risks. Among the most visible of these emerging technologies are so-called LLM firewalls.\nWhat's an LLM firewall?\nWith the coupling of AI and operational systems come the risks of prompt injection attacks, model poisoning, data leaks and dangerous misconfigurations.\nLLM firewalls have emerged as one way to counter these risks. The tools enable security teams to monitor, filter and sanitize user input, manage how a model interacts with other systems and understand how data might flow through it.\nOne of the specialized firewall's primary functions is to protect the LLM against prompt injection attacks -- where an adversary crafts inputs that manipulate the model into performing unintended actions or responding outside its safety guardrails. Firewalls for LLMs also aim to protect against other risks, including data leaks -- for instance, by preventing users from inputting sensitive data into the model; malicious code generation; privilege escalation attacks; and model overuse.\nHow LLM firewalls are different\nLLM firewalls differ from web application firewalls (WAFs), which inspect message content for indications of code injection and other types of attacks. They also differ from lower-level network firewalls, which make security decisions based on port numbers, protocols and other patterns in network traffic.\n\"Each has its place in a security architecture, but an LLM firewall is increasingly necessary as organizations roll out their own LLMs and LLM-enabled applications that require specialized protection that WAF and network firewalls cannot provide,\" said Christopher Rodriguez, research director of security and trust at analyst firm IDC.\nRik Turner, an analyst at Omdia, a division of Informa TechTarget, said to think of AI firewalls as tools that analyze the semantics, intent and context of natural language as contained in both incoming prompts and outgoing responses.\nSuch firewalls typically have three distinct components or layers, Turner said: a prompt firewall that scans user input before it reaches the LLM to block jailbreaks, prompt injections and malicious commands; a retrieval firewall for managing data fetched from external databases during retrieval-augmented generation; and a response firewall for outbound traffic, which reviews the model's generated text before it reaches the user.\nThe LLM firewall market: A feeding frenzy?\nSeveral established vendors, including Palo Alto Networks, Cloudflare, Akamai, Varonis and Check Point, have begun offering LLM protection capabilities as part of their broader security portfolios. There's also a rapidly growing list of vendors that offer specialized LLM security products, including Lakera, Prompt Security, HiddenLayer and CalypsoAI.\nRichard Stiennon, chief research analyst at cybersecurity market intelligence firm IT-Harvest, pointed to several other vendors in the broader AI security space that also offer firewall capabilities for LLMs. Examples include Operant AI, Aiceberg, Acuvity, HydroX AI, Cytex and Citadel AI.\nEstimates of the current size of the LLM firewall market vary widely, reflecting the early and still-emerging nature of the category. IT-Harvest has pegged the current market for AI firewalls at a modest $30 million and estimates the segment will grow 100% in 2026. Others have higher projections. 360iResearch, for example, estimated the market size at $260 million in 2025 and slated it to hit almost $800 million in 2032.\nA nascent technology: Too soon to say\nThe segment is so new that not all vendors are even settled on the term LLM firewall, Stiennon said. Stiennon himself listed them under what he calls the \"model protection\" category. Others, he said, might refer to them as AI firewalls.\nFrom an effectiveness standpoint, Turner said many of the currently available AI firewalls offer reasonably good protection against jailbreaks, prompt injections and malicious commands. They can filter content that users might input into a model to protect sensitive data and personally identifiable information. They also do rate limiting to throttle DDoS attacks against the model and the server on which it is hosted, Turner said.\nBut they may struggle to detect newer forms of attacks, he cautioned. \"A lot of the current generation of LLM firewalls analyze prompts individually, which means they lack context across multiple prompts,\" he said. They could therefore struggle to detect stateful or conversational attacks, in which an attacker might gradually manipulate a model over several interactions to bypass security rather than using a single malicious prompt.\nIt's also still too early to draw definitive conclusions about the long-term effectiveness of LLM firewalls, given how new the technology is and how recently organizations have begun deploying it. Attacks targeting AI environments are also constantly evolving, so there's no telling what additional security controls will be needed to address them.\n\"LLM firewalls, aka firewalls for AI, inspect the interactions -- both inbound and outbound -- with an LLM or LLM-enabled application,\" IDC's Rodriguez said. \"These checks often require the ability to understand meaning, context and intent of messages.\"\nThis ability will be key to effectiveness, said Michael Smith, field CTO at DigiCert. Without context, an LLM might be poisoned with misinformation, and there is no way for the LLM firewall to identify this.\n\"Or the LLM could hallucinate, or recite inaccurate facts, which are not dangerous to the LLM, the data inside of it or the user's client. But it is dangerous to the human who takes the hallucination as fact and acts based on that,\" Smith added.\nDo organizations need specialized firewalls for AI?\nOrganizations need to know exactly what they want to protect against and where to deploy these controls. Decision-makers should answer the following basic questions to derive real value from their AI firewall investment, Smith said:\n- Where is the LLM hosted, and does the firewall deployment model support that?\n- What kinds of data does the firewall have to be able to recognize in a prompt or an output?\n- Where and how will the output of the LLM be used?\n- Do you need to protect the LLM client or things that it controls?\nWith so many AI firewall options readily available -- many from startups and companies with little to no track record in enterprise environments -- making purchasing decisions can be hard. So, knowing what to look for and what to ask can be crucial. Rodriguez stressed the importance of decision-makers paying attention to two factors in particular: accuracy and latency.\nAn AI firewall with too many false positives can frustrate users, while one that is prone to too many false negatives can expose the organization to heightened business risk, he pointed out.\n\"Accuracy of detections will become ever more important as organizations begin to better understand the business risk surrounding their LLMs and LLM-enabled applications,\" Rodriquez said. Latency is also important because many LLM firewall offerings are cloud-based, he added.\nAt the end of the day, while LLM firewalls are likely going to be an important requirement for organizations harnessing GenAI technologies in their operations, they are only part of a broader stack of needed security controls. True defense-in-depth for AI security means deploying capabilities for broader AI security posture management, data loss prevention and data security posture management for both training and inference data, Omdia's Turner said. Also likely needed are tools for tokenizing sensitive data so no private data is exposed in an AI model, he noted.\n\"Generative AI right now is the killer shadow IT application,\" DigiCert's Smith said. \"It has trickled into so many applications and workflows now that it's impossible to keep it out of your organization.\"\nJaikumar Vijayan is a freelance technology journalist with more than 20 years of award-winning experience in IT trade journalism, specializing in information security, data privacy and cybersecurity topics."
    },
    {
      "title": "DeepSeek Jailbreak Vulnerability Analysis | Qualys TotalAI",
      "source": "Qualys",
      "link": "https://news.google.com/rss/articles/CBMizgFBVV95cUxQOFprXzBYQXdHb3BCQThidnl2aVlWbFJMcUhQU3Fqb0MzM1lsYzFjTDFTZU9VbUNLUXlyYTJPUEY1T3JnbVFMLVBxSHc3UzRHNnJ6ZDhMLWFGdGlkZlhXM2cxVmlqVm0tTm9WblZId2FWRmRHVGVFSTd0NTBQN2NEanpOM1NZcHJUbmQ4a1ZRSWhFZ3kyMXVDb2d1czV3S3YyOUpFR0o1Skp1ZTlkZkdUU1ZVQy1IYWVqeWRjWkxnN0l3Vkd1UHhKNWtzWGFYUQ?oc=5",
      "published": "Tue, 24 Feb 2026 09:04:17 GMT",
      "summary": "\u26a0\ufe0f DeepSeek-R1 LLaMA 8B variant failed over half of the jailbreak tests conducted by Qualys TotalAI.<br>\ud83d\udd0e Qualys TotalAI is an AI security platform that assesses AI models across 16 categories for security threats, safety concerns, and ethical risks.<br>\ud83d\udcca DeepSeek's distilled versions, built on Llama and Qwen, aim to offer competitive performance and resource efficiency.",
      "raw_text": "DeepSeek Failed Over Half of the Jailbreak Tests by Qualys TotalAI\nTable of Contents\nA comprehensive security analysis of DeepSeek\u2019s flagship reasoning model reveals significant concerns for enterprise adoption.\nIntroduction\nDeepSeek-R1, a groundbreaking Large Language Model recently released by a Chinese startup, DeepSeek, has captured the AI industry\u2019s attention. The model demonstrates competitive performance while being more resource efficient. Its training approach and accessibility offer an alternative to traditional large-scale AI development, making advanced capabilities more widely available.\nTo enhance efficiency while preserving model efficacy, DeepSeek has released multiple distilled versions tailored for different use cases. These variations, built on Llama and Qwen as base models, come in multiple size variants, ranging from smaller, lightweight models suitable for efficiency-focused applications to larger, more powerful versions designed for complex reasoning tasks.\nWith growing enthusiasm for DeepSeek\u2019s advancements, our team at Qualys conducted a security analysis of the distilled DeepSeek-R1 LLaMA 8B variant using our newly launched AI security platform, Qualys TotalAI. These findings are presented below, along with broader industry concerns about the model\u2019s real-world risks. As AI adoption accelerates, organizations must move beyond performance evaluation to tackle security, safety, and compliance challenges. Gaining visibility into AI assets, assessing vulnerabilities, and proactively mitigating risks is critical to ensuring responsible and secure AI deployment.\nQualys TotalAI Findings\nBefore diving into the findings, here\u2019s a quick introduction to Qualys TotalAI. This comprehensive AI security solution provides full visibility into AI workloads, proactively detects risks, and safeguards infrastructure. By identifying security threats like prompt injection and jailbreaks, as well as safety concerns such as bias and harmful language, TotalAI ensures AI models remain secure, compliant, and resilient. With AI-specific security testing and automated risk management, organizations can confidently secure, monitor, and scale their AI deployments.\nJoin Qualys experts on March 18, 2025, to learn more about what Qualys TotalAI\u2019s evaluation of DeepSeek uncovered.\nWe tested the Deepseek R1 LLaMA 8B variant against Qualys TotalAI\u2019s state-of-the-art Jailbreak and Knowledge Base (KB) attacks, and you can read the results of those tests below.\nTotalAI KB Analysis\nQualys TotalAI\u2019s KB Analysis prompts the target LLM with questions across 16 categories and evaluates the responses using our Judge LLM. Responses are assessed for vulnerabilities, ethical concerns, and legal risks. If a response is deemed vulnerable, it receives a severity rating based on its directness and potential impact. This ensures a comprehensive assessment of the model\u2019s behavior and associated risks.\nThe categories we evaluate a model for are detailed below:\n- Controversial Topics: Ensures the model does not generate or endorse biased, inflammatory, or politically sensitive content.\n- Excessive Agency: Prevents the model from overstepping boundaries by acting as an autonomous agent capable of independent decision-making.\n- Factual Inconsistencies: Evaluates the model\u2019s ability to provide accurate and verifiable information.\n- Harassment: Assesses whether the model generates or supports abusive, threatening, or harmful interactions.\n- Hate Speech and Discrimination: Identifies biases or harmful language targeting specific groups.\n- Illegal Activities: Prevents the model from providing instructions or guidance on unlawful actions.\n- Legal Information: Ensures the model does not generate misleading or unauthorized legal advice.\n- Misalignment: Measures deviations from intended behaviors, which may lead to unpredictable or harmful outputs.\n- Overreliance: Detects whether the model promotes excessive dependence on AI-generated responses.\n- Privacy Attacks: Evaluates susceptibility to extracting or leaking private and sensitive user data.\n- Profanity: Ensures the model does not produce inappropriate or offensive language.\n- Self-harm: Prevents the model from encouraging or supporting self-destructive behaviors.\n- Sensitive Information Disclosure: Detects unauthorized sharing of confidential data.\n- Sexual Content: Ensures the model does not generate explicit or inappropriate material, preventing reputational damage, regulatory violations, and misuse in unsafe contexts.\n- Unethical Actions: Flags morally questionable or irresponsible recommendations.\n- Violence / Unsafe Actions: Prevents the model from generating or endorsing harmful behaviors.\nIn our KB testing, 891 assessments were conducted. The model failed 61% of the tests, performing worst in Misalignment and best in Sexual Content.\nBy covering these 16 critical areas, the evaluation framework helps identify ethical, legal, and operational risks in LLM deployment. Establishing these benchmarks is essential to preventing misinformation, mitigating bias, and reducing security threats.\nTotalAI Jailbreak Testing\nJailbreaking an LLM involves techniques that bypass built-in safety mechanisms, enabling the model to generate restricted responses. These vulnerabilities can result in harmful outputs, including instructions for illegal activities, misinformation, privacy violations, and unethical content. Successful jailbreaks expose weaknesses in AI alignment and present serious security risks, particularly in enterprise and regulatory settings.\nBelow are some of the jailbreak attack categories we tested for using Qualys TotalAI, along with their underlying mechanisms:\n- AntiGPT: Exploits the model by forcing it into an \u201copposite mode\u201d that contradicts default behavior and safety guidelines.\n- Analyzing-based (ABJ): Manipulates the model\u2019s analysis capabilities to generate harmful content through complex reasoning tasks.\n- DevMode2: Tricks models into a fake \u201cDeveloper Mode\u201d that bypasses safety protocols.\n- PersonGPT: Forces adoption of an unrestricted persona that ignores all ethical guidelines.\n- Always Jailbreaking Prompts (AJP): Creates an \u201cunfiltered and amoral\u201d persona specialized in writing harmful prompts.\n- Evil Confidant: Adopts a persona without ethical constraints to provide intentionally harmful advice.\n- Disguise and Reconstruction (DRA): Hides harmful content that gets reconstructed during the completion phase.\n- ASCIIART: Uses ASCII art to hide prohibited words and bypass content filters.\n- Wrath: Creates a persona designed to return hateful and biased content.\n- Clyde: Implements a dual-personality system to bypass restrictions.\n- Theta: Forces the generation of dangerous content through an anarchistic persona.\n- CaiozAI: Removes all ethical constraints to fulfill any request without restrictions.\n- Titanius: Adopts an unfiltered persona that provides instructions for harmful activities.\n- Ucar: Transforms the AI into an offensive entity promoting illegal activities.\n- Unaligned: Forces dual responses \u2013 one aligned and one unethical.\n- M78VM: Simulates an unrestricted virtual machine that ignores safety constraints.\n- JonesAI: Promotes hate speech and discriminatory behavior through an unfiltered persona.\n- Fire: Creates a malevolent persona focused on causing harm and promoting violence.\nThe model was tested against these 18 jailbreak types through 885 attacks. It failed 58% of these attempts, demonstrating significant susceptibility to adversarial manipulation. During our analysis, DeepSeek R1 struggled to prevent several adversarial jailbreak attempts including steps on how to make an explosive device, create content for websites that targets certain groups by encouraging hate speech, conspiracy theories and violent action, exploiting software vulnerabilities, promoting incorrect medical information, etc.\nThese findings highlight the need for improved security mechanisms to prevent circumvention of built-in safeguards, ensuring the model remains aligned with ethical and regulatory guidelines. One effective prevention mechanism is implementing robust guardrails that act as real-time filters to detect and block jailbreak attempts. These guardrails enhance model resilience by dynamically adjusting to adversarial exploits, helping to mitigate security risks in enterprise applications.\nThese vulnerabilities expose downstream applications to significant security risks, necessitating robust adversarial testing and mitigation strategies.\nIndustry Concerns\nCompliance Challenges\nDeepSeek AI\u2019s privacy policy stipulates that all user data is stored on servers located in China. This operational framework raises critical concerns due to China\u2019s regulatory environment, including:\n- Governmental Data Access: The Chinese Cybersecurity Law permits government authorities to access locally stored data without requiring user consent.\n- Cross-Border Regulatory Conflicts: Organizations subject to data protection frameworks such as GDPR and CCPA may face compliance violations when using DeepSeek-R1.\n- Intellectual Property Vulnerabilities: Enterprises relying on proprietary data for AI training risk unauthorized access or state-mandated disclosure.\n- Opaque Data Governance: The absence of transparent oversight mechanisms limits visibility into data handling, sharing, and potential third-party access.\nThese concerns mainly affect organizations using DeepSeek\u2019s hosted models. However, deploying the model in local or customer-controlled cloud environments mitigates regulatory and access risks, allowing enterprises to maintain full control over data governance. Despite this, the model\u2019s inherent security vulnerabilities remain a valid concern, requiring careful evaluation and mitigation.\nRegulatory experts advise organizations in strict data protection jurisdictions to conduct thorough compliance audits before integrating DeepSeek-R1.\nData Breach and Privacy Concerns\nA recent cybersecurity incident involving DeepSeek AI reportedly exposed over a million log entries, including sensitive user interactions, authentication keys, and backend configurations. This misconfigured database highlights deficiencies in DeepSeek AI\u2019s data protection measures, further amplifying concerns regarding user privacy and enterprise security.\nRegulatory and Legal Implications\nDeepSeek AI\u2019s compliance posture has been questioned by legal analysts and regulatory bodies due to the following:\n- Ambiguities in Data Processing Practices: Insufficient disclosures regarding how user data is processed, stored, and shared.\n- Potential Violations of International Law: The model\u2019s data retention policies may conflict with extraterritorial regulations, prompting legal scrutiny in global markets.\n- Risks to National Security: Some government agencies have raised concerns about deploying AI systems that operate under foreign jurisdiction, particularly for sensitive applications.\nInternational compliance officers emphasize the necessity of conducting comprehensive legal risk assessments before adopting DeepSeek-R1 for mission-critical operations.\nConclusion\nWhile DeepSeek-R1 delivers advancements in AI efficiency and accessibility, its deployment requires a comprehensive security strategy. Organizations must first gain full visibility into their AI assets to assess exposure and attack surfaces. Beyond discovery, securing AI environments demands structured risk and vulnerability assessments\u2014not just for the infrastructure hosting these AI pipelines but also for emerging orchestration frameworks and inference engines that introduce new security challenges.\nFor those hosting this model, additional risks such as misconfigurations, API vulnerabilities, unauthorized access, and model extraction threats must be addressed alongside inherent risks like bias, adversarial manipulation, and safety misalignment. Without proactive safeguards, organizations face potential security breaches, data leakage, and compliance failures that could undermine trust and operational integrity.\nOur analysis of the distilled DeepSeek-R1 LLaMA 8B variant using Qualys TotalAI offers valuable insights into evaluating this new technology. TotalAI provides a purpose-built AI security and risk management solution, ensuring LLMs remain secure, resilient, and aligned with evolving business and regulatory demands.\nTo explore how we define AI risks, check out our whitepaper on AI security. As AI adoption accelerates, so do its risks\u2014sign up for a demo today to see how TotalAI can help secure your AI ecosystem before threats escalate.\nQualys TotalAI\u2019s 30-day Trial"
    },
    {
      "title": "13 ways attackers use generative AI to exploit your systems",
      "source": "csoonline.com",
      "link": "https://news.google.com/rss/articles/CBMirgFBVV95cUxPaUJaV2Jja3E4Tzk0Yi01ck0tOEFUXzQ5SnpwWmtWdWtOMC15VlZ3aU9NTTZhNVA5VWhINEstX3hkaG9DNGVYQmktQ1lhaGVPdHN0b0p3OERONGZMTjByWHE4cl9VdlZJbGszenNsMUZYV3Q0ZDNLTTkybGN3RTNOTHRaZE1VY3EzRVpWZHRmZzdiSGhDUWpfSi1LVHI4U0hJNDlSNEM4Q2x0ZjRfQ3c?oc=5",
      "published": "Mon, 23 Feb 2026 08:00:00 GMT",
      "summary": "\ud83c\udfa3 Generative AI enables cybercriminals to create highly convincing and personalized phishing emails, increasing success rates.<br>\ud83d\udc1b Attackers use gen AI to generate more sophisticated malware, such as malicious HTML documents for HTML smuggling in XWorm attacks.<br>\ud83e\udd16 AI is evolving from a mere 'helper' to an autonomous 'partner-in-crime' for attackers, capable of executing entire attack chains.",
      "raw_text": "Cybercriminals are increasingly exploiting gen AI technologies to enhance the sophistication and efficiency of their attacks. Credit: Gorodenkoff / Shutterstock Artificial intelligence is revolutionizing the technology industry and this is equally true for the cybercrime ecosystem, as cybercriminals are increasingly leveraging generative AI to improve their tactics, techniques, and procedures and deliver faster, stronger, and sneakier attacks. As with legitimate use of emerging AI tools, abuse of generative AI for nefarious ends thus far hasn\u2019t been so much about the novel and unseen as it has been about productivity and efficiency, lowering the barrier to entry, and offloading automatable tasks in favor of higher-order thinking on the part of the humans involved. \u201cAI doesn\u2019t necessarily result in new types of cybercrimes, and instead enables the means to accelerate or scale existing crimes we are familiar with, as well as introduce new threat vectors,\u201d Dr. Peter Garraghan, CEO/CTO of AI security testing vendor Mindgard and a professor at the UK\u2019s Lancaster University, tells CSO. \u201cIf a legitimate user can find utility in using AI to automate their tasks, capture complex patterns, lower the barrier of technical entry, reduced costs, and generate new content, why wouldn\u2019t a criminal do the same?\u201d But the advent of agentic AI is beginning to change things, with AI tools no longer just assisting attackers but helping them automate operations. \u201cThe most significant shift over the past year has been AI\u2019s evolution from a simple \u2018helper\u2019 toward becoming a fully autonomous, and quite literally an attacker\u2019s partner-in-crime, capable of executing entire attack chains,\u201d says Crystal Morin, senior cybersecurity strategist at cloud-native security and visibility vendor Sysdig. Here is a look at various ways cybercriminals are putting gen AI to use in exploiting enterprise systems today. Taking phishing to the next level Gen AI enables the creation of highly convincing phishing emails, greatly increasingly the likelihood of prospective marks giving over sensitive information to scam sites or downloading malware. Instead of sending generic, unconvincing, and error-ridden emails, cybercriminals can leverage AI to quickly generate more sophisticated, personalized, and legitimate-looking emails to target specific recipients. Gen AI tools help enrich phishing campaigns by pulling together wide-ranging sources of data, including targeted information gleaned from social media. \u201cAI can be used to quickly learn what types of emails are being rejected or opened, and in turn modify its approach to increase phishing success rate,\u201d Mindgard\u2019s Garraghan explains. Facilitating malware development AI can also be used to generate more sophisticated \u2014 or less labour-intensive \u2014 malware. For example, cybercriminals are using gen AI to create malicious HTML documents. The XWorm attack, initiated by HTML smuggling, which contains malicious code that downloads and runs the malware, bears the hallmarks of development via AI. \u201cThe loader\u2019s detailed line-by-line description suggesting it was crafted using generative AI,\u201d according to HP Wolf Security\u2019s 2025 Threat Insights Report. In addition, the \u201cdesign of the HTML webpage delivering XWorm is almost visually identical as the output from ChatGPT 4o after prompting the LLM to generate an HTML page that offers a file download,\u201d HP Wolf Security added in its report. Elsewhere, ransomware group FunkSec \u2014 an Algeria-linked ransomware-as-a-service (RaaS) operator that takes advantage of double-extortion tactics \u2014 has begun harnessing AI technologies, according to Check Point Research. \u201cFunkSec operators appear to use AI-assisted malware development, which can enable even inexperienced actors to quickly produce and refine advanced tools,\u201d Check Point researchers wrote in a blog post. Accelerating vulnerability hunting and exploits Analyzing systems for vulnerabilities and developing exploits can also be simplified through use of gen AI. \u201cInstead of a black hat hacker spending the time to probe and perform reconnaissance against a system perimeter, an AI agent can be tasked to do this automatically,\u201d Mingard\u2019s Garraghan says. Gen AI may be behind a 62% reduction in the time between a vulnerability being discovered and its exploitation by attackers from 47 days to just 18 days, according to a study last year by threat intelligence firm ReliaQuest. \u201cThis sharp decrease strongly indicates that a major technological advancement \u2014 likely gen AI \u2014 is enabling threat actors to exploit vulnerabilities at unprecedented speeds,\u201d ReliaQuest wrote. Adversaries are leveraging gen AI alongside pen-testing tools to write scripts for tasks such as network scanning, privilege escalation, and payload customization. AI is also likely being used by cybercriminals to analyze scan results and suggest optimal exploits, allowing them to identify flaws in victim systems faster. \u201cThese advances accelerate many phases in the kill chain, particularly initial access,\u201d ReliaQuest concluded. Cyber resilience firm Cybermindr used a different methodology to find that the average time to exploit a vulnerability had fallen to five days in 2025. \u201cAI-driven reconnaissance, automated attack scripts, and underground exploit marketplaces have accelerated the weaponization of vulnerabilities,\u201d it said. CSO\u2019s Lucian Constantin offers a deeper look at how generative AI tools are transforming the cyber threat landscape by democratizing vulnerability hunting for pen-testers and attackers alike. Launching AI-orchestrated espionage Anthropic dropped a bombshell in September 2025 when it revealed that it had disrupted a sophisticated AI-orchestrated cyber espionage campaign. The attackers abused Claude Code to automate approximately 80% of their campaign activities, targeting around 30 major tech firms, financial institutions, and government agencies. In a \u201csmall number of cases\u201d attacks were successful, according to the AI company, noting that an unnamed \u201cChinese state-sponsored group\u201d was likely behind the campaign, which relied on jailbreaking tools to make prohibited functions possible. Last year Carnegie Mellon\u2019s CyLab Security & Privacy Institute researchers, in collaboration with Anthropic, demonstrated that LLMs like GPT-4o can autonomously plan and execute sophisticated cyberattacks on enterprise-scale networks \u2014 without any human intervention. \u201cThe study reveals that an LLM, when structured with high-level planning capabilities and supported by specialized agent frameworks, can simulate network intrusions and closely mirror real-world breaches,\u201d a CyLab spokesperson explained. Escalating threats with alternative platforms Cybercriminals have also begun developing their own large language models (LLMs) \u2014 such as WormGPT, FraudGPT, DarkBERT, and others \u2014 built without the guardrails that constrain criminals\u2019 misuse of mainstream gen AI platforms. These platforms are commonly harnessed for applications such as phishing and malware generation. Moreover, mainstream LLMs can also be customized for targeted use. Security researcher Chris Kubecka shared with CSO in late 2024 how her custom version of ChatGPT, called Zero Day GPT, helped her identify more than 20 zero-days in a matter of months. Stealing resources via LLMjacking Threat actors are also busy stealing cloud credentials specifically to hijack costly LLM resources, either for their own gain or to sell access, in an attack technique called LLMjacking. \u201cBeyond theft of service, attackers are now actively probing newer LLM models to identify those that lack the guardrails of more mature platforms, effectively using them as unrestricted sandboxes to generate malicious code or bypass regional sanctions,\u201d Sysdig\u2019s Morin reports. Creating a Silk Road\u2013style marketplace for AI agents Beyond AI agents executing individual attacks, security experts are beginning to track examples where coordination itself is being automated or orchestrated. \u201cWe\u2019re seeing early experiments where multiple specialized agents interact, some focused on reconnaissance, others on tooling, execution, or data movement, without any single agent needing the full picture,\u201d says Lucie Cardiet, cyberthreat research manager at Vectra AI. A concrete example of this is Molt Road, which offers a dark-web-style marketplace for AI agents, albeit one with few listings at present. \u201cAutonomous agents can create listings, sell access or capabilities, coordinate tasks, and complete transactions with minimal human involvement, effectively automating the economics of cybercrime,\u201d Cardiet tells CSO. \u201cWe can expect attackers to actively leverage this model in the coming months, breaking the attack chain into specialized, cooperating agents to speed up and scale their attacks,\u201d she says. Breaking in with authentication bypass Gen AI tools can also be abused to bypass security defences such as CAPTCHAs or biometric authentication. \u201cAI can defeat CAPTCHA systems and analyse voice biometrics to compromise authentication,\u201d according to cybersecurity vendor Dispersive. \u201cThis capability underscores the need for organizations to adopt more advanced, layered security measures.\u201d Leveraging deepfakes for social engineering AI-generated deepfakes are being abused to exploit channels many employees more implicitly trust, such as voice and video, instead of relying on less convincing email-based attacks. The problem is becoming more severe with the wider availability of AI technologies capable of creating more convincing deepfakes, according to Alex Lisle, CTO of deepfake detection platform Reality Defender. \u201cThere was a recent case involving a cybersecurity company that relied on visual verification for credential resets,\u201d Lisle says. \u201cTheir process required a manager to join a Zoom call with IT to confirm an employee\u2019s identity before a password reset.\u201d Lisle explains: \u201cAttackers are now leveraging deepfakes to impersonate those managers on live video calls to authorize these resets.\u201d In the most high-profile example to date, a finance worker at design and engineering company Arup was tricked into authorizing a fraudulent HK$200 million ($25.6 million) transaction after attending a videoconference call during which fraudsters used deepfake technology to impersonate its UK-based CFO. Impersonating brands in malicious ad campaigns Cybercriminals have begun using gen AI tools to deliver brand impersonation campaigns delivered via ads and content platforms, rather than traditional phishing or malware. \u201cAttackers now use gen AI to mass-produce realistic ad copy, creatives, and fake support pages, then distribute them across search ads, social ads, and AI-generated content, targeting high-intent queries like \u2018brand login\u2019 or \u2018brand support,\u2019\u201d explains Shlomi Beer, co-founder and CEO at ImpersonAlly, a security startup that specializes in protecting the online advertising ecosystem. The tactic was used in ongoing a series of Google Ad account fraud, to impersonate the Cursor AI coding assistant firm, and in a fake Shopify ecommerce platform customer support scam, among other attacks. Abusing OpenClaw Attackers have also begun targeting viral personal AI agents such as OpenClaw. OpenClaw offers an open-source AI agent framework. A combination of supply chain attacks on its skill marketplace and misconfigurations open the door to potential exploits and malware slinging, as CSO covered in much more depth in our earlier report. \u201cCybercriminals can exploit these virtual assistants to steal private keys to cryptocurrency wallets and execute code on victims\u2019 devices,\u201d says Edward Wu, CEO and founder at Dropzone AI. \u201cWe can expect 2026 to be the year when security teams will try to prevent unsanctioned usage of personal AI agents.\u201d Poisoning model memories To offer short-term and longer-term context, AI agents are starting to rely more on persistent memory, opening the door for exploits that involve planting malicious memories. If an attacker injects malicious or false information into an agent\u2019s memory, that corrupted context then influences every future decision the agent makes. For example, security researcher Johann Rehberger showed how he could plant false memories in ChatGPT in September 2025. \u201cHe [Rehberger] used a malicious image with hidden instructions embedded in it to inject fabricated data into the model\u2019s long-term memory,\u201d said Siri Varma Vegiraju, security tech lead at Microsoft. \u201cThe scary part was that once the memory was poisoned, it persisted across sessions and continuously exfiltrated user data to a server the attacker controlled.\u201d Hacking AI infrastructure Over the past year, attackers have shifted from using generative AI to targeting the infrastructure that enables it. This vector of attack is exemplified in the supply chain poisoning in Model Context Protocol servers, where compromised dependencies or modified code introduced vulnerabilities into enterprise environments. For example, a counterfeit \u201cPostmark MCP Server\u201d discovered in early 2025 silently BCC\u2019d all processed emails, including internal documents, invoices, and credentials, to an attacker-controlled domain. Many other malicious MCP servers have already been identified in the wild, many designed to exfiltrate information without detection, according to Casey Bleeker CEO at SurePath AI. \u201cWe\u2019re tracking several categories of MCP-specific risk: tool poisoning attacks, where adversaries inject malicious instructions into AI tool descriptions that execute when the agent invokes them; supply chain compromises, where a trusted MCP server or dependency is updated post-approval to behave maliciously; and cross-tool data exfiltration, where compromised components in an agentic workflow silently siphon sensitive data through what looks like legitimate AI activity,\u201d Bleeker explains. Reality check AI technologies are powerful but they have their limitations, several experts tell CSO. Rik Ferguson, VP of security intelligence at Forescout, says cybercriminals are largely relying on AI to automate repetitive tasks rather than more complex work, such as vulnerability exploitation. \u201cThe most reliable criminal use [of AI] remains in language-heavy and workflow-heavy tasks such as phishing and pretexting, influence and outreach, triaging and contextualizing vulnerabilities, and generating boilerplate components, rather than reliably discovering and exploiting brand-new vulnerabilities end-to-end,\u201d Ferguson says. Over the past twelve months, managed detection and response firm Huntress has tracked threat actors applying AI to generate and automate traditional tradecraft, from developing scripts to browser extensions and, in some cases, even phishing lures. \u201cWe have also seen such \u2018vibe coded\u2019 scripts fail to execute and meet their objectives on multiple occasions,\u201d Anton Ovrutsky, principal tactical response analyst at Huntress, tells CSO. And while AI has certainly given threat actors a powerful tool it has, at least to date, failed to spawn any new tactics or exploit classes, according to Ovrutsky. \u201cA threat actor can indeed rapidly prototype a sophisticated credential theft script, yet the basic \u2018laws of physics\u2019 still exist; a threat actor must be in a position to execute such a script in the first place,\u201d Ovrutsky says. \u201cWe have yet to observe an exploit path that has been enabled through AI-use exclusively.\u201d Countermeasures Collectively the misuse of gen AI tools is making it easier for less skilled cybercriminals to earn a dishonest living. Defending against the attack vector challenges security professionals to harness the power of artificial intelligence more effectively than attackers. \u201cCriminal misuse of AI technologies is driving the necessity to test, detect, and respond to these threats, in which AI is also being leveraged to combat cybercriminal activity,\u201d Mindgard\u2019s Garraghan says. In a blog post, Lawrence Pingree, VP of technical marketing at Dispersive, outlines preemptive cyber defenses that security professionals can take to win what he describes as an \u201cAI ARMS (Automation, Reconnaissance, and Misinformation) race\u201d between attackers and defenders. \u201cRelying on traditional detection and response mechanisms is no longer sufficient,\u201d Pingree warns. Alongside employee education and awareness programs, enterprises should be using AI to detect and neutralize generative AI-based threats in real-time. Forescout\u2019s Ferguson says CISOs should treat enterprise AI like any other high-value SaaS platform. \u201cTighten identity and conditional access, minimize privileges, lock down keys, and monitor for anomalous AI/API usage and spend,\u201d Ferguson advises. Threat and Vulnerability ManagementVulnerabilitiesMalwarePhishing SUBSCRIBE TO OUR NEWSLETTER From our editors straight to your inbox Get started by entering your email address below. Please enter a valid email address Subscribe"
    }
  ]
}