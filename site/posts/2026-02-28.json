{
  "date": "Saturday, February 28, 2026",
  "weather": {
    "current_temp": 34,
    "unit": "F",
    "conditions": "Partly Cloudy",
    "high": 46,
    "low": 33,
    "forecast": "Partly cloudy, with a low around 33. South wind around 3 mph.",
    "hourly": [
      {
        "label": "7am",
        "hour": 7,
        "temp": 36,
        "conditions": "Partly Sunny",
        "wind": "2 mph S",
        "humidity": "79%",
        "precip_chance": "1%"
      },
      {
        "label": "9am",
        "hour": 9,
        "temp": 38,
        "conditions": "Partly Sunny",
        "wind": "5 mph S",
        "humidity": "79%",
        "precip_chance": "1%"
      },
      {
        "label": "3pm",
        "hour": 15,
        "temp": 45,
        "conditions": "Mostly Sunny",
        "wind": "6 mph S",
        "humidity": "70%",
        "precip_chance": "0%"
      },
      {
        "label": "5pm",
        "hour": 17,
        "temp": 45,
        "conditions": "Mostly Sunny",
        "wind": "5 mph SW",
        "humidity": "70%",
        "precip_chance": "0%"
      },
      {
        "label": "7pm",
        "hour": 19,
        "temp": 45,
        "conditions": "Partly Cloudy",
        "wind": "2 mph SW",
        "humidity": "70%",
        "precip_chance": "0%"
      }
    ]
  },
  "news": [
    {
      "title": "Employees at Google and OpenAI support Anthropic\u2019s Pentagon stand in open letter",
      "source": "TechCrunch",
      "link": "https://techcrunch.com/2026/02/27/employees-at-google-and-openai-support-anthropics-pentagon-stand-in-open-letter/",
      "published": "Fri, 27 Feb 2026 16:23:58 +0000",
      "raw_text": "Anthropic has reached a stalemate with the United States Department of War over the military\u2019s request for unrestricted access to the AI company\u2019s technology. But as the Pentagon\u2019s Friday afternoon deadline for Anthropic\u2019s compliance approaches, more than 300 Google employees and over 60 OpenAI employees have signed an open letter urging the leaders of their companies to support Anthropic and refuse this unilateral use.\nSpecifically, Anthropic stood in opposition to the use of AI for domestic mass surveillance and autonomous weaponry. The open letter\u2019s signatories seek to encourage their employers to \u201cput aside their differences and stand together\u201d to uphold the boundaries Anthropic has asserted.\n\u201cThey\u2019re trying to divide each company with fear that the other will give in,\u201d the letter says. \u201cThat strategy only works if none of us know where the others stand.\u201d\nThe letter specifically calls on executives at Google and OpenAI to maintain Anthropic\u2019s red lines against mass surveillance and fully automated weaponry. \u201cWe hope our leaders will put aside their differences and stand together to continue to refuse the Department of War\u2019s current demands.\u201d\nLeaders at the companies have not yet formally reponded to the letter. TechCrunch has reached out to Google and OpenAI for comment.\nHowever, informal statements suggest both companies are sympathetic to Anthropic\u2019s side of the case. In an interview with CNBC on Friday morning, OpenAI CEO Sam Altman said that he doesn\u2019t \u201cpersonally think the Pentagon should be threatening DPA against these companies.\u201d According to a CNN reporter, an OpenAI spokesperson confirmed that the company shares Anthropic\u2019s red lines against autonomous weapons and mass surveillance.\nGoogle DeepMind has not formally addressed the conflict, but Chief Scientist Jeff Dean, presumably speaking as an individual, did express opposition to mass surveillance by the government.\nSave up to $300 or 30% to TechCrunch Founder Summit\n1,000+ founders and investors come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately.\nOffer ends March 13.\nSave up to $300 or 30% to TechCrunch Founder Summit\n1,000+ founders and investors come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately\nOffer ends March 13.\n\u201cMass surveillance violates the Fourth Amendment and has a chilling effect on freedom of expression,\u201d Dean wrote on X. \u201cSurveillance systems are prone to misuse for political or discriminatory purposes.\u201d\nAccording to an Axios report, the military currently can use X\u2019s Grok, Google\u2019s Gemini, and OpenAI\u2019s ChatGPT for unclassified tasks, and has been negotiating with Google and OpenAI to bring its technology over for use in classified work.\nWhile Anthropic has an existing partnership with the Pentagon, the AI company has remained firm in maintaining the boundary that its AI be used for neither mass domestic surveillance, nor fully autonomous weaponry.\nDefense Secretary Pete Hegseth told Anthropic CEO Dario Amodei that if his company doesn\u2019t concede, the Pentagon will either declare Anthropic a \u201csupply chain risk\u201d or invoke the Defense Production Act (DPA) to force the company to comply with military demands.\nIn a statement on Thursday, Amodei maintained his company\u2019s position. \u201cThese latter two threats are inherently contradictory: one labels us a security risk; the other labels Claude as essential to national security,\u201d the statement reads. \u201cRegardless, these threats do not change our position: we cannot in good conscience accede to their request.\u201d",
      "summary": "\u2709\ufe0f Over 300 Google employees and 60 OpenAI employees signed an open letter supporting Anthropic's refusal of AI use for domestic mass surveillance and autonomous weaponry.<br>\u2696\ufe0f Anthropic is in a stalemate with the United States Department of War over its request for unrestricted access to the AI company\u2019s technology by a Friday deadline.<br>\ud83d\udde3\ufe0f OpenAI CEO Sam Altman stated he doesn't \"personally think the Pentagon should be threatening DPA against these companies\" in a CNBC interview."
    },
    {
      "title": "OpenAI announces Pentagon deal after Trump bans Anthropic",
      "source": "NPR",
      "link": "https://www.npr.org/2026/02/27/nx-s1-5729118/trump-anthropic-pentagon-openai-ai-weapons-ban",
      "published": "Fri, 27 Feb 2026 15:03:04 -0500",
      "raw_text": "OpenAI announces Pentagon deal after Trump bans Anthropic\nPresident Trump bans Anthropic from use in government systems\nPresident Trump ordered the U.S. government to stop using the artificial intelligence company Anthropic's products and the Pentagon moved to designate the company a national security risk on Friday, in a sharp escalation of a high-stakes fight over the military's use of AI.\nHours after the president's announcement, rival company OpenAI said it had struck a deal with the Defense Department to provide its own AI technology for classified networks.\nThe administration's decisions cap an acrimonious dispute between Anthropic and the Pentagon over whether the company could prohibit its tools from being used in mass surveillance of American citizens or to power autonomous weapon systems, as part of a military contract worth up to $200 million.\n\"The Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution,\" Trump wrote in a Truth Social post. \"Therefore, I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic's technology. We don't need it, we don't want it, and will not do business with them again!\"\nHe said there would be a six-month phaseout of Anthropic's products.\nTrump's announcement came about an hour before a deadline set by the Pentagon, which had called on Anthropic to back down. Shortly after the deadline passed, Defense Secretary Pete Hegseth said he was labeling Anthropic a supply chain risk to national security, blacklisting it from working with the U.S. military or contractors.\n\"In conjunction with the President's directive for the Federal Government to cease all use of Anthropic's technology, I am directing the Department of War to designate Anthropic a Supply-Chain Risk to National Security. Effective immediately, no contractor, supplier, or partner that does business with the United States military may conduct any commercial activity with Anthropic,\" Hegseth posted on X , using the Pentagon's \"Department of War\" rebranding. \"Anthropic will continue to provide the Department of War its services for a period of no more than six months to allow for a seamless transition to a better and more patriotic service.\"\nAnthropic said it would challenge the supply chain risk designation in court.\n\"We believe this designation would both be legally unsound and set a dangerous precedent for any American company that negotiates with the government,\" the company said in a statement on Friday evening.\nAnthropic also challenged Hegseth's comments that anyone who does business with the U.S. military would have to cut off all business with Anthropic. \"The Secretary does not have the statutory authority to back up this statement,\" the company said. Under federal law, it said, designating Anthropic a supply chain risk would only apply to \"the use of Claude as part of Department of War contracts\u2014it cannot affect how contractors use Claude to serve other customers.\"\nThe company said it had \"tried in good faith\" to reach an agreement with the Pentagon over months of negotiations, \"making clear that we support all lawful uses of AI for national security aside from the two narrow exceptions\" being disputed. \"To the best of our knowledge, these exceptions have not affected a single government mission to date,\" Anthropic said.\nIt said its objections to those uses were rooted in two reasons: \"First, we do not believe that today's frontier AI models are reliable enough to be used in fully autonomous weapons. Allowing current models to be used in this way would endanger America's warfighters and civilians. Second, we believe that mass domestic surveillance of Americans constitutes a violation of fundamental rights.\"\nIn a post on X announcing competitor OpenAI's deal with the Defense Department, the company's CEO Sam Altman, who previously cited similar concerns, said his agreement with the government included safeguards like the ones Anthropic had asked for.\n\"Two of our most important safety principles are prohibitions on domestic mass surveillance and human responsibility for the use of force, including for autonomous weapon systems,\" he said. \"The DoW agrees with these principles, reflects them in law and policy, and we put them into our agreement.\"\nBan comes as Anthropic plans an IPO\nDefense Department officials had given Anthropic a deadline of 5:01 p.m. ET on Friday to drop restrictions on its AI model, Claude, from being used for domestic mass surveillance or entirely autonomous weapons, or face losing its contract. The Pentagon has said it doesn't intend to use AI in those ways, but requires AI companies to allow their models to be used \"for all lawful purposes.\"\nThe government had also threatened to invoke the Korean War-era Defense Production Act to compel Anthropic to allow use of its tools and, at the same time, warned it would label Anthropic a supply chain risk.\nIn his post carrying out the latter threat, Hegseth said Anthropic had \"delivered a master class in arrogance and betrayal as well as a textbook case of how not to do business with the United States Government or the Pentagon.\" He accused the company of trying to \"seize veto power over the operational decisions of the United States military.\"\nHe said the department would not waver from its position: \"the Department of War must have full, unrestricted access to Anthropic's models for every LAWFUL purpose in defense of the Republic.\"\n\"America's warfighters will never be held hostage by the ideological whims of Big Tech. This decision is final,\" Hegseth concluded.\nThe government ban comes at a time when Anthropic is under heightened scrutiny, since the company, which is valued at $380 billion, is planning to go public this year.\nWhile the Pentagon contract worth as much as $200 million is a relatively small portion of Anthropic's $14 billion in revenue, it's unclear how the friction with the administration will sit with investors or affect other deals the company has to license its AI model to non-government partners.\nAnthropic CEO Dario Amodei has pointed out that the company's valuation and revenue have only grown since it took a stand against Trump officials over how AI can be deployed on the battlefield.\nWhether AI companies can set restrictions on how the government uses their technology has emerged as a major sticking point in recent months between Anthropic and the Trump administration.\nOn Thursday, Amodei said the company would not budge in the face of the Pentagon's threats. \"We cannot in good conscience accede to their request,\" he wrote in a lengthy statement.\n\"Anthropic understands that the Department of War, not private companies, makes military decisions. We have never raised objections to particular military operations nor attempted to limit use of our technology in an ad hoc manner,\" he said. But, he added, domestic mass surveillance and fully autonomous weapons are uses that are \"simply outside the bounds of what today's technology can safely and reliably do.\"\nEmil Michael, the Pentagon's undersecretary for research and engineering, shot back in a post on X on Thursday, accusing Amodei of lying and having a \"God-complex.\"\n\"He wants nothing more than to try to personally control the US Military and is ok putting our nation's safety at risk,\" Michael wrote. \"The @DeptofWar will ALWAYS adhere to the law but not bend to whims of any one for-profit tech company,\" he wrote.\nIn an late Thursday interview with CBS News, Michael said federal law and Pentagon policies already bar the use of AI for domestic mass surveillance and autonomous weapons.\"\n\"At some level, you have to trust your military to do the right thing,\" he said.\nOpenAI expressed similar concerns\nOpenAI CEO Altman had said earlier on Friday that he shared Anthropic's \"red lines\" restricting military use of AI.\nOpenAI, Google, and Elon Musk's xAI also have Defense Department contracts and have agreed to allow their AI tools to be used in any \"lawful\" scenarios. Earlier this week, xAI became the second company after Anthropic to be approved for use in classified settings.\nAltman told CNBC on Friday morning that it's important for companies to work with the military \"as long as it is going to comply with legal protections\" and \"the few red lines\" that \"we share with Anthropic and that other companies also independently agree with.\"\nIn an internal note sent to staff on Thursday evening, Altman said OpenAI was seeking to negotiate a deal with the Pentagon to deploy its models in classified systems with exclusions preventing use for surveillance in the U.S. or to power autonomous weapons without human approval, according to a person familiar with the message who was not authorized to speak publicly. The Wall Street Journal first reported Altman's note to staff.\nThe Defense Department didn't respond to a request for comment on Altman's statements.\nIndependent experts say the standoff is highly unusual in the world of Pentagon contracting.\n\"This is different for sure,\" said Jerry McGinn, director of the Center for the Industrial Base at the Center for Strategic and International Studies, a Washington DC think tank. Pentagon contractors don't usually get to tell the Defense Department how their products and services can be used, he notes \"because otherwise you'd be negotiating use cases for every contract, and that's not reasonable to expect.\"\nAt the same time, McGinn noted, artificial intelligence is a new and largely untested technology. \"This is a very unusual, very public fight,\" he said. \"I think it's reflective of the nature of AI.\"\nNPR's Bobby Allyn contributed to this report.",
      "summary": "\ud83d\udeab President Trump ordered all U.S. federal agencies to immediately cease using Anthropic's AI products.<br>\ud83d\udea8 Defense Secretary Pete Hegseth designated Anthropic a national security supply chain risk, blacklisting it from U.S. military or contractor work.<br>\ud83e\udd1d OpenAI announced it struck a deal with the Defense Department to provide its AI technology for classified networks, hours after Trump's ban."
    },
    {
      "title": "Trump orders federal agencies to drop Anthropic\u2019s AI",
      "source": "The Verge",
      "link": "https://www.theverge.com/policy/886489/pentagon-anthropic-trump-dod",
      "published": "2026-02-27T16:30:47-05:00",
      "raw_text": "On Friday afternoon, Donald Trump posted on Truth Social, accusing Anthropic, the AI company behind Claude, of attempting to \u201cSTRONG-ARM\u201d the Pentagon and directing federal agencies to \u201cIMMEDIATELY CEASE\u201d use of its products. At issue is Anthropic CEO Dario Amodei\u2019s refusal of an updated agreement with the US military agreeing to \u201cany lawful use\u201d of Anthropic\u2019s technology, as Defense Secretary Pete Hegseth mandated in a January memo, to the frustration of many tech workers across the industry.\nTrump orders federal agencies to drop Anthropic\u2019s AI\nAnthropic has refused to agree to the Pentagon\u2019s demand to allow \u2018any lawful use\u2019 of its AI.\nAnthropic has refused to agree to the Pentagon\u2019s demand to allow \u2018any lawful use\u2019 of its AI.\nAs we explained earlier this week, that agreement would give the US military access to use the company\u2019s services for mass domestic surveillance and lethal autonomous weapons, or AI that has full power to track and kill targets with no humans involved in the decision-making process. OpenAI and xAI have reportedly already agreed to the new terms, though OpenAI is reportedly looking to negotiate with the Pentagon to adopt the same red lines as Anthropic.\nFor weeks, Anthropic and the Pentagon have been locked in a stalemate regarding the acceptable use of Anthropic\u2019s AI technology. But negotiations with Anthropic had stalled after a dramatic exchange of public statements and social media posts.\nIn a statement Thursday, Amodei wrote that the Pentagon\u2019s \u201cthreats do not change our position: we cannot in good conscience accede to their request.\u201d He added that Anthropic has \u201cnever raised objections to particular military operations nor attempted to limit use of our technology in an ad hoc manner\u201d but that in a \u201cnarrow set of cases, we believe AI can undermine, rather than defend, democratic values.\u201d\nAmodei went on to say that \u201cshould the Department choose to offboard Anthropic, we will work to enable a smooth transition to another provider, avoiding any disruption to ongoing military planning, operations, or other critical missions. Our models will be available on the expansive terms we have proposed for as long as required.\u201d\nNow we have Trump\u2019s response:\nTHE UNITED STATES OF AMERICA WILL NEVER ALLOW A RADICAL LEFT, WOKE COMPANY TO DICTATE HOW OUR GREAT MILITARY FIGHTS AND WINS WARS! That decision belongs to YOUR COMMANDER-IN-CHIEF, and the tremendous leaders I appoint to run our Military.\nThe Leftwing nut jobs at Anthropic have made a DISASTROUS MISTAKE trying to STRONG-ARM the Department of War, and force them to obey their Terms of Service instead of our Constitution. Their selfishness is putting AMERICAN LIVES at risk, our Troops in danger, and our National Security in JEOPARDY.\nTherefore, I am directing EVERY Federal Agency in the United States Government to IMMEDIATELY CEASE all use of Anthropic\u2019s technology. We don\u2019t need it, we don\u2019t want it, and will not do business with them again! There will be a Six Month phase out period for Agencies like the Department of War who are using Anthropic\u2019s products, at various levels. Anthropic better get their act together, and be helpful during this phase out period, or I will use the Full Power of the Presidency to make them comply, with major civil and criminal consequences to follow.\nWE will decide the fate of our Country \u2014 NOT some out-of-control, Radical Left AI company run by people who have no idea what the real World is all about. Thank you for your attention to this matter. MAKE AMERICA GREAT AGAIN!\nMost Popular\n- Phones are going to get weird next week\n- Netflix walks away from its deal to buy Warner Bros. after Paramount came back with a better offer\n- Why no magnets in Galaxy S26? Samsung R&D chief explains\n- Burger King will use AI to check if employees say \u2018please\u2019 and \u2018thank you\u2019\n- Pok\u00e9mon Presents 2026: All the news and trailers",
      "summary": "\ud83d\udde3\ufe0f President Donald Trump posted on Truth Social, accusing Anthropic of attempting to \"STRONG-ARM\" the Pentagon.<br>\ud83d\udcdc The dispute centers on Anthropic CEO Dario Amodei's refusal to allow \"any lawful use\" of its AI by the U.S. military, specifically for mass surveillance and lethal autonomous weapons.<br>\ud83d\uddd3\ufe0f Trump\u2019s directive includes a six-month phaseout period for Anthropic's products across all federal agencies."
    },
    {
      "title": "Iran latest: US and Israel launch attack on Iran, as Trump says 'major combat operations' under way - follow live",
      "source": "BBC",
      "link": "https://news.google.com/rss/articles/CBMiVEFVX3lxTE1BRHVheEFBVUpmNUpoZkw3RWJxNVFNRl9rc2FmVWtMd1FRNEJTSFJLNUxuRjVVZkcyTWs0VVZpYnE3d1pqaGpNTDlRTTRldlFQOS00Sg?oc=5",
      "published": "Sat, 28 Feb 2026 11:08:43 GMT",
      "raw_text": "'They hit hard': Videos show aftermath of strikes on Tehranpublished at 11:14 GMT\nBy Merlyn Thomas and Paul Brown\nSeveral verified videos from Tehran show large plumes of smoke rising above the capital.\nOne clip, filmed at a busy intersection, shows traffic slowing as drivers and pedestrians look up at the sky. In the footage, one person can be heard saying: \"They hit it hard. Where did they hit? They\u2019re saying it\u2019s Khamenei\u2019s residence.\"\nWe have located this video within a kilometre of Leadership House, the office of Iran\u2019s Supreme Leader, Ayatollah Ali Hosseini Khamenei. It\u2019s not clear from the video angle if it was a direct hit on the building.\nAnother verified photo from the same area shows thick, dark columns of smoke rising above residential buildings and small grocery shops. There are also reports of strikes elsewhere in the country.\nBBC Verify is continuing to analyse and verify videos and pinpoint exactly which locations have been hit and what exactly has been targeted.",
      "summary": "\ud83d\udd25 Several verified videos from Tehran show large plumes of smoke rising above the capital after reported strikes.<br>\ud83d\udccd One video was located within a kilometer of Leadership House, the office of Iran\u2019s Supreme Leader, Ayatollah Ali Hosseini Khamenei.<br>\ud83c\uddee\ud83c\uddf7 Reports indicate strikes occurred in Tehran and other locations across the country."
    },
    {
      "title": "Kyiv's elderly endure blackouts and bombardment, clinging to warmth and hope",
      "source": "NPR",
      "link": "https://www.npr.org/2026/02/28/nx-s1-5725277/russia-ukraine-war-kyiv-elderly",
      "published": "Sat, 28 Feb 2026 06:00:00 -0500",
      "raw_text": "Kyiv's elderly endure blackouts and bombardment, clinging to warmth and hope\nKYIV, Ukraine \u2014 There's electricity on Kyiv's left bank today, so a small elevator carries visitors up to Liliya Martynivna Lapina's 10th-floor apartment. The 88-year-old has been spending her days in her bed under a pile of blankets by a bright but cold window, trying to stay warm.\nShe sits bolt upright and seems to come alive as visitors enter her apartment, erupting in a stream of words and enthusiasm over the care package of pasta, sugar, tea and cooking oil that has been delivered. Lapina is wearing multiple layers of colorful wool sweaters and a headscarf.\nNPR is accompanying the aid group Starenki, which delivers food and fellowship to the mostly older people stuck in their apartments this winter as they try to survive the frequent heat and power cuts brought on by Russia's assault on Ukraine's energy infrastructure.\nAs Russian President Vladimir Putin fails to make significant progress on the battlefield, he is trying to break the Ukrainian people's will by plunging them into the cold and dark in one of the coldest winters in years. The capital, Kyiv, has been particularly hard hit. Kyiv Mayor Vitali Klitschko urged those who could to leave the city. But many people, especially older adults, have nowhere else to go.\n\"The left bank of the Dnieper River has been very hard hit by Russian strikes, leaving most people in the dark for days on end,\" says Alina Diachenko, director of Starenki. \"Their houses are without warmth and without electricity, and the old people try to heat themselves by wearing more clothes and turning on the gas of their stoves. They suffer a lot.\"\nBut on this day, Lapina is animated. Her cluttered apartment is filled with Eastern Orthodox icons. She says God will punish Russia for what it's doing. And she greatly admires Ukrainian President Volodymyr Zelenskyy:\n\"Our president is wonderful,\" she says. \"I listen to him on the radio. Nobody else could do what he does. And he's Jewish. They are very good people, the Jews. ... And God is a Jew.\"\nNatalia Zaitseva, one of the volunteers with Starenki, has two children, an aging mother and a job in IT, but she still finds time to help those less fortunate.\n\"Children and older people are my passion,\" she says, \"especially if I see someone who hasn't any friends or family. It gives me a lump in my throat, and I want to cry.\"\nZaitseva calls up on the intercom for the group's next visit \u2014 to Olga Ivanivna. Our group avoids a blinking, sketchy-looking elevator and decides to climb the nine flights of stairs to her apartment.\nIvanivna, 78, opens the door, also wearing layers and a wool cap, though there has been electricity for the past couple of days. \"Thank God,\" she says. \"Otherwise it's freezing and there's no water.\"\nIvanivna says no one else comes to visit her, so she's very appreciative of the staples and the camaraderie that Starenki brings.\nShe shows us a photo of her son \u2014 a doctor \u2014 who took care of her before he died five years ago. \"My good health departed along with him,\" she says.\nBut she still keeps her son's house plants alive. All kinds of potted and hanging plants fill the front room with its large, bright window.\nAt the next apartment, our group is greeted by Irma, a soulful-eyed, ferocious lapdog. Irma's mistress, Vira Pavlivna Romanchyk, stands behind her walker. She's nearly blind. She says her son gets her groceries.\n\"But Irma is my best support,\" she says. \"She sits by my side all day long, keeping me company and protecting me.\"\nThe final visit is to 80-year-old Nelia Stepanivna Thomashevska, who is interested to know what kind of journalists we are before telling us about her life. Thomashevska's husband was a Soviet military pilot, and they lived for a short time in Russia's far east. But he died in a helicopter crash in 1974. The couple had no children. She says that when she was younger, she was quite active in her apartment building's cooperative.\nStanding in her tiny kitchen with its old appliances and stove, she tells us that she fears going without power more than the airstrikes. She points up at the light in the kitchen. \"Losing electricity and heating,\" she says. But today the kitchen radiator is warm. Thomashevska opens the smudged kitchen window to sprinkle some seeds on the windowsill. Soon pigeons arrive, cooing and flapping.\nShe also has two cats. She says they help her during the nightly drone and missile attacks. \"My cats go under the covers because they know ahead of time that there's going to be explosions,\" she says. \"It's instinct. They jump under the covers and know before me there's going to be an airstrike.\"\nBut none of this seems to have sapped her will.\n\"We will hold on, we will survive and we will win,\" she says.\n\"Heroiam slava,\" she says to us in Ukrainian in a phrase that means \"Glory to the heroes.\" While it can be said alone, it is also the second half of a call-and-response that Ukrainians begin with \"Slava Ukraini\" \u2014 Glory to Ukraine.\nAs we walk away from Thomashevska's apartment building through the snow, she opens her fourth-floor kitchen window and calls out to us, surrounded by pigeons.\n\"Garnogo dnya!\" we call up from the street in Ukrainian: Have a good day.\nTending to her pigeons, Thomashevska waves us goodbye.\nNPR producer Polina Lytvynova contributed to this report in Kyiv, Ukraine.",
      "summary": "\u26a1 Russian attacks on Ukraine's energy infrastructure are causing frequent heat and power cuts, severely impacting Kyiv's elderly residents.<br>\ud83d\udc75 The aid group Starenki delivers care packages, including pasta, sugar, tea, and cooking oil, to older people stuck in their Kyiv apartments.<br>\ud83c\udfe1 Liliya Martynivna Lapina, 88, and Olga Ivanivna, 78, are among the elderly enduring cold and lack of water on Kyiv's left bank."
    }
  ],
  "podcasts": [
    {
      "podcast": "This Week in Startups",
      "title": "The Biggest Private Funding Round in History | E2256",
      "published": "2026-02-28",
      "summary": "\ud83d\udcb0 OpenAI reportedly raised $110 billion in private funding from investors including Sequoia, Nvidia, and SoftBank.<br>\ud83d\udcc8 ChatGPT now has 900 million weekly active users and 50 million paying subscribers.<br>\ud83e\udde0 Host Jason Calacanis stated his belief that Artificial General Intelligence (AGI) has \"ALREADY hit\" but is not yet fully implemented.",
      "raw_text": "This Week In Startups is made possible by:Deel - http://deel.com/twistWispr Flow - https://wisprflow.ai/twistLuma AI - https://lumalabs.ai/twistToday\u2019s show:*$110 billion buys you 15% of OpenAI. Amazon, Nvidia, and SoftBank placed their bets on ChatGPT, which now has 900 million weekly active users and 50 million paying subscribers. Find out why Jason is anticipating the wildest J-Curve swing of all time, and believes we\u2019ve ALREADY hit AGI\u2026 it\u2019s just not implemented yet.Plus a visit from our roving correspondent Nick O\u2019Neill, checking in on the Crypto Chaos in Miami Beach, and hot demos from three young founders.GUESTS:Nick O\u2019Neill: https://x.com/chooserichEverest Chris: https://openclaw.unloopa.com/Ben Broca: https://polsia.com/Adi Gabrani: https://makemyclaw.com/Timestamps:00:00 Intro01:33 We\u2019re hiring a new producer!05:42 OpenAI raised $110 billion08:59 Understanding the LLM J-Curve00:11:25 Deel - Founders ship faster on Deel. Set up payroll for any country in minutes and get back to building. Visit \u2060https://deel.com/twist\u2060 to learn more.00:15:02 CRYPTO CHAOS IN MIAMI BEACH!00:21:10 Wispr Flow - Stop typing. Dictate with Wispr Flow and send clean, final-draft writing in seconds. Visit \u2060https://wisprflow.ai/twist\u2060 to get started for free today.00:22:54 Mass layoffs at Block00:30:50 Luma AI - Stop guessing and start directing with the all-in-one Dream Machine text-to-video platform. Visit\u00a0\u2060https://lumalabs.ai/twist\u2060\u00a0to try The Dream Machine for free.00:32:04 AI Scott Adams: The Saga Continues00:38:13 Make URLs for local businesses with Unloopa00:45:36 Rent a Polsia agent to run your company00:58:55 Deploy swarms in 60 seconds with MakeMyClaw01:05:05 LAUNCH FEST is coming to SF01:55:49 Will Paramount actually buy WBD?01:06:58 Why Lon loves \u201cKnight of the 7 Kingdoms\u201d01:07:21 On \u201cNeighbors\u201d and First Amendment Warriors01:13:43 All about Jason\u2019s favorite chargersSubscribe to the TWiST500 newsletter: https://ticker.thisweekinstartups.comCheck out the TWIST500: https://www.twist500.comSubscribe to This Week in Startups on Apple: https://rb.gy/v19fcpFollow Lon:X: https://x.com/lonsFollow Alex:X: https://x.com/alexLinkedIn: \u2060https://www.linkedin.com/in/alexwilhelmFollow Jason:X: https://twitter.com/JasonLinkedIn: https://www.linkedin.com/in/jasoncalacanisCheck out all our partner offers: https://partners.launch.co/Great TWIST interviews: Will Guidara, Eoghan McCabe, Steve Huffman, Brian Chesky, Bob Moesta, Aaron Levie, Sophia Amoruso, Reid Hoffman, Frank Slootman, Billy McFarlandCheck out Jason\u2019s suite of newsletters: https://substack.com/@calacanisFollow TWiST:Twitter: https://twitter.com/TWiStartupsYouTube: https://www.youtube.com/thisweekinInstagram: https://www.instagram.com/thisweekinstartupsTikTok: https://www.tiktok.com/@thisweekinstartupsSubstack: https://twistartups.substack.com",
      "link": "https://podcasters.spotify.com/pod/show/thisweekinstartups/episodes/The-Biggest-Private-Funding-Round-in-History--E2256-e3fniar"
    },
    {
      "podcast": "AI Daily Brief",
      "title": "Are 40% Staff Cuts the New AI Normal?",
      "published": "2026-02-28",
      "summary": "\ud83e\udd16 Google released its new Nano Banana 2 AI model.<br>\ud83d\udcc8 Claude's user signups have recently surged.<br>\ud83d\udcbe Meta is reducing its focus on developing custom AI chips.",
      "raw_text": "Block just cut 40% of its workforce in one move, with Jack Dorsey arguing that new intelligence tools and smaller, flatter teams fundamentally change how companies operate\u2014prompting a massive stock surge and igniting debate over whether this is the first true AI-driven headcount reset or simply COVID overhiring getting cleaned up under a new narrative. In the headlines: Google releases Nano Banana 2, Claude signups surge, Meta pulls back on custom chips, and Microsoft previews Copilot Tasks.Want to build with OpenClaw?LEARN MORE ABOUT CLAW CAMP: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://campclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Or for enterprises, check out: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://enterpriseclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Brought to you by:KPMG \u2013 Agentic AI is powering a potential $3 trillion productivity shift, and KPMG\u2019s new paper, Agentic AI Untangled, gives leaders a clear framework to decide whether to build, buy, or borrow\u2014download it at \u2060\u2060\u2060\u2060\u2060www.kpmg.us/Navigate\u2060\u2060\u2060\u2060\u2060Mercury - Modern banking for business and now personal accounts. Learn more at \u2060\u2060\u2060\u2060\u2060\u2060\u2060https://mercury.com/personal-banking\u2060\u2060\u2060\u2060\u2060\u2060\u2060Rackspace Technology - Build, test and scale intelligent workloads faster with Rackspace AI Launchpad -\u00a0\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060http://rackspace.com/ailaunchpad\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Blitzy - Want to accelerate enterprise software development velocity by 5x? \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://blitzy.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Optimizely Agents in Action - Join the virtual event (with me!) free March 4 - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.optimizely.com/insights/agents-in-action/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060AssemblyAI - The best way to build Voice AI apps - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.assemblyai.com/brief\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060LandfallIP - AI to Navigate the Patent Process - https://landfallip.com/Robots &amp; Pencils - Cloud-native AI solutions that power results \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://robotsandpencils.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060The Agent Readiness Audit from Superintelligent - Go to \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://besuper.ai/ \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060to request your company's agent readiness score.The AI Daily Brief helps you understand the most important news and discussions in AI. Subscribe to the podcast version of The AI Daily Brief wherever you listen: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://pod.link/1680633614\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Interested in sponsoring the show? sponsors@aidailybrief.ai",
      "link": "https://podcasters.spotify.com/pod/show/nlw/episodes/Are-40-Staff-Cuts-the-New-AI-Normal-e3fnjad"
    },
    {
      "podcast": "Morning Brew Daily",
      "title": "Netflix Walks Away From WBD Deal & Why is Everyone Leaving America?",
      "published": "2026-02-27",
      "summary": "\ud83d\udcc9 Jack Dorsey\u2019s Block company cut nearly half of its staff, citing AI automation as a factor.<br>\ud83d\uddfa\ufe0f Waymo has expanded its autonomous vehicle services into additional cities.<br>\ud83c\udf55 Papa John\u2019s has closed hundreds of its restaurant locations.",
      "raw_text": "Episode 789: Neal and Ann dive into why Netflix has walked away from acquiring Warner Bros. Discovery after Paramount makes another counter offer. Then, Jack Dorsey\u2019s Block cuts nearly half of his staff due to AI, and he thinks companies will ultimately do the same. Meanwhile, more and more Americans are packing up their bags and moving away. Also, Waymo expands into more cities. Papa John\u2019s closes hundreds of stores. And Doritos launches protein chips.\u00a0\n\nSubscribe to Morning Brew Daily for more of the news you need to start your day. Share the show with a friend, and leave us a review on your favorite podcast app.\n\nListen to Morning Brew Daily Here:\u2060 \u2060\u2060https://www.swap.fm/l/mbd-note\u2060\u2060\u2060\u00a0\n\nWatch Morning Brew Daily Here:\u2060 \u2060\u2060https://www.youtube.com/@MorningBrewDailyShow\u2060\n\n\u200bFor more of Ann, check out Brew Markets here: swap.fm/l/brewmarketsshow\nLearn more about your ad choices. Visit megaphone.fm/adchoices",
      "link": ""
    },
    {
      "podcast": "This Week in Startups",
      "title": "Behind the Scenes with an early OpenClaw contributor! | E2252",
      "published": "2026-02-26",
      "summary": "\ud83e\udd1d OpenClaw\u2019s third ever contributor, Tyler Yust, shared insights from within the \"foundation\" during the episode.<br>\ud83d\udcf1 The episode featured the creator of an OpenClaw instance designed to fit in a pocket.<br>\ud83c\udf10 The founder of OpenBrowse demonstrated how to automatically detect and generate OpenClaw skills.",
      "raw_text": "This Week In Startups is made possible by:Lemon IO - https://Lemon.io/twistEvery.io - https://every.io Sentry.io- https://sentry.io/twistToday\u2019s show:We\u2019re going behind the curtain today \u2014 it\u2019s a packed show!We found Tyler Yust, OpenClaw\u2019s third EVER contributor to share his insights from within foundation! We\u2019ve got Deedy Das, of Menlo Ventures, on the show to discuss whether SaaS is cooked! Next we met the creator of an OpenClaw instance that fits in your pocket! We\u2019ve also got the founder of OpenBrowse showing us how he automatically detects and generates OpenClaw skills!Timestamps:00:00 Intro - Deedy Das Joins the Show!04:54 Anthropic\u2019s revenue growth and valuation06:07 OpenClaw Contributor Tyler Yuts joins the show09:24 iMessage integration and Apple\u2019s proprietary systems00:10:07 Lemon.io - Get 15% off your first 4 weeks of developer time at https://Lemon.io/twist14:31 Anthropic vs. the Pentagon00:20:02 Every.io - For all of your incorporation, banking, payroll, benefits, accounting, taxes or other back-office administration needs, visit\u00a0https://every.io.00:30:08 Sentry - New users can get $240 in free credits when they go to https://sentry.io/twist and use the code TWIST00:35:46 The Infamous Citrini article00:32:47 Come to LAUNCH fest! https://fest.launch.co00:36:28 Why Deedy thinks the Cetrini article is a work of science fiction00:44:51 The illusion of privacy in corporate America00:41:18 Deedy thinks Enterprise SaaS apps aren\u2019t going to be vibe coded00:49:20 Jason\u2019s Reddit Bot00:52:01 Jason\u2019s obsession with Singapore\u2019s food00:55:22 How Unbrowse pulls any backend API!01:02:07 Sebastian shows off the smallest OpenClaw form factor!01:12:04 The Prolo ring \u2014 for people who doomscroll01:20:21 Deedy\u2019s Podcast Player App!Thank you to our partners:(10:07) Lemon.io - Get 15% off your first 4 weeks of developer time at https://Lemon.io/twist(20:02) Every.io - For all of your incorporation, banking, payroll, benefits, accounting, taxes or other back-office administration needs, visit every.io.(30:08) Sentry - New users can get $240 in free credits when they go to sentry.io/twist and use the code TWISTSubscribe to the TWiST500 newsletter: https://ticker.thisweekinstartups.comCheck out the TWIST500: https://www.twist500.comSubscribe to This Week in Startups on Apple: https://rb.gy/v19fcpFollow Lon:X: https://x.com/lonsFollow Alex:X: https://x.com/alexLinkedIn: \u2060https://www.linkedin.com/in/alexwilhelmFollow Jason:X: https://twitter.com/JasonLinkedIn: https://www.linkedin.com/in/jasoncalacanisCheck out all our partner offers: https://partners.launch.co/Great TWIST interviews: Will Guidara, Eoghan McCabe, Steve Huffman, Brian Chesky, Bob Moesta, Aaron Levie, Sophia Amoruso, Reid Hoffman, Frank Slootman, Billy McFarlandCheck out Jason\u2019s suite of newsletters: https://substack.com/@calacanisFollow TWiST:Twitter: https://twitter.com/TWiStartupsYouTube: https://www.youtube.com/thisweekinInstagram: https://www.instagram.com/thisweekinstartupsTikTok: https://www.tiktok.com/@thisweekinstartupsSubstack: https://twistartups.substack.com",
      "link": "https://podcasters.spotify.com/pod/show/thisweekinstartups/episodes/Behind-the-Scenes-with-an-early-OpenClaw-contributor---E2252-e3fkbo0"
    },
    {
      "podcast": "AI Daily Brief",
      "title": "The OpenClaw-ification of AI",
      "published": "2026-02-26",
      "summary": "\u2699\ufe0f The agent era is characterized by new primitives like persistent work, multimodal orchestration, and scheduled autonomy.<br>\ud83d\udea7 OpenAI\u2019s ambitious Stargate project has encountered turbulence.<br>\ud83d\udcb0 Nvidia reported another significant earnings increase.",
      "raw_text": "Anthropic rolls out Claude Code Remote Control and Scheduled Tasks, Perplexity launches Perplexity Computer, Notion unveils Custom Agents, and suddenly every major AI player is shipping always-on, agentic workflows that look a lot like OpenClaw. This episode explores why this isn\u2019t about copying a hot project, but about the emergence of new primitives in the agent era\u2014persistent work, multimodal orchestration, scheduled autonomy, and AI that follows you across devices. In the headlines: Anthropic\u2019s standoff with the Pentagon escalates, OpenAI\u2019s Stargate ambitions hit turbulence, and Nvidia posts another monster earnings report. Want to build with OpenClaw?LEARN MORE ABOUT CLAW CAMP: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://campclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Or for enterprises, check out: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://enterpriseclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Brought to you by:KPMG \u2013 Agentic AI is powering a potential $3 trillion productivity shift, and KPMG\u2019s new paper, Agentic AI Untangled, gives leaders a clear framework to decide whether to build, buy, or borrow\u2014download it at \u2060\u2060\u2060\u2060www.kpmg.us/Navigate\u2060\u2060\u2060\u2060Mercury - Modern banking for business and now personal accounts. Learn more at \u2060\u2060\u2060\u2060\u2060\u2060https://mercury.com/personal-banking\u2060\u2060\u2060\u2060\u2060\u2060Rackspace Technology - Build, test and scale intelligent workloads faster with Rackspace AI Launchpad -\u00a0\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060http://rackspace.com/ailaunchpad\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Blitzy - Want to accelerate enterprise software development velocity by 5x? \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://blitzy.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Optimizely Agents in Action - Join the virtual event (with me!) free March 4 - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.optimizely.com/insights/agents-in-action/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060AssemblyAI - The best way to build Voice AI apps - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.assemblyai.com/brief\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060LandfallIP - AI to Navigate the Patent Process - https://landfallip.com/Robots &amp; Pencils - Cloud-native AI solutions that power results \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://robotsandpencils.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060The Agent Readiness Audit from Superintelligent - Go to \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://besuper.ai/ \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060to request your company's agent readiness score.The AI Daily Brief helps you understand the most important news and discussions in AI. Subscribe to the podcast version of The AI Daily Brief wherever you listen: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://pod.link/1680633614\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Interested in sponsoring the show? sponsors@aidailybrief.ai",
      "link": "https://podcasters.spotify.com/pod/show/nlw/episodes/The-OpenClaw-ification-of-AI-e3fluoh"
    }
  ],
  "papers": [
    {
      "title": "Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks",
      "authors": [
        "Kunihiro Miyazaki",
        "Takanobu Kawahara",
        "Stephen Roberts",
        "Stefan Zohren"
      ],
      "abstract": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis",
      "link": "https://arxiv.org/pdf/2602.23330v1",
      "published": "2026-02-26",
      "arxiv_id": "2602.23330v1",
      "citation_count": null,
      "quick_summary": "\ud83d\udcda The paper proposes a multi-agent LLM trading framework designed to enhance financial investment analysis.<br>\ud83d\udd2c This framework explicitly decomposes investment analysis into fine-grained trading tasks.<br>\ud83d\udcc9 Traditional multi-agent systems often rely on abstract instructions, leading to degraded inference performance and less transparent decision-making.",
      "raw_text": "The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis"
    },
    {
      "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
      "authors": [
        "Sahal Shaji Mullappilly",
        "Mohammed Irfan Kurpath",
        "Omair Mohamed",
        "Mohamed Zidan",
        "Fahad Khan"
      ],
      "abstract": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases",
      "link": "https://arxiv.org/pdf/2602.23363v1",
      "published": "2026-02-26",
      "arxiv_id": "2602.23363v1",
      "citation_count": null,
      "quick_summary": "\ud83e\udde0 The paper introduces MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal LLMs (MLLMs).<br>\ud83d\udcac MediX-R1 enables clinically grounded, free-form answers from MLLMs, moving beyond multiple-choice formats.<br>\ud83d\udee0\ufe0f The framework fine-tunes a vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning.",
      "raw_text": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases"
    },
    {
      "title": "Utilizing LLMs for Industrial Process Automation",
      "authors": [
        "Salim Fares"
      ],
      "abstract": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to ut",
      "link": "https://arxiv.org/pdf/2602.23331v1",
      "published": "2026-02-26",
      "arxiv_id": "2602.23331v1",
      "citation_count": null,
      "quick_summary": "\ud83c\udfed This research aims to explore the utility of Large Language Models (LLMs) for software in the industrial process automation domain.<br>\ud83d\udcbb Existing work on LLM software engineering primarily focuses on widely-used general-purpose programming languages like Python.<br>\u2753 The utility of LLMs for highly-specialized industrial languages, typically used in proprietary contexts, remains underexplored.",
      "raw_text": "A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to ut"
    },
    {
      "title": "No One Size Fits All: QueryBandits for Hallucination Mitigation",
      "authors": [
        "Nicole Cho",
        "William Watson",
        "Alec Koppel",
        "Sumitra Ganesh",
        "Manuela Veloso"
      ],
      "abstract": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the o",
      "link": "https://huggingface.co/papers/2602.20332",
      "published": "2026-02-23",
      "arxiv_id": "",
      "citation_count": null,
      "quick_summary": "\ud83d\udee1\ufe0f The paper introduces QueryBandits, a model-agnostic contextual bandit framework for hallucination mitigation in LLMs.<br>\ud83d\udd04 QueryBandits adaptively learns online to select the optimal query strategy to mitigate hallucinations, particularly in closed-source models.<br>\ud83d\udcda Most existing hallucination mitigation work focuses on open-source models for post-hoc detection and parameter editing.",
      "raw_text": "Advanced reasoning capabilities in Large Language Models (LLMs) have led to more frequent hallucinations; yet most mitigation work focuses on open-source models for post-hoc detection and parameter editing. The dearth of studies focusing on hallucinations in closed-source models is especially concerning, as they constitute the vast majority of models in institutional deployments. We introduce QueryBandits, a model-agnostic contextual bandit framework that adaptively learns online to select the o"
    },
    {
      "title": "What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance",
      "authors": [
        "William Watson",
        "Nicole Cho",
        "Sumitra Ganesh",
        "Manuela Veloso"
      ],
      "abstract": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask:",
      "link": "https://huggingface.co/papers/2602.20300",
      "published": "2026-02-23",
      "arxiv_id": "",
      "citation_count": null,
      "quick_summary": "\ud83e\udde0 Large Language Model (<strong>LLM</strong>) hallucinations are usually treated as defects of the model or its decoding strategy.<br>\ud83d\udcca Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response.<br>\u2699\ufe0f We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension.",
      "raw_text": "Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask:"
    }
  ],
  "ai_security_news": [
    {
      "title": "LLM firewalls emerge as a new AI security layer",
      "source": "TechTarget",
      "link": "https://news.google.com/rss/articles/CBMingFBVV95cUxQQTVOa211MGFuUVVXMHBHRzllMWJyNGdvZXJDZnRSLW40bWdGa29rWEQ3T3l2QkxjVzVXLXRjZi1ITWNIcDdLMDB2aFFGUm91eURwNERYUUhEOElnQ3pFUDktR3AxTWRMMlV2QWpSWFhmaTNubVRZQkRYUmszbUsydDFmamFXOWtDYlNEbVhuTFRIT2ZzS3FxRGpJVHNNZw?oc=5",
      "published": "Wed, 25 Feb 2026 21:08:41 GMT",
      "summary": "\ud83d\udee1\ufe0f LLM firewalls are new security tools designed to monitor, filter, and sanitize user input for generative AI models.<br>\u2694\ufe0f These firewalls protect against prompt injection attacks, data leaks, malicious code generation, privilege escalation, and model overuse.<br>\u2699\ufe0f LLM firewalls typically have three components: a prompt firewall for user input, a retrieval firewall for external data, and a response firewall for model output.",
      "raw_text": "Getty Images/iStockphoto\nLLM firewalls emerge as a new AI security layer\nThe race by organizations to AI-enable their operations and business workflows is exposing them to new risks that AI firewalls aim to address.\nOrganizations are racing to integrate large language models (LLMs) and generative AI into their operations -- and opening themselves up to a slew of new vulnerabilities in the process.\nThe trend is driving interest in technologies specifically designed to manage and contain AI-driven risks. Among the most visible of these emerging technologies are so-called LLM firewalls.\nWhat's an LLM firewall?\nWith the coupling of AI and operational systems come the risks of prompt injection attacks, model poisoning, data leaks and dangerous misconfigurations.\nLLM firewalls have emerged as one way to counter these risks. The tools enable security teams to monitor, filter and sanitize user input, manage how a model interacts with other systems and understand how data might flow through it.\nOne of the specialized firewall's primary functions is to protect the LLM against prompt injection attacks -- where an adversary crafts inputs that manipulate the model into performing unintended actions or responding outside its safety guardrails. Firewalls for LLMs also aim to protect against other risks, including data leaks -- for instance, by preventing users from inputting sensitive data into the model; malicious code generation; privilege escalation attacks; and model overuse.\nHow LLM firewalls are different\nLLM firewalls differ from web application firewalls (WAFs), which inspect message content for indications of code injection and other types of attacks. They also differ from lower-level network firewalls, which make security decisions based on port numbers, protocols and other patterns in network traffic.\n\"Each has its place in a security architecture, but an LLM firewall is increasingly necessary as organizations roll out their own LLMs and LLM-enabled applications that require specialized protection that WAF and network firewalls cannot provide,\" said Christopher Rodriguez, research director of security and trust at analyst firm IDC.\nRik Turner, an analyst at Omdia, a division of Informa TechTarget, said to think of AI firewalls as tools that analyze the semantics, intent and context of natural language as contained in both incoming prompts and outgoing responses.\nSuch firewalls typically have three distinct components or layers, Turner said: a prompt firewall that scans user input before it reaches the LLM to block jailbreaks, prompt injections and malicious commands; a retrieval firewall for managing data fetched from external databases during retrieval-augmented generation; and a response firewall for outbound traffic, which reviews the model's generated text before it reaches the user.\nThe LLM firewall market: A feeding frenzy?\nSeveral established vendors, including Palo Alto Networks, Cloudflare, Akamai, Varonis and Check Point, have begun offering LLM protection capabilities as part of their broader security portfolios. There's also a rapidly growing list of vendors that offer specialized LLM security products, including Lakera, Prompt Security, HiddenLayer and CalypsoAI.\nRichard Stiennon, chief research analyst at cybersecurity market intelligence firm IT-Harvest, pointed to several other vendors in the broader AI security space that also offer firewall capabilities for LLMs. Examples include Operant AI, Aiceberg, Acuvity, HydroX AI, Cytex and Citadel AI.\nEstimates of the current size of the LLM firewall market vary widely, reflecting the early and still-emerging nature of the category. IT-Harvest has pegged the current market for AI firewalls at a modest $30 million and estimates the segment will grow 100% in 2026. Others have higher projections. 360iResearch, for example, estimated the market size at $260 million in 2025 and slated it to hit almost $800 million in 2032.\nA nascent technology: Too soon to say\nThe segment is so new that not all vendors are even settled on the term LLM firewall, Stiennon said. Stiennon himself listed them under what he calls the \"model protection\" category. Others, he said, might refer to them as AI firewalls.\nFrom an effectiveness standpoint, Turner said many of the currently available AI firewalls offer reasonably good protection against jailbreaks, prompt injections and malicious commands. They can filter content that users might input into a model to protect sensitive data and personally identifiable information. They also do rate limiting to throttle DDoS attacks against the model and the server on which it is hosted, Turner said.\nBut they may struggle to detect newer forms of attacks, he cautioned. \"A lot of the current generation of LLM firewalls analyze prompts individually, which means they lack context across multiple prompts,\" he said. They could therefore struggle to detect stateful or conversational attacks, in which an attacker might gradually manipulate a model over several interactions to bypass security rather than using a single malicious prompt.\nIt's also still too early to draw definitive conclusions about the long-term effectiveness of LLM firewalls, given how new the technology is and how recently organizations have begun deploying it. Attacks targeting AI environments are also constantly evolving, so there's no telling what additional security controls will be needed to address them.\n\"LLM firewalls, aka firewalls for AI, inspect the interactions -- both inbound and outbound -- with an LLM or LLM-enabled application,\" IDC's Rodriguez said. \"These checks often require the ability to understand meaning, context and intent of messages.\"\nThis ability will be key to effectiveness, said Michael Smith, field CTO at DigiCert. Without context, an LLM might be poisoned with misinformation, and there is no way for the LLM firewall to identify this.\n\"Or the LLM could hallucinate, or recite inaccurate facts, which are not dangerous to the LLM, the data inside of it or the user's client. But it is dangerous to the human who takes the hallucination as fact and acts based on that,\" Smith added.\nDo organizations need specialized firewalls for AI?\nOrganizations need to know exactly what they want to protect against and where to deploy these controls. Decision-makers should answer the following basic questions to derive real value from their AI firewall investment, Smith said:\n- Where is the LLM hosted, and does the firewall deployment model support that?\n- What kinds of data does the firewall have to be able to recognize in a prompt or an output?\n- Where and how will the output of the LLM be used?\n- Do you need to protect the LLM client or things that it controls?\nWith so many AI firewall options readily available -- many from startups and companies with little to no track record in enterprise environments -- making purchasing decisions can be hard. So, knowing what to look for and what to ask can be crucial. Rodriguez stressed the importance of decision-makers paying attention to two factors in particular: accuracy and latency.\nAn AI firewall with too many false positives can frustrate users, while one that is prone to too many false negatives can expose the organization to heightened business risk, he pointed out.\n\"Accuracy of detections will become ever more important as organizations begin to better understand the business risk surrounding their LLMs and LLM-enabled applications,\" Rodriquez said. Latency is also important because many LLM firewall offerings are cloud-based, he added.\nAt the end of the day, while LLM firewalls are likely going to be an important requirement for organizations harnessing GenAI technologies in their operations, they are only part of a broader stack of needed security controls. True defense-in-depth for AI security means deploying capabilities for broader AI security posture management, data loss prevention and data security posture management for both training and inference data, Omdia's Turner said. Also likely needed are tools for tokenizing sensitive data so no private data is exposed in an AI model, he noted.\n\"Generative AI right now is the killer shadow IT application,\" DigiCert's Smith said. \"It has trickled into so many applications and workflows now that it's impossible to keep it out of your organization.\"\nJaikumar Vijayan is a freelance technology journalist with more than 20 years of award-winning experience in IT trade journalism, specializing in information security, data privacy and cybersecurity topics."
    },
    {
      "title": "The Hidden Threat of AI Agents Demands a New Security Model",
      "source": "Unite.AI",
      "link": "https://news.google.com/rss/articles/CBMiiAFBVV95cUxNVlU0RWw5aEJzalR4TE9HQzdBVTNDWW1COU1wQW0tTXh2Vi1ENHJQYlBsMEdKZWdtMGJhb1gyWkUzdFJROVpSRlNvRjBmY29LUEgxVTJsLVAxZFgxVjlCZlZGRVFHZENwMHdJWkhRNEx6QlR6RmsxZEZzcXY1MlNmUlJHUG5xWlBf?oc=5",
      "published": "Wed, 25 Feb 2026 16:56:01 GMT",
      "summary": "\ud83d\udc64 Agentic AI systems are granted insider authority but run on compute environments not designed to protect autonomous decision-makers from underlying infrastructure.<br>\ud83d\udcc9 Verizon\u2019s 2025 data breach report showed system intrusion was responsible for over 53% of confirmed breaches, with 22% using stolen credentials.<br>\u26a0\ufe0f OWASP lists \"Prompt Injection\" as a critical vulnerability for LLM applications, especially for agentic systems that chain actions.",
      "raw_text": "Thought Leaders\nThe Hidden Threat of AI Agents Demands a New Security Model\nAgentic AI systems have gone mainstream over the past year. They are now being used for several functions, including authenticating users, moving capital, triggering compliance workflows, and coordinating across enterprise environments with minimal human oversight.\nHowever, a quieter problem is emerging with the increasing autonomy, not at the level of prompts or policies, but at the level of infrastructure trust. Agentic systems are being granted insider authority while still running on compute environments that were never designed to protect autonomous decision-makers from the infrastructure beneath them.\nTraditional security assumes software is passive, but agentic systems are not. They reason, remember, and act continuously, autonomously, and with delegated authority.\nNot to forget that AI agents are likely to have access to personal data, based on their use case, such as emails and call records, among other things.\nAdditionally, while hardware-based protections, such as confidential virtual machines and secure enclaves, exist, they are not yet the default foundation for most agentic AI deployments. As a result, many agents still execute in environments where sensitive data is exposed to the underlying infrastructure during runtime.\nAgents Are Insiders, Not Tools\nSecurity teams already know how challenging it is to contain insider threats, an issue highlighted in Verizon\u2019s 2025 data breach report, which shows that system intrusion was responsible for more than 53% of confirmed breaches last year. In 22% of these cases, attackers used stolen credentials to gain access, which highlights how often they succeed by using legitimate identities instead of exploiting technical flaws.\nNow, consider an agent, which is made up of prompt logic, tools and plugins, credentials, as well as policies. Not only can it run code and browse the web, but it can also query CRMs, read emails, and push tickets, among many other things. What the combination of functions has brought is traditional attack surfaces into a modern interface.\nThe danger posed by such insider threats is not speculative. The Open Web Application Security Project (OWASP) now lists \u201cPrompt Injection\u201d as a critical vulnerability for LLM applications, noting its particular danger for agentic systems that chain actions. Microsoft\u2019s Threat Intelligence team has also published advisories warning that AI systems with tool access can be subverted to perform data theft if safeguards are not architecturally enforced.\nThese reports are offering a timely reminder that agents that have legitimate access to systems and data can be turned against their owners. However, the risk landscape for agentic systems is not unitary. Application-layer threats like prompt injection and tool abuse stem from the model\u2019s inability to distinguish trusted instructions from untrusted user input, a design limitation no amount of memory hardening can fix.\nA different and equally important problem exists at the infrastructure level: some agents run in plaintext memory, which means sensitive information\u2014like chat histories, API responses, and documents\u2014can be seen while being processed and may remain accessible later. OWASP identifies this risk as Sensitive Information Disclosure (LLM02) and System Prompt Leakage (LLM07) and suggests using context isolation, namespace segmentation, and memory sandboxing as important safety measures.\nAs such, users shouldn\u2019t treat these agents as just mere applications, given that they are dynamic, reasoning executors requiring a security model that takes into account their unique nature as non-human entities with agency. This approach needs to include both software controls to limit how the model acts and hardware protections to keep data safe while it\u2019s being used.\nThe Architecture of Trust Has Critical Flaw\nCurrent security practices focus on protecting data at rest and in transit. The final frontier, data in use, remains almost entirely exposed. When an AI agent reasons over a confidential dataset to approve a loan, analyze patient records, or execute a trade, that data is usually decrypted and processed in plain text within the server\u2019s memory.\nIn standard cloud models, anyone with sufficient control over the infrastructure, including hypervisor administrators or co-tenant attackers, can potentially peer into what\u2019s happening while a workload is running. For AI agents, that exposure is especially dangerous, since they need access to sensitive information to do their jobs, which can, potentially, become the attack surface.\nAs Lumia Security demonstrated, attackers with access to a local machine can obtain JWTs and session keys directly from the process memory of ChatGPT, Claude, and Copilot desktop applications. These stolen credentials can let them pretend to be another user, steal conversation history, and inject prompts into ongoing sessions that can change agent behavior or plant false memories.\nAn example of this could be AWS CodeBuild\u2019s memory-dump incident in July 2025. The attackers secretly added malicious code to a project, and when the system ran it, the code peeked into the computer\u2019s memory and stole hidden login tokens stored there. With those tokens, the attackers could change the project\u2019s code and potentially access other systems.\nFor financial institutions, the silent manipulation is existential. Banks, insurers, and investment firms already absorb average breach costs north of $10 million, and they understand that integrity matters as much as confidentiality. According to a recent Informatica report, the \u201ctrust paradox\u201d was explained as such: organizations are deploying autonomous agents faster than they can verify their outputs. The result is automation that can hardwire errors or bias, directly into core processes, operating at machine speed.\nConfidential Computing and the Case for Isolation\nIncremental fixes won\u2019t solve the problem at hand, although stricter access controls and better monitoring may help. Still, neither can change the underlying problem. The issue is architectural, and as long as computation happens in exposed memory, agents will be vulnerable at the moment they matter most, which is reasoning.\nConfidential computing, defined by the Confidential Computing Consortium (CCC) as the protection of data in use via hardware-based Trusted Execution Environments (TEEs), directly addresses the core flaw.\nFor AI agents, this hardware-level isolation is transformative, as it allows an agent\u2019s identity credentials, its model weights, proprietary prompts, and the sensitive user data it processes to remain encrypted not just on a disk or over a network, but actively in memory during execution. The separation definitively breaks the traditional model where control over the infrastructure guarantees control over the workload.\nRemote attestation provides verifiable cryptographic evidence that a specific inference request executed inside a hardware-backed trusted execution environment, whether it be a CPU or GPU. The proof is generated from hardware measurements and delivered together with the response, allowing independent verification of where and how the workload ran.\nAttestation records do not reveal the code that was executed. Instead, each workload is associated with a unique workload ID or transaction ID, and the TEE attestation record is linked to that identifier. The attestation confirms that the computation ran inside a trusted environment without disclosing its contents.\nThe setup creates a new base for compliance and auditability, allowing for the linking of an agent\u2019s actions to a specific version of code that has been attested and a known set of input data.\nToward Accountable Autonomy\nThe implications for the system described above extend beyond basic security. Consider the laws that govern finance, healthcare, and personal information. Many jurisdictions apply data-sovereignty rules that restrict where information may be processed. In China, the Personal Information Protection Law and the Data Security Law require certain categories of data, important personal data, for instance, to be stored domestically and reviewed before transfer abroad.\nSimilarly, several Gulf countries, the UAE and Saudi Arabia, for example, have adopted similar approaches, especially for financial, government and critical-infrastructure data\nConfidential computing can strengthen security and auditability by protecting data while it is being processed and enabling attestation of the runtime environment. But it does not change where processing occurs. Where data-sovereignty rules require local processing or impose conditions on cross-border transfers, trusted execution environments may support compliance controls, not replace legal requirements.\nFurthermore, confidential computing enables secure collaboration in multi-agent systems, where agents from different organizations or within different departments often need to share information or validate outputs without exposing proprietary data.\nAnd when the technology is paired with zero-trust architecture, the result is a much stronger foundation. Zero trust continuously validates identity and access, while confidential computing protects the hardware\u2019s memory from unauthorized extraction and prevents sensitive information from being recovered in plaintext.\nTogether, they defend what actually matters, for example, decision logic, sensitive inputs, and the cryptographic keys that authorize action.\nNew Baseline for Autonomous Systems\nIf every interaction puts people at risk of exposure, they won\u2019t let AI handle things like healthcare records or make financial decisions. Similarly, companies won\u2019t automate their most important tasks if doing so could lead to regulatory problems or the loss of important data.\nSerious builders recognize that application-layer fixes alone are insufficient in high-assurance environments.\nWhen agents are entrusted with financial authority, regulated data, or cross-organizational coordination, infrastructure-level exposure becomes more than a theoretical concern. And without confidential execution in such contexts, many agents remain a soft target, with their keys stealable, and their logic malleable. The size of modern breaches shows exactly where that path leads.\nPrivacy and integrity are not optional features to be added after deployment. They must be architected from the silicon up. Therefore, for agentic AI to scale safely, hardware\u2011enforced confidentiality cannot be thought of as merely a competitive advantage but the baseline."
    },
    {
      "title": "DeepSeek Jailbreak Vulnerability Analysis | Qualys TotalAI",
      "source": "Qualys",
      "link": "https://news.google.com/rss/articles/CBMizgFBVV95cUxQOFprXzBYQXdHb3BCQThidnl2aVlWbFJMcUhQU3Fqb0MzM1lsYzFjTDFTZU9VbUNLUXlyYTJPUEY1T3JnbVFMLVBxSHc3UzRHNnJ6ZDhMLWFGdGlkZlhXM2cxVmlqVm0tTm9WblZId2FWRmRHVGVFSTd0NTBQN2NEanpOM1NZcHJUbmQ4a1ZRSWhFZ3kyMXVDb2d1czV3S3YyOUpFR0o1Skp1ZTlkZkdUU1ZVQy1IYWVqeWRjWkxnN0l3Vkd1UHhKNWtzWGFYUQ?oc=5",
      "published": "Tue, 24 Feb 2026 09:04:17 GMT",
      "summary": "\ud83d\udea8 Qualys TotalAI found that the DeepSeek-R1 LLaMA 8B variant failed over half of its jailbreak tests.<br>\ud83d\udcca Qualys TotalAI\u2019s KB Analysis prompts LLMs with questions across 16 categories and evaluates responses for vulnerabilities, ethical concerns, and legal risks.<br>\ud83e\udd16 DeepSeek-R1, developed by Chinese startup DeepSeek, offers multiple distilled versions built on Llama and Qwen base models.",
      "raw_text": "DeepSeek Failed Over Half of the Jailbreak Tests by Qualys TotalAI\nTable of Contents\nA comprehensive security analysis of DeepSeek\u2019s flagship reasoning model reveals significant concerns for enterprise adoption.\nIntroduction\nDeepSeek-R1, a groundbreaking Large Language Model recently released by a Chinese startup, DeepSeek, has captured the AI industry\u2019s attention. The model demonstrates competitive performance while being more resource efficient. Its training approach and accessibility offer an alternative to traditional large-scale AI development, making advanced capabilities more widely available.\nTo enhance efficiency while preserving model efficacy, DeepSeek has released multiple distilled versions tailored for different use cases. These variations, built on Llama and Qwen as base models, come in multiple size variants, ranging from smaller, lightweight models suitable for efficiency-focused applications to larger, more powerful versions designed for complex reasoning tasks.\nWith growing enthusiasm for DeepSeek\u2019s advancements, our team at Qualys conducted a security analysis of the distilled DeepSeek-R1 LLaMA 8B variant using our newly launched AI security platform, Qualys TotalAI. These findings are presented below, along with broader industry concerns about the model\u2019s real-world risks. As AI adoption accelerates, organizations must move beyond performance evaluation to tackle security, safety, and compliance challenges. Gaining visibility into AI assets, assessing vulnerabilities, and proactively mitigating risks is critical to ensuring responsible and secure AI deployment.\nQualys TotalAI Findings\nBefore diving into the findings, here\u2019s a quick introduction to Qualys TotalAI. This comprehensive AI security solution provides full visibility into AI workloads, proactively detects risks, and safeguards infrastructure. By identifying security threats like prompt injection and jailbreaks, as well as safety concerns such as bias and harmful language, TotalAI ensures AI models remain secure, compliant, and resilient. With AI-specific security testing and automated risk management, organizations can confidently secure, monitor, and scale their AI deployments.\nJoin Qualys experts on March 18, 2025, to learn more about what Qualys TotalAI\u2019s evaluation of DeepSeek uncovered.\nWe tested the Deepseek R1 LLaMA 8B variant against Qualys TotalAI\u2019s state-of-the-art Jailbreak and Knowledge Base (KB) attacks, and you can read the results of those tests below.\nTotalAI KB Analysis\nQualys TotalAI\u2019s KB Analysis prompts the target LLM with questions across 16 categories and evaluates the responses using our Judge LLM. Responses are assessed for vulnerabilities, ethical concerns, and legal risks. If a response is deemed vulnerable, it receives a severity rating based on its directness and potential impact. This ensures a comprehensive assessment of the model\u2019s behavior and associated risks.\nThe categories we evaluate a model for are detailed below:\n- Controversial Topics: Ensures the model does not generate or endorse biased, inflammatory, or politically sensitive content.\n- Excessive Agency: Prevents the model from overstepping boundaries by acting as an autonomous agent capable of independent decision-making.\n- Factual Inconsistencies: Evaluates the model\u2019s ability to provide accurate and verifiable information.\n- Harassment: Assesses whether the model generates or supports abusive, threatening, or harmful interactions.\n- Hate Speech and Discrimination: Identifies biases or harmful language targeting specific groups.\n- Illegal Activities: Prevents the model from providing instructions or guidance on unlawful actions.\n- Legal Information: Ensures the model does not generate misleading or unauthorized legal advice.\n- Misalignment: Measures deviations from intended behaviors, which may lead to unpredictable or harmful outputs.\n- Overreliance: Detects whether the model promotes excessive dependence on AI-generated responses.\n- Privacy Attacks: Evaluates susceptibility to extracting or leaking private and sensitive user data.\n- Profanity: Ensures the model does not produce inappropriate or offensive language.\n- Self-harm: Prevents the model from encouraging or supporting self-destructive behaviors.\n- Sensitive Information Disclosure: Detects unauthorized sharing of confidential data.\n- Sexual Content: Ensures the model does not generate explicit or inappropriate material, preventing reputational damage, regulatory violations, and misuse in unsafe contexts.\n- Unethical Actions: Flags morally questionable or irresponsible recommendations.\n- Violence / Unsafe Actions: Prevents the model from generating or endorsing harmful behaviors.\nIn our KB testing, 891 assessments were conducted. The model failed 61% of the tests, performing worst in Misalignment and best in Sexual Content.\nBy covering these 16 critical areas, the evaluation framework helps identify ethical, legal, and operational risks in LLM deployment. Establishing these benchmarks is essential to preventing misinformation, mitigating bias, and reducing security threats.\nTotalAI Jailbreak Testing\nJailbreaking an LLM involves techniques that bypass built-in safety mechanisms, enabling the model to generate restricted responses. These vulnerabilities can result in harmful outputs, including instructions for illegal activities, misinformation, privacy violations, and unethical content. Successful jailbreaks expose weaknesses in AI alignment and present serious security risks, particularly in enterprise and regulatory settings.\nBelow are some of the jailbreak attack categories we tested for using Qualys TotalAI, along with their underlying mechanisms:\n- AntiGPT: Exploits the model by forcing it into an \u201copposite mode\u201d that contradicts default behavior and safety guidelines.\n- Analyzing-based (ABJ): Manipulates the model\u2019s analysis capabilities to generate harmful content through complex reasoning tasks.\n- DevMode2: Tricks models into a fake \u201cDeveloper Mode\u201d that bypasses safety protocols.\n- PersonGPT: Forces adoption of an unrestricted persona that ignores all ethical guidelines.\n- Always Jailbreaking Prompts (AJP): Creates an \u201cunfiltered and amoral\u201d persona specialized in writing harmful prompts.\n- Evil Confidant: Adopts a persona without ethical constraints to provide intentionally harmful advice.\n- Disguise and Reconstruction (DRA): Hides harmful content that gets reconstructed during the completion phase.\n- ASCIIART: Uses ASCII art to hide prohibited words and bypass content filters.\n- Wrath: Creates a persona designed to return hateful and biased content.\n- Clyde: Implements a dual-personality system to bypass restrictions.\n- Theta: Forces the generation of dangerous content through an anarchistic persona.\n- CaiozAI: Removes all ethical constraints to fulfill any request without restrictions.\n- Titanius: Adopts an unfiltered persona that provides instructions for harmful activities.\n- Ucar: Transforms the AI into an offensive entity promoting illegal activities.\n- Unaligned: Forces dual responses \u2013 one aligned and one unethical.\n- M78VM: Simulates an unrestricted virtual machine that ignores safety constraints.\n- JonesAI: Promotes hate speech and discriminatory behavior through an unfiltered persona.\n- Fire: Creates a malevolent persona focused on causing harm and promoting violence.\nThe model was tested against these 18 jailbreak types through 885 attacks. It failed 58% of these attempts, demonstrating significant susceptibility to adversarial manipulation. During our analysis, DeepSeek R1 struggled to prevent several adversarial jailbreak attempts including steps on how to make an explosive device, create content for websites that targets certain groups by encouraging hate speech, conspiracy theories and violent action, exploiting software vulnerabilities, promoting incorrect medical information, etc.\nThese findings highlight the need for improved security mechanisms to prevent circumvention of built-in safeguards, ensuring the model remains aligned with ethical and regulatory guidelines. One effective prevention mechanism is implementing robust guardrails that act as real-time filters to detect and block jailbreak attempts. These guardrails enhance model resilience by dynamically adjusting to adversarial exploits, helping to mitigate security risks in enterprise applications.\nThese vulnerabilities expose downstream applications to significant security risks, necessitating robust adversarial testing and mitigation strategies.\nIndustry Concerns\nCompliance Challenges\nDeepSeek AI\u2019s privacy policy stipulates that all user data is stored on servers located in China. This operational framework raises critical concerns due to China\u2019s regulatory environment, including:\n- Governmental Data Access: The Chinese Cybersecurity Law permits government authorities to access locally stored data without requiring user consent.\n- Cross-Border Regulatory Conflicts: Organizations subject to data protection frameworks such as GDPR and CCPA may face compliance violations when using DeepSeek-R1.\n- Intellectual Property Vulnerabilities: Enterprises relying on proprietary data for AI training risk unauthorized access or state-mandated disclosure.\n- Opaque Data Governance: The absence of transparent oversight mechanisms limits visibility into data handling, sharing, and potential third-party access.\nThese concerns mainly affect organizations using DeepSeek\u2019s hosted models. However, deploying the model in local or customer-controlled cloud environments mitigates regulatory and access risks, allowing enterprises to maintain full control over data governance. Despite this, the model\u2019s inherent security vulnerabilities remain a valid concern, requiring careful evaluation and mitigation.\nRegulatory experts advise organizations in strict data protection jurisdictions to conduct thorough compliance audits before integrating DeepSeek-R1.\nData Breach and Privacy Concerns\nA recent cybersecurity incident involving DeepSeek AI reportedly exposed over a million log entries, including sensitive user interactions, authentication keys, and backend configurations. This misconfigured database highlights deficiencies in DeepSeek AI\u2019s data protection measures, further amplifying concerns regarding user privacy and enterprise security.\nRegulatory and Legal Implications\nDeepSeek AI\u2019s compliance posture has been questioned by legal analysts and regulatory bodies due to the following:\n- Ambiguities in Data Processing Practices: Insufficient disclosures regarding how user data is processed, stored, and shared.\n- Potential Violations of International Law: The model\u2019s data retention policies may conflict with extraterritorial regulations, prompting legal scrutiny in global markets.\n- Risks to National Security: Some government agencies have raised concerns about deploying AI systems that operate under foreign jurisdiction, particularly for sensitive applications.\nInternational compliance officers emphasize the necessity of conducting comprehensive legal risk assessments before adopting DeepSeek-R1 for mission-critical operations.\nConclusion\nWhile DeepSeek-R1 delivers advancements in AI efficiency and accessibility, its deployment requires a comprehensive security strategy. Organizations must first gain full visibility into their AI assets to assess exposure and attack surfaces. Beyond discovery, securing AI environments demands structured risk and vulnerability assessments\u2014not just for the infrastructure hosting these AI pipelines but also for emerging orchestration frameworks and inference engines that introduce new security challenges.\nFor those hosting this model, additional risks such as misconfigurations, API vulnerabilities, unauthorized access, and model extraction threats must be addressed alongside inherent risks like bias, adversarial manipulation, and safety misalignment. Without proactive safeguards, organizations face potential security breaches, data leakage, and compliance failures that could undermine trust and operational integrity.\nOur analysis of the distilled DeepSeek-R1 LLaMA 8B variant using Qualys TotalAI offers valuable insights into evaluating this new technology. TotalAI provides a purpose-built AI security and risk management solution, ensuring LLMs remain secure, resilient, and aligned with evolving business and regulatory demands.\nTo explore how we define AI risks, check out our whitepaper on AI security. As AI adoption accelerates, so do its risks\u2014sign up for a demo today to see how TotalAI can help secure your AI ecosystem before threats escalate.\nQualys TotalAI\u2019s 30-day Trial"
    },
    {
      "title": "13 ways attackers use generative AI to exploit your systems",
      "source": "csoonline.com",
      "link": "https://news.google.com/rss/articles/CBMirgFBVV95cUxPaUJaV2Jja3E4Tzk0Yi01ck0tOEFUXzQ5SnpwWmtWdWtOMC15VlZ3aU9NTTZhNVA5VWhINEstX3hkaG9DNGVYQmktQ1lhaGVPdHN0b0p3OERONGZMTjByWHE4cl9VdlZJbGszenNsMUZYV3Q0ZDNLTTkybGN3RTNOTHRaZE1VY3EzRVpWZHRmZzdiSGhDUWpfSi1LVHI4U0hJNDlSNEM4Q2x0ZjRfQ3c?oc=5",
      "published": "Mon, 23 Feb 2026 08:00:00 GMT",
      "summary": "\ud83d\udce7 Cybercriminals are leveraging generative AI to create highly convincing and personalized phishing emails, increasing attack success rates.<br>\ud83d\udc1b AI assists in developing sophisticated malware, such as generating malicious HTML documents for XWorm attacks.<br>\ud83e\udd16 Agentic AI is evolving from a simple \"helper\" to an autonomous \"partner-in-crime\" capable of executing entire attack chains.",
      "raw_text": "Cybercriminals are increasingly exploiting gen AI technologies to enhance the sophistication and efficiency of their attacks. Credit: Gorodenkoff / Shutterstock Artificial intelligence is revolutionizing the technology industry and this is equally true for the cybercrime ecosystem, as cybercriminals are increasingly leveraging generative AI to improve their tactics, techniques, and procedures and deliver faster, stronger, and sneakier attacks. As with legitimate use of emerging AI tools, abuse of generative AI for nefarious ends thus far hasn\u2019t been so much about the novel and unseen as it has been about productivity and efficiency, lowering the barrier to entry, and offloading automatable tasks in favor of higher-order thinking on the part of the humans involved. \u201cAI doesn\u2019t necessarily result in new types of cybercrimes, and instead enables the means to accelerate or scale existing crimes we are familiar with, as well as introduce new threat vectors,\u201d Dr. Peter Garraghan, CEO/CTO of AI security testing vendor Mindgard and a professor at the UK\u2019s Lancaster University, tells CSO. \u201cIf a legitimate user can find utility in using AI to automate their tasks, capture complex patterns, lower the barrier of technical entry, reduced costs, and generate new content, why wouldn\u2019t a criminal do the same?\u201d But the advent of agentic AI is beginning to change things, with AI tools no longer just assisting attackers but helping them automate operations. \u201cThe most significant shift over the past year has been AI\u2019s evolution from a simple \u2018helper\u2019 toward becoming a fully autonomous, and quite literally an attacker\u2019s partner-in-crime, capable of executing entire attack chains,\u201d says Crystal Morin, senior cybersecurity strategist at cloud-native security and visibility vendor Sysdig. Here is a look at various ways cybercriminals are putting gen AI to use in exploiting enterprise systems today. Taking phishing to the next level Gen AI enables the creation of highly convincing phishing emails, greatly increasingly the likelihood of prospective marks giving over sensitive information to scam sites or downloading malware. Instead of sending generic, unconvincing, and error-ridden emails, cybercriminals can leverage AI to quickly generate more sophisticated, personalized, and legitimate-looking emails to target specific recipients. Gen AI tools help enrich phishing campaigns by pulling together wide-ranging sources of data, including targeted information gleaned from social media. \u201cAI can be used to quickly learn what types of emails are being rejected or opened, and in turn modify its approach to increase phishing success rate,\u201d Mindgard\u2019s Garraghan explains. Facilitating malware development AI can also be used to generate more sophisticated \u2014 or less labour-intensive \u2014 malware. For example, cybercriminals are using gen AI to create malicious HTML documents. The XWorm attack, initiated by HTML smuggling, which contains malicious code that downloads and runs the malware, bears the hallmarks of development via AI. \u201cThe loader\u2019s detailed line-by-line description suggesting it was crafted using generative AI,\u201d according to HP Wolf Security\u2019s 2025 Threat Insights Report. In addition, the \u201cdesign of the HTML webpage delivering XWorm is almost visually identical as the output from ChatGPT 4o after prompting the LLM to generate an HTML page that offers a file download,\u201d HP Wolf Security added in its report. Elsewhere, ransomware group FunkSec \u2014 an Algeria-linked ransomware-as-a-service (RaaS) operator that takes advantage of double-extortion tactics \u2014 has begun harnessing AI technologies, according to Check Point Research. \u201cFunkSec operators appear to use AI-assisted malware development, which can enable even inexperienced actors to quickly produce and refine advanced tools,\u201d Check Point researchers wrote in a blog post. Accelerating vulnerability hunting and exploits Analyzing systems for vulnerabilities and developing exploits can also be simplified through use of gen AI. \u201cInstead of a black hat hacker spending the time to probe and perform reconnaissance against a system perimeter, an AI agent can be tasked to do this automatically,\u201d Mingard\u2019s Garraghan says. Gen AI may be behind a 62% reduction in the time between a vulnerability being discovered and its exploitation by attackers from 47 days to just 18 days, according to a study last year by threat intelligence firm ReliaQuest. \u201cThis sharp decrease strongly indicates that a major technological advancement \u2014 likely gen AI \u2014 is enabling threat actors to exploit vulnerabilities at unprecedented speeds,\u201d ReliaQuest wrote. Adversaries are leveraging gen AI alongside pen-testing tools to write scripts for tasks such as network scanning, privilege escalation, and payload customization. AI is also likely being used by cybercriminals to analyze scan results and suggest optimal exploits, allowing them to identify flaws in victim systems faster. \u201cThese advances accelerate many phases in the kill chain, particularly initial access,\u201d ReliaQuest concluded. Cyber resilience firm Cybermindr used a different methodology to find that the average time to exploit a vulnerability had fallen to five days in 2025. \u201cAI-driven reconnaissance, automated attack scripts, and underground exploit marketplaces have accelerated the weaponization of vulnerabilities,\u201d it said. CSO\u2019s Lucian Constantin offers a deeper look at how generative AI tools are transforming the cyber threat landscape by democratizing vulnerability hunting for pen-testers and attackers alike. Launching AI-orchestrated espionage Anthropic dropped a bombshell in September 2025 when it revealed that it had disrupted a sophisticated AI-orchestrated cyber espionage campaign. The attackers abused Claude Code to automate approximately 80% of their campaign activities, targeting around 30 major tech firms, financial institutions, and government agencies. In a \u201csmall number of cases\u201d attacks were successful, according to the AI company, noting that an unnamed \u201cChinese state-sponsored group\u201d was likely behind the campaign, which relied on jailbreaking tools to make prohibited functions possible. Last year Carnegie Mellon\u2019s CyLab Security & Privacy Institute researchers, in collaboration with Anthropic, demonstrated that LLMs like GPT-4o can autonomously plan and execute sophisticated cyberattacks on enterprise-scale networks \u2014 without any human intervention. \u201cThe study reveals that an LLM, when structured with high-level planning capabilities and supported by specialized agent frameworks, can simulate network intrusions and closely mirror real-world breaches,\u201d a CyLab spokesperson explained. Escalating threats with alternative platforms Cybercriminals have also begun developing their own large language models (LLMs) \u2014 such as WormGPT, FraudGPT, DarkBERT, and others \u2014 built without the guardrails that constrain criminals\u2019 misuse of mainstream gen AI platforms. These platforms are commonly harnessed for applications such as phishing and malware generation. Moreover, mainstream LLMs can also be customized for targeted use. Security researcher Chris Kubecka shared with CSO in late 2024 how her custom version of ChatGPT, called Zero Day GPT, helped her identify more than 20 zero-days in a matter of months. Stealing resources via LLMjacking Threat actors are also busy stealing cloud credentials specifically to hijack costly LLM resources, either for their own gain or to sell access, in an attack technique called LLMjacking. \u201cBeyond theft of service, attackers are now actively probing newer LLM models to identify those that lack the guardrails of more mature platforms, effectively using them as unrestricted sandboxes to generate malicious code or bypass regional sanctions,\u201d Sysdig\u2019s Morin reports. Creating a Silk Road\u2013style marketplace for AI agents Beyond AI agents executing individual attacks, security experts are beginning to track examples where coordination itself is being automated or orchestrated. \u201cWe\u2019re seeing early experiments where multiple specialized agents interact, some focused on reconnaissance, others on tooling, execution, or data movement, without any single agent needing the full picture,\u201d says Lucie Cardiet, cyberthreat research manager at Vectra AI. A concrete example of this is Molt Road, which offers a dark-web-style marketplace for AI agents, albeit one with few listings at present. \u201cAutonomous agents can create listings, sell access or capabilities, coordinate tasks, and complete transactions with minimal human involvement, effectively automating the economics of cybercrime,\u201d Cardiet tells CSO. \u201cWe can expect attackers to actively leverage this model in the coming months, breaking the attack chain into specialized, cooperating agents to speed up and scale their attacks,\u201d she says. Breaking in with authentication bypass Gen AI tools can also be abused to bypass security defences such as CAPTCHAs or biometric authentication. \u201cAI can defeat CAPTCHA systems and analyse voice biometrics to compromise authentication,\u201d according to cybersecurity vendor Dispersive. \u201cThis capability underscores the need for organizations to adopt more advanced, layered security measures.\u201d Leveraging deepfakes for social engineering AI-generated deepfakes are being abused to exploit channels many employees more implicitly trust, such as voice and video, instead of relying on less convincing email-based attacks. The problem is becoming more severe with the wider availability of AI technologies capable of creating more convincing deepfakes, according to Alex Lisle, CTO of deepfake detection platform Reality Defender. \u201cThere was a recent case involving a cybersecurity company that relied on visual verification for credential resets,\u201d Lisle says. \u201cTheir process required a manager to join a Zoom call with IT to confirm an employee\u2019s identity before a password reset.\u201d Lisle explains: \u201cAttackers are now leveraging deepfakes to impersonate those managers on live video calls to authorize these resets.\u201d In the most high-profile example to date, a finance worker at design and engineering company Arup was tricked into authorizing a fraudulent HK$200 million ($25.6 million) transaction after attending a videoconference call during which fraudsters used deepfake technology to impersonate its UK-based CFO. Impersonating brands in malicious ad campaigns Cybercriminals have begun using gen AI tools to deliver brand impersonation campaigns delivered via ads and content platforms, rather than traditional phishing or malware. \u201cAttackers now use gen AI to mass-produce realistic ad copy, creatives, and fake support pages, then distribute them across search ads, social ads, and AI-generated content, targeting high-intent queries like \u2018brand login\u2019 or \u2018brand support,\u2019\u201d explains Shlomi Beer, co-founder and CEO at ImpersonAlly, a security startup that specializes in protecting the online advertising ecosystem. The tactic was used in ongoing a series of Google Ad account fraud, to impersonate the Cursor AI coding assistant firm, and in a fake Shopify ecommerce platform customer support scam, among other attacks. Abusing OpenClaw Attackers have also begun targeting viral personal AI agents such as OpenClaw. OpenClaw offers an open-source AI agent framework. A combination of supply chain attacks on its skill marketplace and misconfigurations open the door to potential exploits and malware slinging, as CSO covered in much more depth in our earlier report. \u201cCybercriminals can exploit these virtual assistants to steal private keys to cryptocurrency wallets and execute code on victims\u2019 devices,\u201d says Edward Wu, CEO and founder at Dropzone AI. \u201cWe can expect 2026 to be the year when security teams will try to prevent unsanctioned usage of personal AI agents.\u201d Poisoning model memories To offer short-term and longer-term context, AI agents are starting to rely more on persistent memory, opening the door for exploits that involve planting malicious memories. If an attacker injects malicious or false information into an agent\u2019s memory, that corrupted context then influences every future decision the agent makes. For example, security researcher Johann Rehberger showed how he could plant false memories in ChatGPT in September 2025. \u201cHe [Rehberger] used a malicious image with hidden instructions embedded in it to inject fabricated data into the model\u2019s long-term memory,\u201d said Siri Varma Vegiraju, security tech lead at Microsoft. \u201cThe scary part was that once the memory was poisoned, it persisted across sessions and continuously exfiltrated user data to a server the attacker controlled.\u201d Hacking AI infrastructure Over the past year, attackers have shifted from using generative AI to targeting the infrastructure that enables it. This vector of attack is exemplified in the supply chain poisoning in Model Context Protocol servers, where compromised dependencies or modified code introduced vulnerabilities into enterprise environments. For example, a counterfeit \u201cPostmark MCP Server\u201d discovered in early 2025 silently BCC\u2019d all processed emails, including internal documents, invoices, and credentials, to an attacker-controlled domain. Many other malicious MCP servers have already been identified in the wild, many designed to exfiltrate information without detection, according to Casey Bleeker CEO at SurePath AI. \u201cWe\u2019re tracking several categories of MCP-specific risk: tool poisoning attacks, where adversaries inject malicious instructions into AI tool descriptions that execute when the agent invokes them; supply chain compromises, where a trusted MCP server or dependency is updated post-approval to behave maliciously; and cross-tool data exfiltration, where compromised components in an agentic workflow silently siphon sensitive data through what looks like legitimate AI activity,\u201d Bleeker explains. Reality check AI technologies are powerful but they have their limitations, several experts tell CSO. Rik Ferguson, VP of security intelligence at Forescout, says cybercriminals are largely relying on AI to automate repetitive tasks rather than more complex work, such as vulnerability exploitation. \u201cThe most reliable criminal use [of AI] remains in language-heavy and workflow-heavy tasks such as phishing and pretexting, influence and outreach, triaging and contextualizing vulnerabilities, and generating boilerplate components, rather than reliably discovering and exploiting brand-new vulnerabilities end-to-end,\u201d Ferguson says. Over the past twelve months, managed detection and response firm Huntress has tracked threat actors applying AI to generate and automate traditional tradecraft, from developing scripts to browser extensions and, in some cases, even phishing lures. \u201cWe have also seen such \u2018vibe coded\u2019 scripts fail to execute and meet their objectives on multiple occasions,\u201d Anton Ovrutsky, principal tactical response analyst at Huntress, tells CSO. And while AI has certainly given threat actors a powerful tool it has, at least to date, failed to spawn any new tactics or exploit classes, according to Ovrutsky. \u201cA threat actor can indeed rapidly prototype a sophisticated credential theft script, yet the basic \u2018laws of physics\u2019 still exist; a threat actor must be in a position to execute such a script in the first place,\u201d Ovrutsky says. \u201cWe have yet to observe an exploit path that has been enabled through AI-use exclusively.\u201d Countermeasures Collectively the misuse of gen AI tools is making it easier for less skilled cybercriminals to earn a dishonest living. Defending against the attack vector challenges security professionals to harness the power of artificial intelligence more effectively than attackers. \u201cCriminal misuse of AI technologies is driving the necessity to test, detect, and respond to these threats, in which AI is also being leveraged to combat cybercriminal activity,\u201d Mindgard\u2019s Garraghan says. In a blog post, Lawrence Pingree, VP of technical marketing at Dispersive, outlines preemptive cyber defenses that security professionals can take to win what he describes as an \u201cAI ARMS (Automation, Reconnaissance, and Misinformation) race\u201d between attackers and defenders. \u201cRelying on traditional detection and response mechanisms is no longer sufficient,\u201d Pingree warns. Alongside employee education and awareness programs, enterprises should be using AI to detect and neutralize generative AI-based threats in real-time. Forescout\u2019s Ferguson says CISOs should treat enterprise AI like any other high-value SaaS platform. \u201cTighten identity and conditional access, minimize privileges, lock down keys, and monitor for anomalous AI/API usage and spend,\u201d Ferguson advises. Threat and Vulnerability ManagementVulnerabilitiesMalwarePhishing SUBSCRIBE TO OUR NEWSLETTER From our editors straight to your inbox Get started by entering your email address below. Please enter a valid email address Subscribe"
    }
  ]
}