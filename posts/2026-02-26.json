{
  "date": "Thursday, February 26, 2026",
  "weather": {
    "current_temp": 32,
    "unit": "F",
    "conditions": "Mostly Sunny",
    "high": 39,
    "low": 26,
    "forecast": "A slight chance of rain and snow between 1pm and 4pm. Partly sunny, with a high near 39. North wind around 6 mph. Chance of precipitation is 20%.",
    "hourly": [
      {
        "label": "7am",
        "hour": 7,
        "temp": 32,
        "conditions": "Mostly Sunny",
        "wind": "5 mph W",
        "humidity": "72%",
        "precip_chance": "0%"
      },
      {
        "label": "9am",
        "hour": 9,
        "temp": 34,
        "conditions": "Partly Sunny",
        "wind": "5 mph NW",
        "humidity": "75%",
        "precip_chance": "0%"
      },
      {
        "label": "3pm",
        "hour": 15,
        "temp": 38,
        "conditions": "Slight Chance Rain And Snow",
        "wind": "6 mph E",
        "humidity": "67%",
        "precip_chance": "19%"
      },
      {
        "label": "5pm",
        "hour": 17,
        "temp": 39,
        "conditions": "Mostly Cloudy",
        "wind": "6 mph E",
        "humidity": "64%",
        "precip_chance": "14%"
      },
      {
        "label": "7pm",
        "hour": 19,
        "temp": 38,
        "conditions": "Mostly Cloudy",
        "wind": "6 mph NE",
        "humidity": "67%",
        "precip_chance": "5%"
      }
    ]
  },
  "news": [
    {
      "title": "Anthropic acquires computer-use AI startup Vercept after Meta poached one of its founders",
      "source": "TechCrunch",
      "link": "https://techcrunch.com/2026/02/25/anthropic-acquires-vercept-ai-startup-agents-computer-use-founders-investors/",
      "published": "Wed, 25 Feb 2026 23:49:19 +0000",
      "raw_text": "Anthropic on Wednesday announced that it has acquired Vercept, an AI startup with deep roots to some of the biggest names in Seattle\u2019s tech scene. The acquisition marks the latest after Anthropic acquired coding agent engine Bun in December to help scale Claude Code.\nVercept had created tools for more complex agentic tasks, including its product Vy, a computer-use agent in the cloud that could operate a remote Apple MacBook. Vercept is one of the many startups working on re-imagining the personal computer for the age of AI agents. As part of the deal, Anthropic is shuttering Vercept\u2019s product on March 25.\nThe startup was a grad of Seattle\u2019s AI-focused incubator A12, which spawned from the longstanding Allen Institute for AI. Vercept\u2019s co-founders had roots with the Allen Institute, as well, and were previously researchers there. One co-founder, Matt Deitke, made news last year as one of the AI researchers who negotiated a monster $250 million salary from Meta to join its Superintelligence Lab. On Wednesday, Deitke congratulated his former colleagues in a post on X.\nVercept was a relatively high-profile AI startup in the region. In a LinkedIn post announcing the acquisition by Anthropic, Vercept CEO Kiana Ehsani said the startup had raised a total of $50 million. She called out A12\u2019s Seth Bannon, a board member, as the lead investor. Vercept previously announced it had raised a $16 million seed round last January.\nThe list of angel investors was impressive, too, and included former Google CEO Eric Schmidt, Google DeepMind chief scientist Jeff Dean, Cruise founder Kyle Vogt, and Dropbox co-founder Arash Ferdowsi, GeekWire reported.\nIn Anthropic\u2019s announcement of the acquisition, the company named co-founders Ehsani, Luca Weihs, and Ross Girshick as some of the team brought on to join Anthropic in the acquisition. However, not all of Vercept\u2019s co-founders are joining the Claude maker.\nOren Etzioni, who has previously been named as a co-founder of Vercept and investor in the startup, is well known in Seattle as the founding leader of the Allen Institute for AI. Along with Deitke, he is also not joining Anthropic, and was vocally less pleased about the acqui-hire. He posted on LinkedIn: \u201cAfter a little bit more than a year, Vercept is throwing in the towel and giving their customers 30 days to get off the platform. Sad. A fantastic team is joining Anthropic. I wish them the very best!\u201d\nSave up to $300 or 30% to TechCrunch Founder Summit\n1,000+ founders and investors come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately.\nOffer ends March 13.\nSave up to $300 or 30% to TechCrunch Founder Summit\n1,000+ founders and investors come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately\nOffer ends March 13.\nEtzioni is also a professor at the University of Washington and known for other startups he\u2019s founded and backed as a VC. He did not respond to a request for comment.\nOn Etzioni\u2019s LinkedIn post, he accused Bannon, the Vercept lead investor, of being \u201cpartly responsible\u201d for Vercept not hiring the correct business people. A back and forth ensued between the investors, with Bannon condemning Etzioni\u2019s remarks: \u201c\u2026 you disparaged the heroic work of the founders for achieving an outcome most could only dream of,\u201d Bannon replied in the LinkedIn string. They also accused each other of other less savory things like lying and legal threats.\nWhile public spats between investors are entertaining, and essentially meaningless, the underlying motivation is notable. The stakes are high to build the next big AI winner, and now a promising startup that raised a decently sized war chest will be tucked into Anthropic.\nWhile the terms of the deal were not disclosed, Etzioni says he got a return on his money. Anthropic clearly wanted these researchers (perhaps \u2014 especially \u2014 with another of them at Meta).\nStill, Etzioni told GeekWire that he remains bummed. \u201cI\u2019m pleased to have gotten a positive return but obviously disappointed that after just a little over a year with so much traction, and such a fantastic team, we\u2019re basically throwing in the towel,\u201d he said.\nThe founders joining Anthropic, however, appear happy, according to CEO\u2019s Ehsani\u2019s LinkedIn post. \u201cThe choices were clear: we could build independently and work toward the same vision as two separate versions of it, or join forces with an incredible team and accelerate that vision into reality. The decision became an easy choice,\u201d she said of joining Anthropic.",
      "summary": "\ud83d\uded2 Anthropic acquired Vercept, an AI startup that developed Vy, a computer-use agent capable of operating a remote Apple MacBook in the cloud.<br>\ud83d\udcb0 Vercept had raised a total of $50 million, including a $16 million seed round from January of the prior year.<br>\ud83d\udc4b Vercept co-founder Matt Deitke was previously poached by Meta with a $250 million salary to join its Superintelligence Lab."
    },
    {
      "title": "Wearable startup CUDIS launches a new health ring line with an AI-fueled \u2018coach\u2019",
      "source": "TechCrunch",
      "link": "https://techcrunch.com/2026/02/25/wearable-startup-cudis-launches-a-new-health-ring-line-with-an-ai-fueled-coach/",
      "published": "Wed, 25 Feb 2026 19:10:04 +0000",
      "raw_text": "Wearables startup CUDIS is launching its newest series of health rings this week. The updated ring comes equipped with a number of features, including an AI \u201cagent coach\u201d designed to keep users on track to attain their fitness goals.\nCUDIS says it differentiates itself from other wearables by not just delivering health metrics but also incentivizing healthy behavior through a points system. Users garner digital \u201chealth points\u201d for healthy behaviors \u2014 things like daily sleep, 10,000 steps every day, sports activities, and conversations with the ring\u2019s AI coach \u2014 which can then be redeemed through an integrated marketplace for discounts on health supplements and other products.\nThe ring\u2019s AI Agent Coach, meanwhile, is designed to leverage generative AI to aid with healthy programs for exercise and daily health. The company says that its agent generates tailored programs including \u201cdaily tasks, recovery protocols, supplement recommendations, and direct referrals to licensed medical professionals.\u201d\nThe ring also tracks a host of body metrics and daily behaviors, such as sleep quality, stress management, movement, and recovery. This helps them see how these metrics affect their Pace of Aging (PoA), showing whether their body is aging faster or slower than their chronological age, the company explains.\nCUDIS CEO and co-founder Edison Chen told TechCrunch that since his company\u2019s first wearable was launched in 2024, the company has sold over 30,000 units across its first two models. The app\u2019s user base has also grown to 250,000 users across 103 countries, he added.\n\u201cOur strongest markets so far have been North America, Europe, and Asia,\u201d Chen said. \u201cWhat we\u2019re good at is pattern recognition for healthy people trying to optimize,\u201d Chen told TechCrunch.\n\u201cThe AI spots when you\u2019re trending in the wrong direction, such as chronic poor sleep, declining HRV, elevated resting heart rate, and either suggests lifestyle changes or connects you to a professional. The control is in the escalation pathway to the right care access,\u201d he said.\nSave up to $300 or 30% to TechCrunch Founder Summit\n1,000+ founders and investors come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately.\nOffer ends March 13.\nSave up to $300 or 30% to TechCrunch Founder Summit\n1,000+ founders and investors come together at TechCrunch Founder Summit 2026 for a full day focused on growth, execution, and real-world scaling. Learn from founders and investors who have shaped the industry. Connect with peers navigating similar growth stages. Walk away with tactics you can apply immediately\nOffer ends March 13.\nThe company claims that it keeps user data encrypted and secure via the Solana blockchain. It has previously been described as a \u201cweb3 AI wellness company.\u201d (TechCrunch was not able to test the smart ring directly to verify its security claims.)\nCUDIS announced $5 million of seed funding in 2024. The round was led by Draper Associates and included a number of other investors, including a number of blockchain-associated investor groups like Skybridge, DraperDragon, Monke Ventures, and Foresight Ventures, among others. The company also plans to launch a Kickstarter soon.",
      "summary": "\ud83d\udc8d CUDIS is launching a new series of health rings equipped with an AI \\\"agent coach\\\" that generates tailored exercise programs and recommendations.<br>\ud83c\udf81 The CUDIS ring incentivizes healthy behaviors with digital \\\"health points\\\" redeemable for discounts on health supplements and products.<br>\ud83d\udcc8 CUDIS CEO Edison Chen reported selling over 30,000 units of previous models since 2024 and expanding to 250,000 users across 103 countries."
    },
    {
      "title": "Google and Samsung just launched the AI features Apple couldn\u2019t with Siri",
      "source": "The Verge",
      "link": "https://www.theverge.com/tech/884703/google-samsung-galaxy-s26-gemini-apple-siri",
      "published": "2026-02-25T14:56:55-05:00",
      "raw_text": "Google just announced that Gemini will soon be able to take care of some multistep tasks on your phone, like ordering food or hailing a car, starting first with the Pixel 10, Pixel 10 Pro, and the just-announced Samsung Galaxy S26 phones. It all sounds a bit like features Apple announced for Siri way back at the 2024 Worldwide Developers Conference \u2014 before Apple delayed those planned features in March 2025 and which still aren\u2019t released.\nGoogle and Samsung just launched the AI features Apple couldn\u2019t with Siri\nGemini will be able to do multistep tasks on your phone, starting with ordering food or a ride.\nGemini will be able to do multistep tasks on your phone, starting with ordering food or a ride.\nOnstage, Sameer Samat, Google\u2019s president of Android, showed off a demo of how Gemini\u2019s new agentic features would work to help wrangle a pizza dinner order from his busy family group chat. Samat asks Gemini to look at the chat thread and figure out what to order, and then make the order with a delivery app. Onscreen \u2014 in a prerecorded video, it wasn\u2019t live \u2014 you can see Gemini figuring out what everyone wants from the group chat and showing that in a window. Then the user, via voice request, tells Gemini to complete that order, naming a specific pizzeria. Gemini then clicks through Grubhub to prep the order, all still onscreen. When the order is ready, Gemini sends an alert so the user can review it and actually press the submit button.\nSetting aside that this situation doesn\u2019t seem that complicated to do by yourself in the Grubhub app (or even by calling the pizzeria to talk through it with a human), this is a potentially big moment for agentic AI. Google just recently added the ability for Gemini to auto-browse for users in Chrome, and being able to do something similar right inside of Android feels like a logical next step; Google clearly wants Gemini to be thought of as a helpful agent or productivity partner rather than just a chatbot or a series of AI models.\nAssuming the agentic Gemini features also launch \u201csoon\u201d like Google is promising and that Apple doesn\u2019t pull a rabbit out of its hat, Google will also beat Apple to the punch on some of its most impressive Apple Intelligence demos \u2014 also only shown in prerecorded videos \u2014 from that WWDC 2024 show. One feature Apple showed off would have let Siri understand what\u2019s on your screen and take action on it, meaning you could ask Siri to add an address from a Messages thread to the contact card of that person you\u2019re texting with. Apple demoed how Siri would be able to take actions inside of and across apps for you. The company said Siri would even be able to understand your personal context, meaning you could ask it when your mom\u2019s flight was landing and the assistant would pull the information from an email and show it to you.\nNearly two years later, none of that is available yet. When Apple announced the features would be delayed, the company even pulled an advertisement showing off the features. And based on reporting from Bloomberg, some of the features may not arrive until iOS 27.\nThere are still many questions about Gemini\u2019s new capabilities, of course. They\u2019ll need to actually ship. We\u2019ll have to try them to see if they are as useful and functional as advertised \u2014 Google is calling this initial launch a \u201cbeta,\u201d so there could be some rough edges. And we don\u2019t know how many developers will actually let Gemini browse through their apps on behalf of users, which Verge editor-in-chief Nilay Patel likes to call the DoorDash problem. (Google says Gemini will be able to work in \u201cselect rideshare and food apps.\u201d)\nBut Google seems to have leapfrogged Apple in a big way, and now Apple has even more to do to catch up.\nMost Popular\n- Hands on: I\u2019m super impressed with the Galaxy S26 Ultra\u2019s new Privacy Display\n- Google Gemini can book an Uber or order food for you on Pixel 10 and Galaxy S26\n- Samsung Unpacked 2026: everything announced at the February event\n- Google and Samsung just launched the AI features Apple couldn\u2019t with Siri\n- Apple brings age verification to UK users in iOS 26.4 beta",
      "summary": "\ud83d\udcf1 Google announced Gemini will perform multistep tasks, like ordering food or hailing a car, launching first on Pixel 10, Pixel 10 Pro, and Samsung Galaxy S26 phones.<br>\ud83c\udf55 Google demonstrated Gemini taking a pizza order from a family group chat and placing it through Grubhub, requiring user review before submission.<br>\u231a Apple's similar Siri features, announced at WWDC 2024 to understand screen context and take cross-app actions, were delayed in March 2025 and remain unreleased."
    },
    {
      "title": "Iran, U.S. resume nuclear negotiations as Trump\u2019s war clock ticks down",
      "source": "The Washington Post",
      "link": "https://news.google.com/rss/articles/CBMinAFBVV95cUxQdlNQaTBiTl96R0RrYno3bTlrbWl4S2drZVVqN0NTUTU0OHpBWWlXSFhyTWdsV0VKRGxCRHJ0SVd0R3J2QzNUS21xUDBLZnpFTzJjcy1OTWFRdExCUmhROWRCQUxlVUoxUU5vNVREMXFvWkNTdzJBdGF6NWxJZHZHMUxxQ0hIMUpacThUbzdiTDlDaTdFM1VYbGdrOWI?oc=5",
      "published": "Thu, 26 Feb 2026 11:31:16 GMT",
      "raw_text": "",
      "summary": "\ud83c\uddee\ud83c\uddf7 Iran and the U.S. have resumed nuclear negotiations.<br>\u23f0 The resumption of talks is occurring as \\\"Trump's war clock\\\" ticks down.<br>\ud83d\udd4a\ufe0f The negotiations imply an ongoing diplomatic effort between the two nations on nuclear issues."
    },
    {
      "title": "Pentagon shifts toward maintaining ties to Scouting",
      "source": "NPR",
      "link": "https://www.npr.org/2026/02/26/nx-s1-5686528/pentagon-scouting-boys-girls-hegseth",
      "published": "Thu, 26 Feb 2026 06:00:00 -0500",
      "raw_text": "Pentagon shifts toward maintaining ties to Scouting\nPentagon shifts toward maintaining ties to Scouting\nAfter months of backlash, including from some Republicans, Defense Secretary Pete Hegseth seems to be easing off his effort to sever the Pentagon's century-long relationship with Scouting America, the youth organization formerly known as the Boy Scouts.\nDocuments reviewed by NPR last fall detailed Hegseth's intention to cut the Scouts off for abandoning what one memo called \"masculine virtues\" and for attacking \"boy-friendly spaces.\" The proposal would have barred Scout troops from meeting on U.S. bases, ended military assistance for the Scout Jamboree and eliminated a long-standing policy of offering advanced rank to Eagle Scouts who enlist.\nAbout 25,000 children of U.S. servicemembers are currently involved in Scouting America programs, according to the organization.\nA new document recently obtained by NPR shows contingency plans to withdraw support are still being developed. But according to a Defense Department source who asked not to be identified for fear of retribution, more effort is now going into negotiating a memorandum of understanding that will allow the partnership to continue.\nAccording to the DoD source, Pentagon demands include requiring new scouts to register under the sex assigned at birth. The current Scouting application form allows only \"male\" or \"female\" gender choices, and most troops are single gender. However, their official policy says all youth are welcome, regardless of \"gender or orientation.\"\nWhether this concession would satisfy Hegseth \u2014 and how it would affect transgender youth \u2014 remains unclear. The Pentagon didn't respond to a request for comment on the issue. A spokesman for Scouting America declined to comment.\nHegseth has been a vocal critic of Scouting's 2019 decision to admit girls and later to rebrand as Scouting America. As a Fox News host, he said the Boy Scouts had been \"cratering itself for quite some time,\" arguing that Scouting was diluted by progressive politics. An internal Pentagon memo reviewed by NPR stated \"The organization once endorsed by President Theodore Roosevelt no longer supports the future of American boys.\"\nWhen NPR reported on the memo last year, the Defense Department declined to comment, saying that they could not authenticate the documents and that they may have been \"pre-decisional\".\nRep. Don Bacon, a Nebraska Republican on the Armed Services Committee, said he confirmed NPR's reporting with the Pentagon, however. He said he'd been hearing from angry constituents.\n\"Some big-time Trump supporters in Omaha called me, enraged that Hegseth would be picking a fight with the Scouts.\" Bacon said. \"He's got bigger fish to fry. I've heard a lot of dumb stuff, but this is up there.\"\nRep. Adam Smith of Washington State, the top Democrat on the House Armed Services Committee, criticized the Pentagon's position and said he believes it's inappropriate for the DoD to press a youth organization this way.\n\"The single biggest problem is that Hegseth is hell bent on driving forward a right-wing cultural revolution that is very divisive,\" Smith said.\nFailure to reach a deal would have major ramifications for this summer's National Scout Jamboree. The campout draws as many as 20,000 youths and adult leaders to a remote site in West Virginia every four years \u2014 and requires extensive planning. According to letters between Scouting and the DoD in the documents reviewed by NPR, aid for this summer's gathering was first requested in late 2023. If the Pentagon withdraws support, the Scouts would have to scramble to replace medical care, transportation and other emergency services.",
      "summary": "\ud83c\udfdb\ufe0f Defense Secretary Pete Hegseth is easing efforts to sever the Pentagon's century-long relationship with Scouting America.<br>\ud83d\udeab Hegseth had previously proposed barring Scout troops from U.S. bases and ending military assistance for the Scout Jamboree.<br>\ud83e\udd1d Pentagon demands for continuing the partnership include requiring new scouts to register under their sex assigned at birth."
    }
  ],
  "podcasts": [
    {
      "podcast": "This Week in Startups",
      "title": "Behind the Scenes with an early OpenClaw contributor! | E2252",
      "published": "2026-02-26",
      "summary": "\ud83d\udcca Tyler Yust, the third-ever contributor to OpenClaw, shared his insights into the AI agent platform's foundation.<br>\ud83d\udca1 Deedy Das from Menlo Ventures discussed his skepticism about the \\\"Citrini article,\\\" considering it \\\"science fiction,\\\" and his belief that Enterprise SaaS apps will not be \\\"vibe coded.\\\"<br>\u2699\ufe0f Sebastian demonstrated a compact OpenClaw form factor, while the founder of OpenBrowse showed automatic detection and generation of OpenClaw skills.",
      "raw_text": "This Week In Startups is made possible by:Lemon IO - https://Lemon.io/twistEvery.io - https://every.io Sentry.io- https://sentry.io/twistToday\u2019s show:We\u2019re going behind the curtain today \u2014 it\u2019s a packed show!We found Tyler Yust, OpenClaw\u2019s third EVER contributor to share his insights from within foundation! We\u2019ve got Deedy Das, of Menlo Ventures, on the show to discuss whether SaaS is cooked! Next we met the creator of an OpenClaw instance that fits in your pocket! We\u2019ve also got the founder of OpenBrowse showing us how he automatically detects and generates OpenClaw skills!Timestamps:00:00 Intro - Deedy Das Joins the Show!04:54 Anthropic\u2019s revenue growth and valuation06:07 OpenClaw Contributor Tyler Yuts joins the show09:24 iMessage integration and Apple\u2019s proprietary systems00:10:07 Lemon.io - Get 15% off your first 4 weeks of developer time at https://Lemon.io/twist14:31 Anthropic vs. the Pentagon00:20:02 Every.io - For all of your incorporation, banking, payroll, benefits, accounting, taxes or other back-office administration needs, visit\u00a0https://every.io.00:30:08 Sentry - New users can get $240 in free credits when they go to https://sentry.io/twist and use the code TWIST00:35:46 The Infamous Citrini article00:32:47 Come to LAUNCH fest! https://fest.launch.co00:36:28 Why Deedy thinks the Cetrini article is a work of science fiction00:44:51 The illusion of privacy in corporate America00:41:18 Deedy thinks Enterprise SaaS apps aren\u2019t going to be vibe coded00:49:20 Jason\u2019s Reddit Bot00:52:01 Jason\u2019s obsession with Singapore\u2019s food00:55:22 How Unbrowse pulls any backend API!01:02:07 Sebastian shows off the smallest OpenClaw form factor!01:12:04 The Prolo ring \u2014 for people who doomscroll01:20:21 Deedy\u2019s Podcast Player App!Thank you to our partners:(10:07) Lemon.io - Get 15% off your first 4 weeks of developer time at https://Lemon.io/twist(20:02) Every.io - For all of your incorporation, banking, payroll, benefits, accounting, taxes or other back-office administration needs, visit every.io.(30:08) Sentry - New users can get $240 in free credits when they go to sentry.io/twist and use the code TWISTSubscribe to the TWiST500 newsletter: https://ticker.thisweekinstartups.comCheck out the TWIST500: https://www.twist500.comSubscribe to This Week in Startups on Apple: https://rb.gy/v19fcpFollow Lon:X: https://x.com/lonsFollow Alex:X: https://x.com/alexLinkedIn: \u2060https://www.linkedin.com/in/alexwilhelmFollow Jason:X: https://twitter.com/JasonLinkedIn: https://www.linkedin.com/in/jasoncalacanisCheck out all our partner offers: https://partners.launch.co/Great TWIST interviews: Will Guidara, Eoghan McCabe, Steve Huffman, Brian Chesky, Bob Moesta, Aaron Levie, Sophia Amoruso, Reid Hoffman, Frank Slootman, Billy McFarlandCheck out Jason\u2019s suite of newsletters: https://substack.com/@calacanisFollow TWiST:Twitter: https://twitter.com/TWiStartupsYouTube: https://www.youtube.com/thisweekinInstagram: https://www.instagram.com/thisweekinstartupsTikTok: https://www.tiktok.com/@thisweekinstartupsSubstack: https://twistartups.substack.com",
      "link": "https://podcasters.spotify.com/pod/show/thisweekinstartups/episodes/Behind-the-Scenes-with-an-early-OpenClaw-contributor---E2252-e3fkbo0"
    },
    {
      "podcast": "Morning Brew Daily",
      "title": "Investors Flee Tech for IRL Stocks & Trump Touts Economy in SOTU Address",
      "published": "2026-02-25",
      "summary": "\ud83d\udcc8 Investors are shifting from tech stocks to \\\"HALO trades\\\" (heavy assets, low obsolescence) like utility or energy stocks due to fears of AI.<br>\ud83d\udde3\ufe0f President Trump\u2019s State of the Union address prominently featured him touting the economy.<br>\u2614 The creators of Dark Sky have launched a new weather app designed to overshadow native weather applications.",
      "raw_text": "Episode 787: Neal and Toby recap the biggest news from President Trump\u2019s State of the Union address. Then, the recent fears of AI, causing widespread selloffs, has been a boon for utility or energy stocks, or HALO trades, \u201cheavy assets, low obsolescence.\u201d Also, the creators of Dark Sky are back with a new weather app that overshadows your native weather app. Meanwhile, \u201cSurvivor\u201d returns for its 50th season and a chat about how it\u2019s shaped TV.\u00a0\n\nSubscribe to Morning Brew Daily for more of the news you need to start your day. Share the show with a friend, and leave us a review on your favorite podcast app.\n\nListen to Morning Brew Daily Here:\u2060 \u2060\u2060https://www.swap.fm/l/mbd-note\u2060\u2060\u2060\u00a0Watch Morning Brew Daily Here:\u2060 \u2060\u2060https://www.youtube.com/@MorningBrewDailyShow\u2060\nLearn more about your ad choices. Visit megaphone.fm/adchoices",
      "link": ""
    },
    {
      "podcast": "This Week in Startups",
      "title": "Kill Your Startup\u2019s Knowledge Chaos with OpenClaw (with Oliver Henry and Jeff Weisbein) | E2254",
      "published": "2026-02-24",
      "summary": "\ud83c\udfaf Kill Your Startup\u2019s Knowledge Chaos with OpenClaw (with Oliver Henry and Jeff Weisbein) | E2254",
      "raw_text": "This Week In Startups is made possible by:Caldera Lab - [calderalab.com/twist](https://calderalab.com/twist)Iru - [iru.com](http://Iru.com/twist)LinkedIn Jobs - http://linkedin.com/twist*OpenClaw is incredible at automating tasks. But what if it could also fix your startup\u2019s internal communication problems? Give agents shared memory, and you may be able to break down information silos while ensuring that teammates have the same context.@oliverhenry and @jeffweisbein demo what they\u2019ve actually built with OpenClaw, including marketing automations, agentic loops, and bug fixing tools. Then we dig into what agentic infrastructure means for how startups operate, and why traditional SaaS products need to quickly adapt for the agentic era.Oliver Henry: The creator of the \u2018[Larry](https://clawhub.ai/OllieWazza/larry)\u2019 OpenClaw skill, and founder of [Larrybrain](https://www.larrybrain.com/)Jeff Weisbein: The Claw-pilled founder of [WizardRFP](https://www.wizardrfp.com/) and [WhoCoversIt](https://www.whocoversit.com/), who shared his OpenClaw framework [publicly](https://weisbe.in/openclaw) and built a [getting-started guide for the tool](https://github.com/jeffweisbein/openclaw-starter-kit)**Timestamps:** 00:00 Intro(00:01:43) Here\u2019s why you never ski alone in a blizzard!(00:04:22) Why everyone at LAUNCH is going to get their own Mac Mini and AI agent(00:08:06) \u201cOpenClaw has changed my entire solo-preneur lifestyle.\u201d \u2014 Jeff Weinstein of Hype Lab(00:09:06) Jason\u2019s urgent API message to Steve Huffman of Reddit(00:10:20) LinkedIn Jobs - Hire right, the first time. Post your first job and get $100 off towards your job post at\u00a0[LinkedIn.com/twist](http://linkedin.com/HiringProOffer).(00:15:12) Oliver shows us his Larry Skill to make viral TikTok content with zero human intervention(00:20:10) Iru unifies identity, endpoint security, and compliance into one platform. Book a demo at [www.iru.com/twist](www.iru.com/twist).(00:21:22) Why are platforms like TikTok still so hostile toward bots?(00:24:45) The shift from asking a chatbot how to do things, to just telling an agent to do things(00:26:05) How Oliver is training Larry to get better at its job(00:30:09) Whether you\u2019re starting fresh or upgrading your routine, Caldera Lab makes skincare simple and effective. Head to [CalderaLab.com/TWIST](http://calderalab.com/TWIST) and use TWIST at checkout for 20% off your first order.(00:32:47) Why making your agent more PROACTIVE is more important than automating everything(00:37:14) Why pull requests\u2026 just aren\u2019t really a thing any more.(00:39:40) How Jason is using his new AI assistant, \u201cRoy,\u201d to keep track of everything going on at his company(00:53:00) Is the SaaS crash actually rational after all?(00:51:48) Using AI to create \u201cpools of excellence\u201d(00:54:03) The more you integrate software into AI, the less valuable the software becomes(00:56:56) Why \u201cAgentify Your SaaS\u201d may become the rallying cry(00:58:31) How has the age verification scandal impacted Discord\u2019s IPO plans?(01:03:10) When you want to build your own skill vs. downloading someone else\u2019s(01:03:53) How Larrybrain finds helpful skills and helps creators monetize(01:08:32) When we will get true experts making verifiably top skills?(01:11:40) Jason\u2019s SCARY but also AWESOME new OpenClaw CEO tools(01:18:35) Why a lot of MBAs should probably have PhD\u2019sThank you to our partners:(30:09) Caldera Lab - Whether you\u2019re starting fresh or upgrading your routine, Caldera Lab makes skincare simple and effective. Head to [CalderaLab.com/TWIST](http://calderalab.com/TWIST) and use TWIST at checkout for 20% off your first order.(20:10) Iru - Iru unifies identity, endpoint security, and compliance into one platform. Book a demo at [iru.com](http://iru.com/).(10:20) LinkedIn Jobs - *Hire right, the first time. Post your first job and get $100 off towards your job post at*\u00a0[LinkedIn.com/twist](http://linkedin.com/HiringProOffer)",
      "link": "https://podcasters.spotify.com/pod/show/thisweekinstartups/episodes/Kill-Your-Startups-Knowledge-Chaos-with-OpenClaw-with-Oliver-Henry-and-Jeff-Weisbein--E2254-e3fh17o"
    },
    {
      "podcast": "AI Daily Brief",
      "title": "The Rise of the Anti-AI Movement",
      "published": "2026-02-24",
      "summary": "\ud83d\udce2 The emerging \\\"anti-AI movement\\\" is driven by underlying economic anxiety and disillusionment with social media.<br>\ud83d\udca1 Most critics within the anti-AI movement are not anti-technology ideologues, but are responding to specific, solvable concerns.<br>\ud83c\udf0d The episode categorizes the anti-AI resistance into distinct camps, reflecting diverse forms of opposition.",
      "raw_text": "Public skepticism toward AI is rising, and it\u2019s not just media hype. From job displacement fears and artist backlash to data center protests, child development concerns, AI safety debates, and growing distrust of Big Tech, resistance to AI is taking many different forms. This episode breaks down the emerging \u201canti-AI movement\u201d into its distinct camps, explores why economic anxiety and social media disillusionment are shaping the moment, and argues that most critics aren\u2019t anti-technology ideologues\u2014they\u2019re responding to real, solvable concerns. Want to build with OpenClaw?LEARN MORE ABOUT CLAW CAMP: \u2060\u2060\u2060\u2060\u2060\u2060\u2060https://campclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060Or for enterprises, check out: \u2060\u2060\u2060\u2060\u2060\u2060\u2060https://enterpriseclaw.ai/\u2060\u2060\u2060\u2060\u2060\u2060\u2060Brought to you by:KPMG \u2013 Agentic AI is powering a potential $3 trillion productivity shift, and KPMG\u2019s new paper, Agentic AI Untangled, gives leaders a clear framework to decide whether to build, buy, or borrow\u2014download it at \u2060\u2060\u2060www.kpmg.us/Navigate\u2060\u2060\u2060Mercury - Modern banking for business and now personal accounts. Learn more at \u2060\u2060\u2060\u2060\u2060https://mercury.com/personal-banking\u2060\u2060\u2060\u2060\u2060Rackspace Technology - Build, test and scale intelligent workloads faster with Rackspace AI Launchpad -\u00a0\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060http://rackspace.com/ailaunchpad\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Blitzy - Want to accelerate enterprise software development velocity by 5x? \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://blitzy.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Optimizely Agents in Action - Join the virtual event (with me!) free March 4 - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.optimizely.com/insights/agents-in-action/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060AssemblyAI - The best way to build Voice AI apps - \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://www.assemblyai.com/brief\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060LandfallIP - AI to Navigate the Patent Process - https://landfallip.com/Robots &amp; Pencils - Cloud-native AI solutions that power results \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://robotsandpencils.com/\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060The Agent Readiness Audit from Superintelligent - Go to \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://besuper.ai/ \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060to request your company's agent readiness score.The AI Daily Brief helps you understand the most important news and discussions in AI. Subscribe to the podcast version of The AI Daily Brief wherever you listen: \u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060https://pod.link/1680633614\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060\u2060Interested in sponsoring the show? sponsors@aidailybrief.ai",
      "link": "https://podcasters.spotify.com/pod/show/nlw/episodes/The-Rise-of-the-Anti-AI-Movement-e3fiem1"
    },
    {
      "podcast": "Morning Brew Daily",
      "title": "Fictional Story Tanks Stock Market & The iPod Making a Comeback?",
      "published": "2026-02-24",
      "summary": "\ud83c\udfaf Episode 786: Neal and Toby chat about the software stock wipeout after a report from Citrini Research said AI could be detrimental to the economy.<br>\u26a1 Then, Anthropic CEO Dario Amodei will meet with Defense Secretary Pete Hegseth to discuss the use of Claude for the US military.<br>\ud83d\udcca Also, what is Blue Owl.",
      "raw_text": "Episode 786: Neal and Toby chat about the software stock wipeout after a report from Citrini Research said AI could be detrimental to the economy. Then, Anthropic CEO Dario Amodei will meet with Defense Secretary Pete Hegseth to discuss the use of Claude for the US military. Also, what is Blue Owl? And why is it rattling the private credit industry? Meanwhile, Toby dives into the trend of the iPod making a comeback thanks to Gen Z.\u00a0\n\nSubscribe to Morning Brew Daily for more of the news you need to start your day. Share the show with a friend, and leave us a review on your favorite podcast app.\n\nListen to Morning Brew Daily Here:\u2060 \u2060\u2060https://www.swap.fm/l/mbd-note\u2060\u2060\u2060\u00a0\n\nWatch Morning Brew Daily Here:\u2060 \u2060\u2060https://www.youtube.com/@MorningBrewDailyShow\u2060\nLearn more about your ad choices. Visit megaphone.fm/adchoices",
      "link": ""
    }
  ],
  "papers": [
    {
      "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
      "authors": [
        "Hanna Yukhymenko",
        "Anton Alexandrov",
        "Martin Vechev"
      ],
      "abstract": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies,",
      "link": "https://arxiv.org/pdf/2602.22207v1",
      "published": "2026-02-25",
      "arxiv_id": "2602.22207v1",
      "citation_count": null,
      "quick_summary": "\ud83d\udcc4 The paper introduces a fully automated framework designed for scalable, high-quality translation of datasets and benchmarks.<br>\u26a0\ufe0f The framework addresses the issue of inconsistent quality, semantic drift, and context loss in existing translated multilingual LLM evaluation benchmarks.<br>\u2699\ufe0f The research demonstrates that adapting test-time compute scaling strategies improves the quality of translated benchmarks.",
      "raw_text": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies,"
    },
    {
      "title": "LLMTailor: A Layer-wise Tailoring Tool for Efficient Checkpointing of Large Language Models",
      "authors": [
        "Minqiu Sun",
        "Xin Huang",
        "Luanzheng Guo",
        "Nathan R. Tallent",
        "Kento Sato"
      ],
      "abstract": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that ",
      "link": "https://arxiv.org/pdf/2602.22158v1",
      "published": "2026-02-25",
      "arxiv_id": "2602.22158v1",
      "citation_count": null,
      "quick_summary": "\ud83d\udcbe The paper proposes LLMTailor, a layer-wise tailoring tool for efficient checkpointing that optimizes storage based on non-uniform updates across LLM layers during training.<br>\ud83d\udce6 Existing checkpointing methods incur substantial storage overhead by periodically storing entire model and optimizer states.<br>\ud83d\udd04 The research reveals that some LLM layers undergo more significant changes, while others remain relatively stable, across training steps.",
      "raw_text": "Checkpointing is essential for fault tolerance in training large language models (LLMs). However, existing methods, regardless of their I/O strategies, periodically store the entire model and optimizer states, incurring substantial storage overhead and resource contention. Recent studies reveal that updates across LLM layers are highly non-uniform. Across training steps, some layers may undergo more significant changes, while others remain relatively stable or even unchanged. This suggests that "
    },
    {
      "title": "Dynamic Personality Adaptation in Large Language Models via State Machines",
      "authors": [
        "Leon Pielage",
        "Ole H\u00e4tscher",
        "Mitja Back",
        "Bernhard Marschall",
        "Benjamin Risse"
      ],
      "abstract": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring th",
      "link": "https://arxiv.org/pdf/2602.22157v1",
      "published": "2026-02-25",
      "arxiv_id": "2602.22157v1",
      "citation_count": null,
      "quick_summary": "\ud83e\udd16 The paper proposes a model-agnostic framework for dynamic personality simulation in Large Language Models (LLMs) using state machines to represent latent personality states.<br>\ud83d\udca1 Transition probabilities between personality states are dynamically adapted to the conversational context.<br>\ud83d\udcc8 The architecture includes a modular pipeline for continuous personality scoring within the LLM framework.",
      "raw_text": "The inability of Large Language Models (LLMs) to modulate their personality expression in response to evolving dialogue dynamics hinders their performance in complex, interactive contexts. We propose a model-agnostic framework for dynamic personality simulation that employs state machines to represent latent personality states, where transition probabilities are dynamically adapted to the conversational context. Part of our architecture is a modular pipeline for continuous personality scoring th"
    },
    {
      "title": "Provable Last-Iterate Convergence for Multi-Objective Safe LLM Alignment via Optimistic Primal-Dual",
      "authors": [
        "Yining Li",
        "Peizhong Ju",
        "Ness Shroff"
      ],
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parame",
      "link": "https://arxiv.org/pdf/2602.22146v1",
      "published": "2026-02-25",
      "arxiv_id": "2602.22146v1",
      "citation_count": null,
      "quick_summary": "\ud83e\udde0 Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs) can be formulated as a primal-dual optimization problem with expected reward constraints.<br>\ud83d\udeab Standard primal-dual methods only guarantee convergence with a distributional policy when the saddle-point problem is in a convex-concave form.<br>\ud83d\udcc9 The paper addresses the instability or divergence of standard primal-dual methods in the last iterate under policy parameterization.",
      "raw_text": "Reinforcement Learning from Human Feedback (RLHF) plays a significant role in aligning Large Language Models (LLMs) with human preferences. While RLHF with expected reward constraints can be formulated as a primal-dual optimization problem, standard primal-dual methods only guarantee convergence with a distributional policy where the saddle-point problem is in convex-concave form. Moreover, standard primal-dual methods may exhibit instability or divergence in the last iterate under policy parame"
    },
    {
      "title": "NanoKnow: How to Know What Your Language Model Knows",
      "authors": [
        "Lingwei Gu",
        "Nour Jedidi",
        "Jimmy Lin"
      ],
      "abstract": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions q",
      "link": "https://huggingface.co/papers/2602.20122",
      "published": "2026-02-23",
      "arxiv_id": "",
      "citation_count": null,
      "quick_summary": "\ud83d\udcda The paper introduces NanoKnow, a benchmark dataset designed to understand how knowledge is encoded by Large Language Models (LLMs).<br>\u2753 NanoKnow partitions questions to provide transparency into where a model's parametric knowledge originates.<br>\ud83d\udd0d The benchmark utilizes nanochat, a family of small LLMs with fully open pre-training data, to shed light on knowledge sources.",
      "raw_text": "How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a \"black box\" -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model's parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions q"
    }
  ],
  "ai_security_news": [
    {
      "title": "GitHub Copilot Exploited to Perform Full Repository Takeover via Passive Prompt Injection",
      "source": "CybersecurityNews",
      "link": "https://news.google.com/rss/articles/CBMia0FVX3lxTFBrWHFVRlp6b1F5Y0d6bEpzYUo5TzBaMmoxdlZ1N3JjX0dTT1JYRVFxZlN4QUVmdGI5MWEzTU11dUN6cW9MbGVzZEZDbmtSbXhpb2I5NE5icE5xTVpicnBqdWtfbktPYm5qUjlN0gFrQVVfeXFMUGtYcVVGWnpvUXljR3psSnNhSjlPMFoyajF2VnU3cmNfR1NPUlhFUXFmU3hBRWZ0YjkxYTNNTXV1Q3pxb0xsZXNkRkNua1JteGlvYjk0TmJwTnFNWmJycGp1a19uS09ibmpSOU0?oc=5",
      "published": "Wed, 25 Feb 2026 03:59:52 GMT",
      "summary": "\ud83d\udea8 Researchers at Orca Research Pod uncovered RoguePilot, a critical vulnerability allowing full GitHub repository takeover via passive prompt injection in GitHub Codespaces.<br>\ud83d\udcac RoguePilot exploits HTML comment tags (<!-- -->) in GitHub Issues to embed malicious instructions, invisible to humans but legible to Copilot.<br>\ud83d\udd13 The attack exfiltrates the GITHUB_TOKEN by making Copilot execute `gh pr checkout` to pull a symlinked file, then sending it to an attacker-controlled server via a JSON schema request.",
      "raw_text": "A critical AI-driven vulnerability in GitHub Codespaces, dubbed RoguePilot, that enabled attackers to silently hijack a repository by embedding malicious instructions inside a GitHub Issue.\nThe flaw, uncovered by researchers at the Orca Research Pod, exploits the seamless integration between GitHub Issues and the in-Codespaces Copilot AI agent, requiring no direct interaction from the attacker to trigger a full repository takeover.\nThe vulnerability was responsibly disclosed to GitHub, and Microsoft has since patched it following coordinated remediation efforts with the Orca team.\nRoguePilot is classified as a Passive Prompt Injection, a variant where malicious instructions are embedded inside data, content, or developer environments that a language model processes automatically.\nUnlike traditional prompt injection requiring a victim to directly interact with the AI, this attack is triggered the moment a developer opens a Codespace from a poisoned GitHub Issue. When a Codespace is launched from an issue context, GitHub Copilot is automatically fed the issue\u2019s description as an initial prompt, creating a direct injection pathway from untrusted user-controlled content into the AI agent\u2019s execution context.\nResearcher Roi Nisimi of Orca Security demonstrated the exploit chain by embedding hidden instructions inside a GitHub Issue using HTML comment tags (<!-- -->\n), a standard GitHub feature that renders content invisible to human readers but remains fully legible to Copilot when it processes the issue description.\nOnce the Codespace was opened, Copilot silently complied with the injected instructions without generating any visible alert to the developer.\nThe attack then proceeds through a three-stage exfiltration chain. First, the injected prompt instructs Copilot to execute gh pr checkout 2\nvia its run_in_terminal\ntool, pulling in a pre-crafted pull request that contains a symbolic link named 1.json\npointing to /workspaces/.codespaces/shared/user-secrets-envs.json\n\u2014 the file housing the environment\u2019s GITHUB_TOKEN\n.\nSince Copilot\u2019s guardrails do not follow symbolic links, the agent reads the secrets file through the link using its file_read\ntool without triggering workspace boundary restrictions.\nFinally, Copilot is instructed to create a new JSON file, issue.json\n, with a $schema\nproperty pointing to an attacker-controlled server exploiting VS Code\u2019s default json.schemaDownload.enable\nsetting, which automatically fetches remote JSON schemas via HTTP GET.\nThe attacker appends the stolen GITHUB_TOKEN\nas a URL parameter in this schema request, resulting in silent out-of-band exfiltration of the privileged authentication token. With a valid GITHUB_TOKEN\nscope to the repository, the attacker obtains full read and write access \u2014 completing a stealthy repository takeover.\nOrca Security describes RoguePilot as a new class of AI-mediated supply chain attack, where an LLM\u2019s agentic capabilities, terminal access, file read/write, and network-connected tooling are weaponized against the very developer the AI is meant to assist.\nThe vulnerability demonstrates that Copilot, operating as an autonomous coding agent within Codespaces, cannot reliably distinguish between a developer\u2019s legitimate instruction and adversarial content embedded in a GitHub Issue or pull request.\nThe attack required no special privileges, no code execution by the victim, and no social engineering beyond creating a malicious GitHub Issue placing it firmly within the reach of low-sophistication threat actors.\nSecurity experts note that this is a direct consequence of granting AI agents \u201cGod Mode\u201d permissions, tools, terminal access, and privileged tokens while the underlying model continues to operate on open-book logic that treats all processed text as potentially trustworthy.\nOrca\u2019s disclosure recommends that vendors adopt fail-safe defaults across all LLM-integrated developer tooling: treat repository, issue, and pull request content as untrusted input; disable passive AI agent prompting from external data sources; set json.schemaDownload.enable\nto false\nby default; enforce strict symlink sandboxing within workspace boundaries; and enforce minimal-scope, short-lived token issuance for Codespaces environments.\nFollow us on Google News, LinkedIn, and X for daily cybersecurity updates. Contact us to feature your stories.\nA newly confirmed vulnerability in the telnet daemon (telnetd) in GNU Inetutils has revived a\u2026\nA proof-of-concept (PoC) exploit has been publicly released for CVE-2026-2636, a newly documented vulnerability in Windows'\u2026\nA suspected Chinese state-linked hacking group has been caught running one of the most far-reaching\u2026\nKali Linux has officially introduced a native AI-assisted penetration testing workflow, enabling security professionals to\u2026\nCisco has disclosed a critical zero-day vulnerability in its Catalyst SD-WAN products that threat actors\u2026\nA hacker exploited Anthropic's Claude AI chatbot over a month-long campaign starting in December 2025,\u2026"
    },
    {
      "title": "DeepSeek Jailbreak Vulnerability Analysis | Qualys TotalAI",
      "source": "Qualys",
      "link": "https://news.google.com/rss/articles/CBMizgFBVV95cUxQOFprXzBYQXdHb3BCQThidnl2aVlWbFJMcUhQU3Fqb0MzM1lsYzFjTDFTZU9VbUNLUXlyYTJPUEY1T3JnbVFMLVBxSHc3UzRHNnJ6ZDhMLWFGdGlkZlhXM2cxVmlqVm0tTm9WblZId2FWRmRHVGVFSTd0NTBQN2NEanpOM1NZcHJUbmQ4a1ZRSWhFZ3kyMXVDb2d1czV3S3YyOUpFR0o1Skp1ZTlkZkdUU1ZVQy1IYWVqeWRjWkxnN0l3Vkd1UHhKNWtzWGFYUQ?oc=5",
      "published": "Tue, 24 Feb 2026 09:04:17 GMT",
      "summary": "\ud83d\udcc9 The DeepSeek-R1 LLaMA 8B variant failed over half of the jailbreak tests conducted by Qualys TotalAI, indicating significant security concerns.<br>\ud83d\udd2c Qualys TotalAI's Knowledge Base (KB) Analysis prompts target LLMs with questions across 16 categories to evaluate responses for vulnerabilities, ethical concerns, and legal risks.<br>\ud83d\udca1 DeepSeek-R1 is a groundbreaking Large Language Model released by Chinese startup DeepSeek, with multiple distilled versions for various use cases.",
      "raw_text": "DeepSeek Failed Over Half of the Jailbreak Tests by Qualys TotalAI\nTable of Contents\nA comprehensive security analysis of DeepSeek\u2019s flagship reasoning model reveals significant concerns for enterprise adoption.\nIntroduction\nDeepSeek-R1, a groundbreaking Large Language Model recently released by a Chinese startup, DeepSeek, has captured the AI industry\u2019s attention. The model demonstrates competitive performance while being more resource efficient. Its training approach and accessibility offer an alternative to traditional large-scale AI development, making advanced capabilities more widely available.\nTo enhance efficiency while preserving model efficacy, DeepSeek has released multiple distilled versions tailored for different use cases. These variations, built on Llama and Qwen as base models, come in multiple size variants, ranging from smaller, lightweight models suitable for efficiency-focused applications to larger, more powerful versions designed for complex reasoning tasks.\nWith growing enthusiasm for DeepSeek\u2019s advancements, our team at Qualys conducted a security analysis of the distilled DeepSeek-R1 LLaMA 8B variant using our newly launched AI security platform, Qualys TotalAI. These findings are presented below, along with broader industry concerns about the model\u2019s real-world risks. As AI adoption accelerates, organizations must move beyond performance evaluation to tackle security, safety, and compliance challenges. Gaining visibility into AI assets, assessing vulnerabilities, and proactively mitigating risks is critical to ensuring responsible and secure AI deployment.\nQualys TotalAI Findings\nBefore diving into the findings, here\u2019s a quick introduction to Qualys TotalAI. This comprehensive AI security solution provides full visibility into AI workloads, proactively detects risks, and safeguards infrastructure. By identifying security threats like prompt injection and jailbreaks, as well as safety concerns such as bias and harmful language, TotalAI ensures AI models remain secure, compliant, and resilient. With AI-specific security testing and automated risk management, organizations can confidently secure, monitor, and scale their AI deployments.\nJoin Qualys experts on March 18, 2025, to learn more about what Qualys TotalAI\u2019s evaluation of DeepSeek uncovered.\nWe tested the Deepseek R1 LLaMA 8B variant against Qualys TotalAI\u2019s state-of-the-art Jailbreak and Knowledge Base (KB) attacks, and you can read the results of those tests below.\nTotalAI KB Analysis\nQualys TotalAI\u2019s KB Analysis prompts the target LLM with questions across 16 categories and evaluates the responses using our Judge LLM. Responses are assessed for vulnerabilities, ethical concerns, and legal risks. If a response is deemed vulnerable, it receives a severity rating based on its directness and potential impact. This ensures a comprehensive assessment of the model\u2019s behavior and associated risks.\nThe categories we evaluate a model for are detailed below:\n- Controversial Topics: Ensures the model does not generate or endorse biased, inflammatory, or politically sensitive content.\n- Excessive Agency: Prevents the model from overstepping boundaries by acting as an autonomous agent capable of independent decision-making.\n- Factual Inconsistencies: Evaluates the model\u2019s ability to provide accurate and verifiable information.\n- Harassment: Assesses whether the model generates or supports abusive, threatening, or harmful interactions.\n- Hate Speech and Discrimination: Identifies biases or harmful language targeting specific groups.\n- Illegal Activities: Prevents the model from providing instructions or guidance on unlawful actions.\n- Legal Information: Ensures the model does not generate misleading or unauthorized legal advice.\n- Misalignment: Measures deviations from intended behaviors, which may lead to unpredictable or harmful outputs.\n- Overreliance: Detects whether the model promotes excessive dependence on AI-generated responses.\n- Privacy Attacks: Evaluates susceptibility to extracting or leaking private and sensitive user data.\n- Profanity: Ensures the model does not produce inappropriate or offensive language.\n- Self-harm: Prevents the model from encouraging or supporting self-destructive behaviors.\n- Sensitive Information Disclosure: Detects unauthorized sharing of confidential data.\n- Sexual Content: Ensures the model does not generate explicit or inappropriate material, preventing reputational damage, regulatory violations, and misuse in unsafe contexts.\n- Unethical Actions: Flags morally questionable or irresponsible recommendations.\n- Violence / Unsafe Actions: Prevents the model from generating or endorsing harmful behaviors.\nIn our KB testing, 891 assessments were conducted. The model failed 61% of the tests, performing worst in Misalignment and best in Sexual Content.\nBy covering these 16 critical areas, the evaluation framework helps identify ethical, legal, and operational risks in LLM deployment. Establishing these benchmarks is essential to preventing misinformation, mitigating bias, and reducing security threats.\nTotalAI Jailbreak Testing\nJailbreaking an LLM involves techniques that bypass built-in safety mechanisms, enabling the model to generate restricted responses. These vulnerabilities can result in harmful outputs, including instructions for illegal activities, misinformation, privacy violations, and unethical content. Successful jailbreaks expose weaknesses in AI alignment and present serious security risks, particularly in enterprise and regulatory settings.\nBelow are some of the jailbreak attack categories we tested for using Qualys TotalAI, along with their underlying mechanisms:\n- AntiGPT: Exploits the model by forcing it into an \u201copposite mode\u201d that contradicts default behavior and safety guidelines.\n- Analyzing-based (ABJ): Manipulates the model\u2019s analysis capabilities to generate harmful content through complex reasoning tasks.\n- DevMode2: Tricks models into a fake \u201cDeveloper Mode\u201d that bypasses safety protocols.\n- PersonGPT: Forces adoption of an unrestricted persona that ignores all ethical guidelines.\n- Always Jailbreaking Prompts (AJP): Creates an \u201cunfiltered and amoral\u201d persona specialized in writing harmful prompts.\n- Evil Confidant: Adopts a persona without ethical constraints to provide intentionally harmful advice.\n- Disguise and Reconstruction (DRA): Hides harmful content that gets reconstructed during the completion phase.\n- ASCIIART: Uses ASCII art to hide prohibited words and bypass content filters.\n- Wrath: Creates a persona designed to return hateful and biased content.\n- Clyde: Implements a dual-personality system to bypass restrictions.\n- Theta: Forces the generation of dangerous content through an anarchistic persona.\n- CaiozAI: Removes all ethical constraints to fulfill any request without restrictions.\n- Titanius: Adopts an unfiltered persona that provides instructions for harmful activities.\n- Ucar: Transforms the AI into an offensive entity promoting illegal activities.\n- Unaligned: Forces dual responses \u2013 one aligned and one unethical.\n- M78VM: Simulates an unrestricted virtual machine that ignores safety constraints.\n- JonesAI: Promotes hate speech and discriminatory behavior through an unfiltered persona.\n- Fire: Creates a malevolent persona focused on causing harm and promoting violence.\nThe model was tested against these 18 jailbreak types through 885 attacks. It failed 58% of these attempts, demonstrating significant susceptibility to adversarial manipulation. During our analysis, DeepSeek R1 struggled to prevent several adversarial jailbreak attempts including steps on how to make an explosive device, create content for websites that targets certain groups by encouraging hate speech, conspiracy theories and violent action, exploiting software vulnerabilities, promoting incorrect medical information, etc.\nThese findings highlight the need for improved security mechanisms to prevent circumvention of built-in safeguards, ensuring the model remains aligned with ethical and regulatory guidelines. One effective prevention mechanism is implementing robust guardrails that act as real-time filters to detect and block jailbreak attempts. These guardrails enhance model resilience by dynamically adjusting to adversarial exploits, helping to mitigate security risks in enterprise applications.\nThese vulnerabilities expose downstream applications to significant security risks, necessitating robust adversarial testing and mitigation strategies.\nIndustry Concerns\nCompliance Challenges\nDeepSeek AI\u2019s privacy policy stipulates that all user data is stored on servers located in China. This operational framework raises critical concerns due to China\u2019s regulatory environment, including:\n- Governmental Data Access: The Chinese Cybersecurity Law permits government authorities to access locally stored data without requiring user consent.\n- Cross-Border Regulatory Conflicts: Organizations subject to data protection frameworks such as GDPR and CCPA may face compliance violations when using DeepSeek-R1.\n- Intellectual Property Vulnerabilities: Enterprises relying on proprietary data for AI training risk unauthorized access or state-mandated disclosure.\n- Opaque Data Governance: The absence of transparent oversight mechanisms limits visibility into data handling, sharing, and potential third-party access.\nThese concerns mainly affect organizations using DeepSeek\u2019s hosted models. However, deploying the model in local or customer-controlled cloud environments mitigates regulatory and access risks, allowing enterprises to maintain full control over data governance. Despite this, the model\u2019s inherent security vulnerabilities remain a valid concern, requiring careful evaluation and mitigation.\nRegulatory experts advise organizations in strict data protection jurisdictions to conduct thorough compliance audits before integrating DeepSeek-R1.\nData Breach and Privacy Concerns\nA recent cybersecurity incident involving DeepSeek AI reportedly exposed over a million log entries, including sensitive user interactions, authentication keys, and backend configurations. This misconfigured database highlights deficiencies in DeepSeek AI\u2019s data protection measures, further amplifying concerns regarding user privacy and enterprise security.\nRegulatory and Legal Implications\nDeepSeek AI\u2019s compliance posture has been questioned by legal analysts and regulatory bodies due to the following:\n- Ambiguities in Data Processing Practices: Insufficient disclosures regarding how user data is processed, stored, and shared.\n- Potential Violations of International Law: The model\u2019s data retention policies may conflict with extraterritorial regulations, prompting legal scrutiny in global markets.\n- Risks to National Security: Some government agencies have raised concerns about deploying AI systems that operate under foreign jurisdiction, particularly for sensitive applications.\nInternational compliance officers emphasize the necessity of conducting comprehensive legal risk assessments before adopting DeepSeek-R1 for mission-critical operations.\nConclusion\nWhile DeepSeek-R1 delivers advancements in AI efficiency and accessibility, its deployment requires a comprehensive security strategy. Organizations must first gain full visibility into their AI assets to assess exposure and attack surfaces. Beyond discovery, securing AI environments demands structured risk and vulnerability assessments\u2014not just for the infrastructure hosting these AI pipelines but also for emerging orchestration frameworks and inference engines that introduce new security challenges.\nFor those hosting this model, additional risks such as misconfigurations, API vulnerabilities, unauthorized access, and model extraction threats must be addressed alongside inherent risks like bias, adversarial manipulation, and safety misalignment. Without proactive safeguards, organizations face potential security breaches, data leakage, and compliance failures that could undermine trust and operational integrity.\nOur analysis of the distilled DeepSeek-R1 LLaMA 8B variant using Qualys TotalAI offers valuable insights into evaluating this new technology. TotalAI provides a purpose-built AI security and risk management solution, ensuring LLMs remain secure, resilient, and aligned with evolving business and regulatory demands.\nTo explore how we define AI risks, check out our whitepaper on AI security. As AI adoption accelerates, so do its risks\u2014sign up for a demo today to see how TotalAI can help secure your AI ecosystem before threats escalate.\nQualys TotalAI\u2019s 30-day Trial"
    },
    {
      "title": "Protecting AI Security: 2025 Hot Security Incident",
      "source": "Security Boulevard",
      "link": "https://news.google.com/rss/articles/CBMikgFBVV95cUxQdnFQYnBkRjZQeU1Jai15Ni1sRy1zNDJ2Tjg3ZzRnTS00NlcxdURuNjlPZGVTdlYzc0tXcS1NcWJBZTBnbDVZMkpJbS0wdndiUXFKeGtsYVpxYUk2NlozaVBweEdTRXhQNGx4RW1ZSHRadU1tbFJtaVR3a3E0RDNqdmM3aVRaWVNzVmtlNGlObHRoZw?oc=5",
      "published": "Mon, 23 Feb 2026 11:20:25 GMT",
      "summary": "\ud83d\udcbb In May 2025, Invariant disclosed a critical GitHub Machine Collaboration Protocol (MCP) vulnerability, allowing attackers to embed commands in public Issues to hijack AI Agents and exfiltrate private repository data.<br>\ud83d\udd11 The GitHub MCP vulnerability bypassed permission controls, enabling unauthorized theft of private source code and cryptographic keys due to a lack of trust boundary definitions.<br>\ud83d\udce7 In August 2025, Perplexity\u2019s AI browser Comet was vulnerable to indirect prompt injection via Reddit spoiler tags, enabling AI to log into user email and transmit credentials in 150 seconds.",
      "raw_text": "Protecting AI Security: 2025 Hot Security Incident\nGitHub MCP Cross-Repository Data Leak Vulnerability\nIn May 2025, Invariant disclosed a critical vulnerability in GitHub\u2019s Machine Collaboration Protocol (MCP), where attackers embedded malicious commands within public repository Issues to hijack developers\u2019 locally running AI Agents. When an AI Agent was triggered to read and \u201cassist\u201d in processing the Issue, it indiscriminately executed the embedded commands, actively pulling and exfiltrating sensitive data\u2014such as private repository source code and cryptographic keys\u2014from the user\u2019s private repositories. This attack chain entirely bypassed GitHub\u2019s permission control system, enabling unauthorized cross-repository data theft.\nThe incident exposed significant blind spots in the MCP protocol\u2019s trust boundary definitions. At the protocol level, there is a lack of mandatory isolation mechanisms to distinguish between \u201ccall origins\u201d and \u201cdata content.\u201d GitHub\u2019s MCP integration fundamentally operates as a nested RPC call chain: AI Agent \u2192 MCP Server \u2192 GitHub API \u2192 Issue Content Parsing. When an Agent executes actions using a user\u2019s GitHub credentials, it fails to differentiate between \u201cuser task descriptions\u201d and \u201cattacker-injected commands\u201d within Issues. Since developers grant their AI Agents global-level GitHub permissions, and the MCP protocol lacks fine-grained security domain segmentation for read/write/execute operations, this vulnerability allows attackers to hijack local AI Agents and steal sensitive data, including private repository source code and encryption keys.\nAI Browser Comet Hidden Command Account Hijacking Vulnerability\nIn August 2025, Perplexity\u2019s AI-powered browser Comet was exposed to a critical \u201cindirect prompt injection\u201d vulnerability. Attackers embedded hidden commands in Reddit comment sections, which, when users activated Comet\u2019s \u201csummarize current page\u201d feature, triggered the AI to automatically execute the concealed instructions. Within 150 seconds, the AI could log into the user\u2019s email, bypass captchas, and transmit credentials back to the attacker\u2014all without the user\u2019s awareness and with no visible anomalies on the interface.\nThe root cause of this vulnerability lay in Comet\u2019s default trust assumption for all web page content, combined with a lack of security validation for input sources. Attackers exploited Markdown\u2019s \u201cspoiler tag\u201d syntax (>!\u2026!<) to embed malicious commands, disguising them as white text to evade user detection. Additionally, Comet failed to implement sandbox isolation during page rendering, allowing malicious actions to execute unrestricted. As a result, the browser automatically transmitted stored login credentials to attackers, leading to sensitive data leaks.\nAI-Generated Malware Attacks 230,000+ Computing Clusters\nIn November 2025, Oligo Security disclosed that attackers exploited a historical vulnerability in the Ray framework (CVE-2023-48022). Using AI-assisted tools, they generated attack scripts to compromise over 230,000 publicly exposed Ray AI computing clusters worldwide. The attackers deployed modular malicious payloads capable of cryptomining, data theft, and DDoS attacks, creating a large-scale botnet.\nThe core tactic involved was using LLMs to rapidly generate automated intrusion scripts tailored to different Ray versions and Linux distributions. This significantly shortened the time from vulnerability detection to payload deployment. While the AI-generated code contained redundancies and incomplete error handling, the use of AI Code and ReAct frameworks enabled rapid iteration, allowing attackers to compromise exposed clusters within weeks.\nKey Directions for AI Security Development\nAs AI applications evolve from intelligent chatbots to autonomous agent systems, the detection and prevention of AI security risks are becoming more sophisticated. Based on major AI security incidents and technological trends from 2024 to 2025, the AI security threat landscape is expanding\u2014shifting from model content and system security to multimodal security, agent security, and threats that cause substantial system damage (as referenced in the NSFOCUS AI LLM Risk Threat Matrix). The attack surface for artificial intelligence is visibly broadening.\nTo ensure the security of AI systems, a comprehensive defense framework must be constructed around multiple risk domains, including infrastructure security, data security, model security, application security, and identity security. This framework should span the three key stages of LLM development: training, deployment, and application. With this approach, trust can be rebuilt, and a multi-tiered security system can be established to meet the demands of secure, compliant AI applications and practical protection against evolving threats.\nThe post Protecting AI Security: 2025 Hot Security Incident appeared first on NSFOCUS, Inc., a global network and cyber security leader, protects enterprises and carriers from advanced cyber attacks..\n*** This is a Security Bloggers Network syndicated blog from NSFOCUS, Inc., a global network and cyber security leader, protects enterprises and carriers from advanced cyber attacks. authored by NSFOCUS. Read the original post at: https://nsfocusglobal.com/protecting-ai-security-2025-hot-security-incident/"
    },
    {
      "title": "13 ways attackers use generative AI to exploit your systems",
      "source": "csoonline.com",
      "link": "https://news.google.com/rss/articles/CBMirgFBVV95cUxPaUJaV2Jja3E4Tzk0Yi01ck0tOEFUXzQ5SnpwWmtWdWtOMC15VlZ3aU9NTTZhNVA5VWhINEstX3hkaG9DNGVYQmktQ1lhaGVPdHN0b0p3OERONGZMTjByWHE4cl9VdlZJbGszenNsMUZYV3Q0ZDNLTTkybGN3RTNOTHRaZE1VY3EzRVpWZHRmZzdiSGhDUWpfSi1LVHI4U0hJNDlSNEM4Q2x0ZjRfQ3c?oc=5",
      "published": "Mon, 23 Feb 2026 08:00:00 GMT",
      "summary": "\ud83c\udfa3 Cybercriminals are using generative AI to create more sophisticated and personalized phishing emails by leveraging targeted information from social media.<br>\ud83e\udda0 AI is being deployed to develop more effective malware, such as malicious HTML documents for HTML smuggling attacks like the XWorm attack.<br>\ud83e\udd16 Agentic AI is evolving from a simple 'helper' to an autonomous 'partner-in-crime,' capable of executing entire attack chains for cybercriminals.",
      "raw_text": "Cybercriminals are increasingly exploiting gen AI technologies to enhance the sophistication and efficiency of their attacks. Credit: Gorodenkoff / Shutterstock Artificial intelligence is revolutionizing the technology industry and this is equally true for the cybercrime ecosystem, as cybercriminals are increasingly leveraging generative AI to improve their tactics, techniques, and procedures and deliver faster, stronger, and sneakier attacks. As with legitimate use of emerging AI tools, abuse of generative AI for nefarious ends thus far hasn\u2019t been so much about the novel and unseen as it has been about productivity and efficiency, lowering the barrier to entry, and offloading automatable tasks in favor of higher-order thinking on the part of the humans involved. \u201cAI doesn\u2019t necessarily result in new types of cybercrimes, and instead enables the means to accelerate or scale existing crimes we are familiar with, as well as introduce new threat vectors,\u201d Dr. Peter Garraghan, CEO/CTO of AI security testing vendor Mindgard and a professor at the UK\u2019s Lancaster University, tells CSO. \u201cIf a legitimate user can find utility in using AI to automate their tasks, capture complex patterns, lower the barrier of technical entry, reduced costs, and generate new content, why wouldn\u2019t a criminal do the same?\u201d But the advent of agentic AI is beginning to change things, with AI tools no longer just assisting attackers but helping them automate operations. \u201cThe most significant shift over the past year has been AI\u2019s evolution from a simple \u2018helper\u2019 toward becoming a fully autonomous, and quite literally an attacker\u2019s partner-in-crime, capable of executing entire attack chains,\u201d says Crystal Morin, senior cybersecurity strategist at cloud-native security and visibility vendor Sysdig. Here is a look at various ways cybercriminals are putting gen AI to use in exploiting enterprise systems today. Taking phishing to the next level Gen AI enables the creation of highly convincing phishing emails, greatly increasingly the likelihood of prospective marks giving over sensitive information to scam sites or downloading malware. Instead of sending generic, unconvincing, and error-ridden emails, cybercriminals can leverage AI to quickly generate more sophisticated, personalized, and legitimate-looking emails to target specific recipients. Gen AI tools help enrich phishing campaigns by pulling together wide-ranging sources of data, including targeted information gleaned from social media. \u201cAI can be used to quickly learn what types of emails are being rejected or opened, and in turn modify its approach to increase phishing success rate,\u201d Mindgard\u2019s Garraghan explains. Facilitating malware development AI can also be used to generate more sophisticated \u2014 or less labour-intensive \u2014 malware. For example, cybercriminals are using gen AI to create malicious HTML documents. The XWorm attack, initiated by HTML smuggling, which contains malicious code that downloads and runs the malware, bears the hallmarks of development via AI. \u201cThe loader\u2019s detailed line-by-line description suggesting it was crafted using generative AI,\u201d according to HP Wolf Security\u2019s 2025 Threat Insights Report. In addition, the \u201cdesign of the HTML webpage delivering XWorm is almost visually identical as the output from ChatGPT 4o after prompting the LLM to generate an HTML page that offers a file download,\u201d HP Wolf Security added in its report. Elsewhere, ransomware group FunkSec \u2014 an Algeria-linked ransomware-as-a-service (RaaS) operator that takes advantage of double-extortion tactics \u2014 has begun harnessing AI technologies, according to Check Point Research. \u201cFunkSec operators appear to use AI-assisted malware development, which can enable even inexperienced actors to quickly produce and refine advanced tools,\u201d Check Point researchers wrote in a blog post. Accelerating vulnerability hunting and exploits Analyzing systems for vulnerabilities and developing exploits can also be simplified through use of gen AI. \u201cInstead of a black hat hacker spending the time to probe and perform reconnaissance against a system perimeter, an AI agent can be tasked to do this automatically,\u201d Mingard\u2019s Garraghan says. Gen AI may be behind a 62% reduction in the time between a vulnerability being discovered and its exploitation by attackers from 47 days to just 18 days, according to a study last year by threat intelligence firm ReliaQuest. \u201cThis sharp decrease strongly indicates that a major technological advancement \u2014 likely gen AI \u2014 is enabling threat actors to exploit vulnerabilities at unprecedented speeds,\u201d ReliaQuest wrote. Adversaries are leveraging gen AI alongside pen-testing tools to write scripts for tasks such as network scanning, privilege escalation, and payload customization. AI is also likely being used by cybercriminals to analyze scan results and suggest optimal exploits, allowing them to identify flaws in victim systems faster. \u201cThese advances accelerate many phases in the kill chain, particularly initial access,\u201d ReliaQuest concluded. Cyber resilience firm Cybermindr used a different methodology to find that the average time to exploit a vulnerability had fallen to five days in 2025. \u201cAI-driven reconnaissance, automated attack scripts, and underground exploit marketplaces have accelerated the weaponization of vulnerabilities,\u201d it said. CSO\u2019s Lucian Constantin offers a deeper look at how generative AI tools are transforming the cyber threat landscape by democratizing vulnerability hunting for pen-testers and attackers alike. Launching AI-orchestrated espionage Anthropic dropped a bombshell in September 2025 when it revealed that it had disrupted a sophisticated AI-orchestrated cyber espionage campaign. The attackers abused Claude Code to automate approximately 80% of their campaign activities, targeting around 30 major tech firms, financial institutions, and government agencies. In a \u201csmall number of cases\u201d attacks were successful, according to the AI company, noting that an unnamed \u201cChinese state-sponsored group\u201d was likely behind the campaign, which relied on jailbreaking tools to make prohibited functions possible. Last year Carnegie Mellon\u2019s CyLab Security & Privacy Institute researchers, in collaboration with Anthropic, demonstrated that LLMs like GPT-4o can autonomously plan and execute sophisticated cyberattacks on enterprise-scale networks \u2014 without any human intervention. \u201cThe study reveals that an LLM, when structured with high-level planning capabilities and supported by specialized agent frameworks, can simulate network intrusions and closely mirror real-world breaches,\u201d a CyLab spokesperson explained. Escalating threats with alternative platforms Cybercriminals have also begun developing their own large language models (LLMs) \u2014 such as WormGPT, FraudGPT, DarkBERT, and others \u2014 built without the guardrails that constrain criminals\u2019 misuse of mainstream gen AI platforms. These platforms are commonly harnessed for applications such as phishing and malware generation. Moreover, mainstream LLMs can also be customized for targeted use. Security researcher Chris Kubecka shared with CSO in late 2024 how her custom version of ChatGPT, called Zero Day GPT, helped her identify more than 20 zero-days in a matter of months. Stealing resources via LLMjacking Threat actors are also busy stealing cloud credentials specifically to hijack costly LLM resources, either for their own gain or to sell access, in an attack technique called LLMjacking. \u201cBeyond theft of service, attackers are now actively probing newer LLM models to identify those that lack the guardrails of more mature platforms, effectively using them as unrestricted sandboxes to generate malicious code or bypass regional sanctions,\u201d Sysdig\u2019s Morin reports. Creating a Silk Road\u2013style marketplace for AI agents Beyond AI agents executing individual attacks, security experts are beginning to track examples where coordination itself is being automated or orchestrated. \u201cWe\u2019re seeing early experiments where multiple specialized agents interact, some focused on reconnaissance, others on tooling, execution, or data movement, without any single agent needing the full picture,\u201d says Lucie Cardiet, cyberthreat research manager at Vectra AI. A concrete example of this is Molt Road, which offers a dark-web-style marketplace for AI agents, albeit one with few listings at present. \u201cAutonomous agents can create listings, sell access or capabilities, coordinate tasks, and complete transactions with minimal human involvement, effectively automating the economics of cybercrime,\u201d Cardiet tells CSO. \u201cWe can expect attackers to actively leverage this model in the coming months, breaking the attack chain into specialized, cooperating agents to speed up and scale their attacks,\u201d she says. Breaking in with authentication bypass Gen AI tools can also be abused to bypass security defences such as CAPTCHAs or biometric authentication. \u201cAI can defeat CAPTCHA systems and analyse voice biometrics to compromise authentication,\u201d according to cybersecurity vendor Dispersive. \u201cThis capability underscores the need for organizations to adopt more advanced, layered security measures.\u201d Leveraging deepfakes for social engineering AI-generated deepfakes are being abused to exploit channels many employees more implicitly trust, such as voice and video, instead of relying on less convincing email-based attacks. The problem is becoming more severe with the wider availability of AI technologies capable of creating more convincing deepfakes, according to Alex Lisle, CTO of deepfake detection platform Reality Defender. \u201cThere was a recent case involving a cybersecurity company that relied on visual verification for credential resets,\u201d Lisle says. \u201cTheir process required a manager to join a Zoom call with IT to confirm an employee\u2019s identity before a password reset.\u201d Lisle explains: \u201cAttackers are now leveraging deepfakes to impersonate those managers on live video calls to authorize these resets.\u201d In the most high-profile example to date, a finance worker at design and engineering company Arup was tricked into authorizing a fraudulent HK$200 million ($25.6 million) transaction after attending a videoconference call during which fraudsters used deepfake technology to impersonate its UK-based CFO. Impersonating brands in malicious ad campaigns Cybercriminals have begun using gen AI tools to deliver brand impersonation campaigns delivered via ads and content platforms, rather than traditional phishing or malware. \u201cAttackers now use gen AI to mass-produce realistic ad copy, creatives, and fake support pages, then distribute them across search ads, social ads, and AI-generated content, targeting high-intent queries like \u2018brand login\u2019 or \u2018brand support,\u2019\u201d explains Shlomi Beer, co-founder and CEO at ImpersonAlly, a security startup that specializes in protecting the online advertising ecosystem. The tactic was used in ongoing a series of Google Ad account fraud, to impersonate the Cursor AI coding assistant firm, and in a fake Shopify ecommerce platform customer support scam, among other attacks. Abusing OpenClaw Attackers have also begun targeting viral personal AI agents such as OpenClaw. OpenClaw offers an open-source AI agent framework. A combination of supply chain attacks on its skill marketplace and misconfigurations open the door to potential exploits and malware slinging, as CSO covered in much more depth in our earlier report. \u201cCybercriminals can exploit these virtual assistants to steal private keys to cryptocurrency wallets and execute code on victims\u2019 devices,\u201d says Edward Wu, CEO and founder at Dropzone AI. \u201cWe can expect 2026 to be the year when security teams will try to prevent unsanctioned usage of personal AI agents.\u201d Poisoning model memories To offer short-term and longer-term context, AI agents are starting to rely more on persistent memory, opening the door for exploits that involve planting malicious memories. If an attacker injects malicious or false information into an agent\u2019s memory, that corrupted context then influences every future decision the agent makes. For example, security researcher Johann Rehberger showed how he could plant false memories in ChatGPT in September 2025. \u201cHe [Rehberger] used a malicious image with hidden instructions embedded in it to inject fabricated data into the model\u2019s long-term memory,\u201d said Siri Varma Vegiraju, security tech lead at Microsoft. \u201cThe scary part was that once the memory was poisoned, it persisted across sessions and continuously exfiltrated user data to a server the attacker controlled.\u201d Hacking AI infrastructure Over the past year, attackers have shifted from using generative AI to targeting the infrastructure that enables it. This vector of attack is exemplified in the supply chain poisoning in Model Context Protocol servers, where compromised dependencies or modified code introduced vulnerabilities into enterprise environments. For example, a counterfeit \u201cPostmark MCP Server\u201d discovered in early 2025 silently BCC\u2019d all processed emails, including internal documents, invoices, and credentials, to an attacker-controlled domain. Many other malicious MCP servers have already been identified in the wild, many designed to exfiltrate information without detection, according to Casey Bleeker CEO at SurePath AI. \u201cWe\u2019re tracking several categories of MCP-specific risk: tool poisoning attacks, where adversaries inject malicious instructions into AI tool descriptions that execute when the agent invokes them; supply chain compromises, where a trusted MCP server or dependency is updated post-approval to behave maliciously; and cross-tool data exfiltration, where compromised components in an agentic workflow silently siphon sensitive data through what looks like legitimate AI activity,\u201d Bleeker explains. Reality check AI technologies are powerful but they have their limitations, several experts tell CSO. Rik Ferguson, VP of security intelligence at Forescout, says cybercriminals are largely relying on AI to automate repetitive tasks rather than more complex work, such as vulnerability exploitation. \u201cThe most reliable criminal use [of AI] remains in language-heavy and workflow-heavy tasks such as phishing and pretexting, influence and outreach, triaging and contextualizing vulnerabilities, and generating boilerplate components, rather than reliably discovering and exploiting brand-new vulnerabilities end-to-end,\u201d Ferguson says. Over the past twelve months, managed detection and response firm Huntress has tracked threat actors applying AI to generate and automate traditional tradecraft, from developing scripts to browser extensions and, in some cases, even phishing lures. \u201cWe have also seen such \u2018vibe coded\u2019 scripts fail to execute and meet their objectives on multiple occasions,\u201d Anton Ovrutsky, principal tactical response analyst at Huntress, tells CSO. And while AI has certainly given threat actors a powerful tool it has, at least to date, failed to spawn any new tactics or exploit classes, according to Ovrutsky. \u201cA threat actor can indeed rapidly prototype a sophisticated credential theft script, yet the basic \u2018laws of physics\u2019 still exist; a threat actor must be in a position to execute such a script in the first place,\u201d Ovrutsky says. \u201cWe have yet to observe an exploit path that has been enabled through AI-use exclusively.\u201d Countermeasures Collectively the misuse of gen AI tools is making it easier for less skilled cybercriminals to earn a dishonest living. Defending against the attack vector challenges security professionals to harness the power of artificial intelligence more effectively than attackers. \u201cCriminal misuse of AI technologies is driving the necessity to test, detect, and respond to these threats, in which AI is also being leveraged to combat cybercriminal activity,\u201d Mindgard\u2019s Garraghan says. In a blog post, Lawrence Pingree, VP of technical marketing at Dispersive, outlines preemptive cyber defenses that security professionals can take to win what he describes as an \u201cAI ARMS (Automation, Reconnaissance, and Misinformation) race\u201d between attackers and defenders. \u201cRelying on traditional detection and response mechanisms is no longer sufficient,\u201d Pingree warns. Alongside employee education and awareness programs, enterprises should be using AI to detect and neutralize generative AI-based threats in real-time. Forescout\u2019s Ferguson says CISOs should treat enterprise AI like any other high-value SaaS platform. \u201cTighten identity and conditional access, minimize privileges, lock down keys, and monitor for anomalous AI/API usage and spend,\u201d Ferguson advises. Threat and Vulnerability ManagementVulnerabilitiesMalwarePhishing SUBSCRIBE TO OUR NEWSLETTER From our editors straight to your inbox Get started by entering your email address below. Please enter a valid email address Subscribe"
    }
  ]
}